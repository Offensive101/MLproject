C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 06:48:08
INFO - ******************MLNX*******************
[[[ 34.77 ]]

 [[ 35.285]]

 [[ 35.34 ]]

 [[ 40.3  ]]

 [[ 40.09 ]]

 [[ 40.4  ]]

 [[ 40.1  ]]

 [[ 41.82 ]]

 [[ 41.91 ]]

 [[ 39.25 ]]

 [[ 38.75 ]]

 [[ 39.15 ]]

 [[ 38.94 ]]

 [[ 38.565]]

 [[ 37.97 ]]

 [[ 37.63 ]]

 [[ 38.59 ]]

 [[ 39.16 ]]

 [[ 39.03 ]]

 [[ 39.08 ]]

 [[ 38.07 ]]

 [[ 37.01 ]]

 [[ 37.01 ]]

 [[ 37.16 ]]

 [[ 37.87 ]]

 [[ 37.29 ]]

 [[ 39.5  ]]

 [[ 40.   ]]

 [[ 39.8  ]]

 [[ 39.35 ]]

 [[ 39.789]]

 [[ 39.84 ]]

 [[ 39.73 ]]

 [[ 39.97 ]]

 [[ 39.37 ]]

 [[ 39.31 ]]

 [[ 38.98 ]]

 [[ 38.71 ]]

 [[ 40.08 ]]

 [[ 39.13 ]]

 [[ 39.02 ]]

 [[ 38.69 ]]

 [[ 40.65 ]]

 [[ 43.74 ]]

 [[ 44.74 ]]

 [[ 45.08 ]]

 [[ 43.98 ]]

 [[ 43.59 ]]

 [[ 42.56 ]]

 [[ 39.2  ]]

 [[ 40.65 ]]

 [[ 40.51 ]]

 [[ 39.5  ]]

 [[ 37.53 ]]

 [[ 37.27 ]]

 [[ 34.82 ]]

 [[ 35.53 ]]

 [[ 35.21 ]]

 [[ 35.21 ]]

 [[ 35.35 ]]

 [[ 35.76 ]]

 [[ 35.75 ]]

 [[ 35.91 ]]

 [[ 36.59 ]]

 [[ 37.7  ]]

 [[ 37.85 ]]

 [[ 37.88 ]]

 [[ 37.81 ]]

 [[ 36.86 ]]

 [[ 36.63 ]]

 [[ 36.34 ]]

 [[ 36.87 ]]

 [[ 37.18 ]]

 [[ 36.52 ]]

 [[ 34.73 ]]

 [[ 35.8  ]]

 [[ 37.93 ]]

 [[ 39.   ]]

 [[ 37.92 ]]

 [[ 38.42 ]]

 [[ 37.39 ]]

 [[ 37.6  ]]

 [[ 37.04 ]]

 [[ 38.06 ]]

 [[ 39.   ]]

 [[ 40.81 ]]

 [[ 42.21 ]]

 [[ 42.08 ]]

 [[ 43.14 ]]

 [[ 42.87 ]]

 [[ 40.69 ]]

 [[ 39.7  ]]

 [[ 39.49 ]]

 [[ 38.97 ]]

 [[ 39.13 ]]

 [[ 38.64 ]]

 [[ 38.86 ]]

 [[ 39.24 ]]

 [[ 37.69 ]]

 [[ 37.09 ]]

 [[ 37.89 ]]

 [[ 38.91 ]]

 [[ 37.86 ]]

 [[ 36.85 ]]

 [[ 36.87 ]]

 [[ 36.6  ]]

 [[ 36.67 ]]

 [[ 36.78 ]]

 [[ 37.29 ]]

 [[ 38.9  ]]

 [[ 40.1  ]]

 [[ 39.68 ]]

 [[ 33.61 ]]

 [[ 34.47 ]]

 [[ 34.34 ]]

 [[ 34.97 ]]

 [[ 34.38 ]]

 [[ 34.09 ]]

 [[ 33.85 ]]

 [[ 33.82 ]]

 [[ 32.87 ]]

 [[ 32.82 ]]

 [[ 32.3  ]]

 [[ 32.14 ]]

 [[ 32.33 ]]

 [[ 31.65 ]]

 [[ 31.38 ]]

 [[ 31.69 ]]

 [[ 32.22 ]]

 [[ 32.64 ]]

 [[ 32.76 ]]

 [[ 33.09 ]]

 [[ 33.45 ]]

 [[ 33.   ]]

 [[ 32.49 ]]

 [[ 32.39 ]]

 [[ 31.58 ]]

 [[ 31.61 ]]

 [[ 30.96 ]]

 [[ 31.09 ]]

 [[ 31.42 ]]

 [[ 31.2  ]]

 [[ 31.8  ]]

 [[ 32.09 ]]

 [[ 33.65 ]]

 [[ 33.82 ]]

 [[ 34.36 ]]

 [[ 34.15 ]]

 [[ 35.24 ]]

 [[ 36.36 ]]

 [[ 36.31 ]]

 [[ 36.68 ]]

 [[ 36.85 ]]

 [[ 34.75 ]]

 [[ 35.89 ]]

 [[ 34.6  ]]

 [[ 34.68 ]]

 [[ 34.86 ]]

 [[ 34.87 ]]

 [[ 34.82 ]]

 [[ 34.96 ]]

 [[ 35.02 ]]

 [[ 34.   ]]

 [[ 34.19 ]]

 [[ 34.46 ]]

 [[ 34.54 ]]

 [[ 36.67 ]]

 [[ 35.77 ]]

 [[ 39.68 ]]

 [[ 38.29 ]]

 [[ 37.46 ]]

 [[ 37.28 ]]

 [[ 38.11 ]]

 [[ 37.28 ]]

 [[ 37.57 ]]

 [[ 44.4  ]]

 [[ 43.69 ]]

 [[ 42.73 ]]

 [[ 42.75 ]]

 [[ 41.65 ]]

 [[ 41.83 ]]

 [[ 42.05 ]]

 [[ 41.91 ]]

 [[ 41.69 ]]

 [[ 41.27 ]]

 [[ 41.92 ]]

 [[ 42.31 ]]

 [[ 40.79 ]]

 [[ 41.31 ]]

 [[ 40.74 ]]

 [[ 40.8  ]]

 [[ 41.29 ]]

 [[ 42.15 ]]

 [[ 42.47 ]]

 [[ 42.03 ]]

 [[ 41.47 ]]

 [[ 41.09 ]]

 [[ 41.83 ]]

 [[ 42.47 ]]

 [[ 41.97 ]]

 [[ 41.79 ]]

 [[ 42.46 ]]

 [[ 42.44 ]]

 [[ 42.43 ]]

 [[ 42.8  ]]

 [[ 42.51 ]]

 [[ 42.07 ]]

 [[ 42.15 ]]

 [[ 42.77 ]]

 [[ 41.65 ]]

 [[ 42.8  ]]

 [[ 43.82 ]]

 [[ 43.89 ]]

 [[ 44.15 ]]

 [[ 42.61 ]]

 [[ 42.06 ]]

 [[ 42.04 ]]

 [[ 42.64 ]]

 [[ 41.   ]]

 [[ 41.31 ]]

 [[ 42.58 ]]

 [[ 44.87 ]]

 [[ 44.27 ]]

 [[ 43.8  ]]

 [[ 43.58 ]]

 [[ 43.61 ]]

 [[ 42.89 ]]

 [[ 43.52 ]]

 [[ 42.84 ]]

 [[ 40.1  ]]

 [[ 40.08 ]]

 [[ 40.44 ]]

 [[ 42.26 ]]

 [[ 42.42 ]]

 [[ 42.28 ]]

 [[ 44.29 ]]

 [[ 45.77 ]]

 [[ 45.78 ]]

 [[ 45.2  ]]

 [[ 45.34 ]]

 [[ 44.49 ]]

 [[ 44.62 ]]

 [[ 43.25 ]]

 [[ 43.43 ]]

 [[ 44.79 ]]

 [[ 45.13 ]]

 [[ 44.23 ]]

 [[ 44.44 ]]

 [[ 44.37 ]]

 [[ 44.43 ]]

 [[ 44.44 ]]

 [[ 43.97 ]]

 [[ 43.7  ]]

 [[ 41.8  ]]

 [[ 42.9  ]]

 [[ 42.49 ]]

 [[ 43.   ]]

 [[ 41.47 ]]

 [[ 41.62 ]]

 [[ 42.56 ]]

 [[ 42.61 ]]

 [[ 42.78 ]]

 [[ 42.78 ]]

 [[ 42.65 ]]

 [[ 42.09 ]]

 [[ 44.03 ]]

 [[ 44.17 ]]

 [[ 43.63 ]]

 [[ 43.98 ]]

 [[ 44.18 ]]

 [[ 44.18 ]]

 [[ 42.92 ]]

 [[ 43.87 ]]

 [[ 43.85 ]]

 [[ 43.61 ]]

 [[ 42.57 ]]

 [[ 43.41 ]]

 [[ 43.44 ]]

 [[ 44.06 ]]

 [[ 44.13 ]]

 [[ 43.75 ]]

 [[ 43.89 ]]

 [[ 43.49 ]]

 [[ 42.79 ]]

 [[ 42.87 ]]

 [[ 42.73 ]]

 [[ 42.62 ]]

 [[ 43.16 ]]

 [[ 44.48 ]]

 [[ 43.72 ]]

 [[ 45.16 ]]

 [[ 45.57 ]]

 [[ 45.65 ]]

 [[ 46.35 ]]

 [[ 45.92 ]]

 [[ 44.92 ]]

 [[ 45.96 ]]

 [[ 46.29 ]]

 [[ 46.12 ]]

 [[ 45.77 ]]

 [[ 45.75 ]]

 [[ 45.66 ]]

 [[ 43.87 ]]

 [[ 43.38 ]]

 [[ 44.47 ]]

 [[ 44.02 ]]

 [[ 44.91 ]]

 [[ 45.29 ]]

 [[ 44.61 ]]

 [[ 44.28 ]]

 [[ 43.53 ]]

 [[ 43.34 ]]

 [[ 44.   ]]

 [[ 44.58 ]]

 [[ 44.88 ]]

 [[ 46.35 ]]

 [[ 46.98 ]]

 [[ 46.54 ]]

 [[ 46.7  ]]

 [[ 46.47 ]]

 [[ 45.8  ]]

 [[ 46.55 ]]

 [[ 46.62 ]]

 [[ 48.46 ]]

 [[ 47.64 ]]

 [[ 47.12 ]]

 [[ 46.3  ]]

 [[ 47.   ]]

 [[ 47.44 ]]

 [[ 46.67 ]]

 [[ 46.02 ]]

 [[ 45.47 ]]

 [[ 46.45 ]]

 [[ 45.94 ]]

 [[ 45.68 ]]

 [[ 45.99 ]]

 [[ 46.48 ]]

 [[ 46.68 ]]

 [[ 48.   ]]

 [[ 48.49 ]]

 [[ 47.77 ]]

 [[ 47.64 ]]

 [[ 46.25 ]]

 [[ 44.62 ]]

 [[ 45.72 ]]

 [[ 46.72 ]]

 [[ 45.34 ]]

 [[ 44.91 ]]

 [[ 46.   ]]

 [[ 45.74 ]]

 [[ 45.56 ]]

 [[ 46.36 ]]

 [[ 45.58 ]]

 [[ 45.35 ]]

 [[ 45.11 ]]

 [[ 44.41 ]]

 [[ 46.45 ]]

 [[ 46.37 ]]

 [[ 46.55 ]]

 [[ 47.3  ]]

 [[ 48.3  ]]

 [[ 49.54 ]]

 [[ 49.85 ]]

 [[ 48.4  ]]

 [[ 47.43 ]]

 [[ 47.19 ]]

 [[ 47.31 ]]

 [[ 46.49 ]]

 [[ 47.03 ]]

 [[ 47.34 ]]

 [[ 46.2  ]]

 [[ 46.67 ]]

 [[ 47.5  ]]

 [[ 47.68 ]]

 [[ 48.31 ]]]
[[[ 35.285]]

 [[ 35.34 ]]

 [[ 40.3  ]]

 [[ 40.09 ]]

 [[ 40.4  ]]

 [[ 40.1  ]]

 [[ 41.82 ]]

 [[ 41.91 ]]

 [[ 39.25 ]]

 [[ 38.75 ]]

 [[ 39.15 ]]

 [[ 38.94 ]]

 [[ 38.565]]

 [[ 37.97 ]]

 [[ 37.63 ]]

 [[ 38.59 ]]

 [[ 39.16 ]]

 [[ 39.03 ]]

 [[ 39.08 ]]

 [[ 38.07 ]]

 [[ 37.01 ]]

 [[ 37.01 ]]

 [[ 37.16 ]]

 [[ 37.87 ]]

 [[ 37.29 ]]

 [[ 39.5  ]]

 [[ 40.   ]]

 [[ 39.8  ]]

 [[ 39.35 ]]

 [[ 39.789]]

 [[ 39.84 ]]

 [[ 39.73 ]]

 [[ 39.97 ]]

 [[ 39.37 ]]

 [[ 39.31 ]]

 [[ 38.98 ]]

 [[ 38.71 ]]

 [[ 40.08 ]]

 [[ 39.13 ]]

 [[ 39.02 ]]

 [[ 38.69 ]]

 [[ 40.65 ]]

 [[ 43.74 ]]

 [[ 44.74 ]]

 [[ 45.08 ]]

 [[ 43.98 ]]

 [[ 43.59 ]]

 [[ 42.56 ]]

 [[ 39.2  ]]

 [[ 40.65 ]]

 [[ 40.51 ]]

 [[ 39.5  ]]

 [[ 37.53 ]]

 [[ 37.27 ]]

 [[ 34.82 ]]

 [[ 35.53 ]]

 [[ 35.21 ]]

 [[ 35.21 ]]

 [[ 35.35 ]]

 [[ 35.76 ]]

 [[ 35.75 ]]

 [[ 35.91 ]]

 [[ 36.59 ]]

 [[ 37.7  ]]

 [[ 37.85 ]]

 [[ 37.88 ]]

 [[ 37.81 ]]

 [[ 36.86 ]]

 [[ 36.63 ]]

 [[ 36.34 ]]

 [[ 36.87 ]]

 [[ 37.18 ]]

 [[ 36.52 ]]

 [[ 34.73 ]]

 [[ 35.8  ]]

 [[ 37.93 ]]

 [[ 39.   ]]

 [[ 37.92 ]]

 [[ 38.42 ]]

 [[ 37.39 ]]

 [[ 37.6  ]]

 [[ 37.04 ]]

 [[ 38.06 ]]

 [[ 39.   ]]

 [[ 40.81 ]]

 [[ 42.21 ]]

 [[ 42.08 ]]

 [[ 43.14 ]]

 [[ 42.87 ]]

 [[ 40.69 ]]

 [[ 39.7  ]]

 [[ 39.49 ]]

 [[ 38.97 ]]

 [[ 39.13 ]]

 [[ 38.64 ]]

 [[ 38.86 ]]

 [[ 39.24 ]]

 [[ 37.69 ]]

 [[ 37.09 ]]

 [[ 37.89 ]]

 [[ 38.91 ]]

 [[ 37.86 ]]

 [[ 36.85 ]]

 [[ 36.87 ]]

 [[ 36.6  ]]

 [[ 36.67 ]]

 [[ 36.78 ]]

 [[ 37.29 ]]

 [[ 38.9  ]]

 [[ 40.1  ]]

 [[ 39.68 ]]

 [[ 33.61 ]]

 [[ 34.47 ]]

 [[ 34.34 ]]

 [[ 34.97 ]]

 [[ 34.38 ]]

 [[ 34.09 ]]

 [[ 33.85 ]]

 [[ 33.82 ]]

 [[ 32.87 ]]

 [[ 32.82 ]]

 [[ 32.3  ]]

 [[ 32.14 ]]

 [[ 32.33 ]]

 [[ 31.65 ]]

 [[ 31.38 ]]

 [[ 31.69 ]]

 [[ 32.22 ]]

 [[ 32.64 ]]

 [[ 32.76 ]]

 [[ 33.09 ]]

 [[ 33.45 ]]

 [[ 33.   ]]

 [[ 32.49 ]]

 [[ 32.39 ]]

 [[ 31.58 ]]

 [[ 31.61 ]]

 [[ 30.96 ]]

 [[ 31.09 ]]

 [[ 31.42 ]]

 [[ 31.2  ]]

 [[ 31.8  ]]

 [[ 32.09 ]]

 [[ 33.65 ]]

 [[ 33.82 ]]

 [[ 34.36 ]]

 [[ 34.15 ]]

 [[ 35.24 ]]

 [[ 36.36 ]]

 [[ 36.31 ]]

 [[ 36.68 ]]

 [[ 36.85 ]]

 [[ 34.75 ]]

 [[ 35.89 ]]

 [[ 34.6  ]]

 [[ 34.68 ]]

 [[ 34.86 ]]

 [[ 34.87 ]]

 [[ 34.82 ]]

 [[ 34.96 ]]

 [[ 35.02 ]]

 [[ 34.   ]]

 [[ 34.19 ]]

 [[ 34.46 ]]

 [[ 34.54 ]]

 [[ 36.67 ]]

 [[ 35.77 ]]

 [[ 39.68 ]]

 [[ 38.29 ]]

 [[ 37.46 ]]

 [[ 37.28 ]]

 [[ 38.11 ]]

 [[ 37.28 ]]

 [[ 37.57 ]]

 [[ 44.4  ]]

 [[ 43.69 ]]

 [[ 42.73 ]]

 [[ 42.75 ]]

 [[ 41.65 ]]

 [[ 41.83 ]]

 [[ 42.05 ]]

 [[ 41.91 ]]

 [[ 41.69 ]]

 [[ 41.27 ]]

 [[ 41.92 ]]

 [[ 42.31 ]]

 [[ 40.79 ]]

 [[ 41.31 ]]

 [[ 40.74 ]]

 [[ 40.8  ]]

 [[ 41.29 ]]

 [[ 42.15 ]]

 [[ 42.47 ]]

 [[ 42.03 ]]

 [[ 41.47 ]]

 [[ 41.09 ]]

 [[ 41.83 ]]

 [[ 42.47 ]]

 [[ 41.97 ]]

 [[ 41.79 ]]

 [[ 42.46 ]]

 [[ 42.44 ]]

 [[ 42.43 ]]

 [[ 42.8  ]]

 [[ 42.51 ]]

 [[ 42.07 ]]

 [[ 42.15 ]]

 [[ 42.77 ]]

 [[ 41.65 ]]

 [[ 42.8  ]]

 [[ 43.82 ]]

 [[ 43.89 ]]

 [[ 44.15 ]]

 [[ 42.61 ]]

 [[ 42.06 ]]

 [[ 42.04 ]]

 [[ 42.64 ]]

 [[ 41.   ]]

 [[ 41.31 ]]

 [[ 42.58 ]]

 [[ 44.87 ]]

 [[ 44.27 ]]

 [[ 43.8  ]]

 [[ 43.58 ]]

 [[ 43.61 ]]

 [[ 42.89 ]]

 [[ 43.52 ]]

 [[ 42.84 ]]

 [[ 40.1  ]]

 [[ 40.08 ]]

 [[ 40.44 ]]

 [[ 42.26 ]]

 [[ 42.42 ]]

 [[ 42.28 ]]

 [[ 44.29 ]]

 [[ 45.77 ]]

 [[ 45.78 ]]

 [[ 45.2  ]]

 [[ 45.34 ]]

 [[ 44.49 ]]

 [[ 44.62 ]]

 [[ 43.25 ]]

 [[ 43.43 ]]

 [[ 44.79 ]]

 [[ 45.13 ]]

 [[ 44.23 ]]

 [[ 44.44 ]]

 [[ 44.37 ]]

 [[ 44.43 ]]

 [[ 44.44 ]]

 [[ 43.97 ]]

 [[ 43.7  ]]

 [[ 41.8  ]]

 [[ 42.9  ]]

 [[ 42.49 ]]

 [[ 43.   ]]

 [[ 41.47 ]]

 [[ 41.62 ]]

 [[ 42.56 ]]

 [[ 42.61 ]]

 [[ 42.78 ]]

 [[ 42.78 ]]

 [[ 42.65 ]]

 [[ 42.09 ]]

 [[ 44.03 ]]

 [[ 44.17 ]]

 [[ 43.63 ]]

 [[ 43.98 ]]

 [[ 44.18 ]]

 [[ 44.18 ]]

 [[ 42.92 ]]

 [[ 43.87 ]]

 [[ 43.85 ]]

 [[ 43.61 ]]

 [[ 42.57 ]]

 [[ 43.41 ]]

 [[ 43.44 ]]

 [[ 44.06 ]]

 [[ 44.13 ]]

 [[ 43.75 ]]

 [[ 43.89 ]]

 [[ 43.49 ]]

 [[ 42.79 ]]

 [[ 42.87 ]]

 [[ 42.73 ]]

 [[ 42.62 ]]

 [[ 43.16 ]]

 [[ 44.48 ]]

 [[ 43.72 ]]

 [[ 45.16 ]]

 [[ 45.57 ]]

 [[ 45.65 ]]

 [[ 46.35 ]]

 [[ 45.92 ]]

 [[ 44.92 ]]

 [[ 45.96 ]]

 [[ 46.29 ]]

 [[ 46.12 ]]

 [[ 45.77 ]]

 [[ 45.75 ]]

 [[ 45.66 ]]

 [[ 43.87 ]]

 [[ 43.38 ]]

 [[ 44.47 ]]

 [[ 44.02 ]]

 [[ 44.91 ]]

 [[ 45.29 ]]

 [[ 44.61 ]]

 [[ 44.28 ]]

 [[ 43.53 ]]

 [[ 43.34 ]]

 [[ 44.   ]]

 [[ 44.58 ]]

 [[ 44.88 ]]

 [[ 46.35 ]]

 [[ 46.98 ]]

 [[ 46.54 ]]

 [[ 46.7  ]]

 [[ 46.47 ]]

 [[ 45.8  ]]

 [[ 46.55 ]]

 [[ 46.62 ]]

 [[ 48.46 ]]

 [[ 47.64 ]]

 [[ 47.12 ]]

 [[ 46.3  ]]

 [[ 47.   ]]

 [[ 47.44 ]]

 [[ 46.67 ]]

 [[ 46.02 ]]

 [[ 45.47 ]]

 [[ 46.45 ]]

 [[ 45.94 ]]

 [[ 45.68 ]]

 [[ 45.99 ]]

 [[ 46.48 ]]

 [[ 46.68 ]]

 [[ 48.   ]]

 [[ 48.49 ]]

 [[ 47.77 ]]

 [[ 47.64 ]]

 [[ 46.25 ]]

 [[ 44.62 ]]

 [[ 45.72 ]]

 [[ 46.72 ]]

 [[ 45.34 ]]

 [[ 44.91 ]]

 [[ 46.   ]]

 [[ 45.74 ]]

 [[ 45.56 ]]

 [[ 46.36 ]]

 [[ 45.58 ]]

 [[ 45.35 ]]

 [[ 45.11 ]]

 [[ 44.41 ]]

 [[ 46.45 ]]

 [[ 46.37 ]]

 [[ 46.55 ]]

 [[ 47.3  ]]

 [[ 48.3  ]]

 [[ 49.54 ]]

 [[ 49.85 ]]

 [[ 48.4  ]]

 [[ 47.43 ]]

 [[ 47.19 ]]

 [[ 47.31 ]]

 [[ 46.49 ]]

 [[ 47.03 ]]

 [[ 47.34 ]]

 [[ 46.2  ]]

 [[ 46.67 ]]

 [[ 47.5  ]]

 [[ 47.68 ]]

 [[ 48.31 ]]

 [[ 47.8  ]]]
train data size is: 375
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
{'features': tensor([[[34.7700]],

        [[35.2850]],

        [[35.3400]],

        [[40.3000]],

        [[40.0900]],

        [[40.4000]],

        [[40.1000]],

        [[41.8200]],

        [[41.9100]],

        [[39.2500]],

        [[38.7500]],

        [[39.1500]],

        [[38.9400]],

        [[38.5650]],

        [[37.9700]],

        [[37.6300]],

        [[38.5900]],

        [[39.1600]],

        [[39.0300]],

        [[39.0800]],

        [[38.0700]],

        [[37.0100]],

        [[37.0100]],

        [[37.1600]],

        [[37.8700]],

        [[37.2900]],

        [[39.5000]],

        [[40.0000]],

        [[39.8000]],

        [[39.3500]],

        [[39.7890]],

        [[39.8400]]], dtype=torch.float64), 'value': tensor([[[35.2850]],

        [[35.3400]],

        [[40.3000]],

        [[40.0900]],

        [[40.4000]],

        [[40.1000]],

        [[41.8200]],

        [[41.9100]],

        [[39.2500]],

        [[38.7500]],

        [[39.1500]],

        [[38.9400]],

        [[38.5650]],

        [[37.9700]],

        [[37.6300]],

        [[38.5900]],

        [[39.1600]],

        [[39.0300]],

        [[39.0800]],

        [[38.0700]],

        [[37.0100]],

        [[37.0100]],

        [[37.1600]],

        [[37.8700]],

        [[37.2900]],

        [[39.5000]],

        [[40.0000]],

        [[39.8000]],

        [[39.3500]],

        [[39.7890]],

        [[39.8400]],

        [[39.7300]]], dtype=torch.float64)}
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 580, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 484, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 402, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 223, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 72, in Train
    xs, ys = Variable(xs), Variable(ys)
TypeError: Variable data has to be a tensor, but got str
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 06:51:42
INFO - ******************MLNX*******************
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 580, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 483, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 395, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 167, in ConstructTestData
    logging.debug("x_batch size: " + str(x_batches.shape))
AttributeError: 'NoneType' object has no attribute 'shape'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 06:52:21
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[[[ 35.285]]

 [[ 35.34 ]]

 [[ 40.3  ]]

 [[ 40.09 ]]

 [[ 40.4  ]]

 [[ 40.1  ]]

 [[ 41.82 ]]

 [[ 41.91 ]]

 [[ 39.25 ]]

 [[ 38.75 ]]

 [[ 39.15 ]]

 [[ 38.94 ]]

 [[ 38.565]]

 [[ 37.97 ]]

 [[ 37.63 ]]

 [[ 38.59 ]]

 [[ 39.16 ]]

 [[ 39.03 ]]

 [[ 39.08 ]]

 [[ 38.07 ]]

 [[ 37.01 ]]

 [[ 37.01 ]]

 [[ 37.16 ]]

 [[ 37.87 ]]

 [[ 37.29 ]]

 [[ 39.5  ]]

 [[ 40.   ]]

 [[ 39.8  ]]

 [[ 39.35 ]]

 [[ 39.789]]

 [[ 39.84 ]]

 [[ 39.73 ]]

 [[ 39.97 ]]

 [[ 39.37 ]]

 [[ 39.31 ]]

 [[ 38.98 ]]

 [[ 38.71 ]]

 [[ 40.08 ]]

 [[ 39.13 ]]

 [[ 39.02 ]]

 [[ 38.69 ]]

 [[ 40.65 ]]

 [[ 43.74 ]]

 [[ 44.74 ]]

 [[ 45.08 ]]

 [[ 43.98 ]]

 [[ 43.59 ]]

 [[ 42.56 ]]

 [[ 39.2  ]]

 [[ 40.65 ]]

 [[ 40.51 ]]

 [[ 39.5  ]]

 [[ 37.53 ]]

 [[ 37.27 ]]

 [[ 34.82 ]]

 [[ 35.53 ]]

 [[ 35.21 ]]

 [[ 35.21 ]]

 [[ 35.35 ]]

 [[ 35.76 ]]

 [[ 35.75 ]]

 [[ 35.91 ]]

 [[ 36.59 ]]

 [[ 37.7  ]]

 [[ 37.85 ]]

 [[ 37.88 ]]

 [[ 37.81 ]]

 [[ 36.86 ]]

 [[ 36.63 ]]

 [[ 36.34 ]]

 [[ 36.87 ]]

 [[ 37.18 ]]

 [[ 36.52 ]]

 [[ 34.73 ]]

 [[ 35.8  ]]

 [[ 37.93 ]]

 [[ 39.   ]]

 [[ 37.92 ]]

 [[ 38.42 ]]

 [[ 37.39 ]]

 [[ 37.6  ]]

 [[ 37.04 ]]

 [[ 38.06 ]]

 [[ 39.   ]]

 [[ 40.81 ]]

 [[ 42.21 ]]

 [[ 42.08 ]]

 [[ 43.14 ]]

 [[ 42.87 ]]

 [[ 40.69 ]]

 [[ 39.7  ]]

 [[ 39.49 ]]

 [[ 38.97 ]]

 [[ 39.13 ]]

 [[ 38.64 ]]

 [[ 38.86 ]]

 [[ 39.24 ]]

 [[ 37.69 ]]

 [[ 37.09 ]]

 [[ 37.89 ]]

 [[ 38.91 ]]

 [[ 37.86 ]]

 [[ 36.85 ]]

 [[ 36.87 ]]

 [[ 36.6  ]]

 [[ 36.67 ]]

 [[ 36.78 ]]

 [[ 37.29 ]]

 [[ 38.9  ]]

 [[ 40.1  ]]

 [[ 39.68 ]]

 [[ 33.61 ]]

 [[ 34.47 ]]

 [[ 34.34 ]]

 [[ 34.97 ]]

 [[ 34.38 ]]

 [[ 34.09 ]]

 [[ 33.85 ]]

 [[ 33.82 ]]

 [[ 32.87 ]]

 [[ 32.82 ]]

 [[ 32.3  ]]

 [[ 32.14 ]]

 [[ 32.33 ]]

 [[ 31.65 ]]

 [[ 31.38 ]]

 [[ 31.69 ]]

 [[ 32.22 ]]

 [[ 32.64 ]]

 [[ 32.76 ]]

 [[ 33.09 ]]

 [[ 33.45 ]]

 [[ 33.   ]]

 [[ 32.49 ]]

 [[ 32.39 ]]

 [[ 31.58 ]]

 [[ 31.61 ]]

 [[ 30.96 ]]

 [[ 31.09 ]]

 [[ 31.42 ]]

 [[ 31.2  ]]

 [[ 31.8  ]]

 [[ 32.09 ]]

 [[ 33.65 ]]

 [[ 33.82 ]]

 [[ 34.36 ]]

 [[ 34.15 ]]

 [[ 35.24 ]]

 [[ 36.36 ]]

 [[ 36.31 ]]

 [[ 36.68 ]]

 [[ 36.85 ]]

 [[ 34.75 ]]

 [[ 35.89 ]]

 [[ 34.6  ]]

 [[ 34.68 ]]

 [[ 34.86 ]]

 [[ 34.87 ]]

 [[ 34.82 ]]

 [[ 34.96 ]]

 [[ 35.02 ]]

 [[ 34.   ]]

 [[ 34.19 ]]

 [[ 34.46 ]]

 [[ 34.54 ]]

 [[ 36.67 ]]

 [[ 35.77 ]]

 [[ 39.68 ]]

 [[ 38.29 ]]

 [[ 37.46 ]]

 [[ 37.28 ]]

 [[ 38.11 ]]

 [[ 37.28 ]]

 [[ 37.57 ]]

 [[ 44.4  ]]

 [[ 43.69 ]]

 [[ 42.73 ]]

 [[ 42.75 ]]

 [[ 41.65 ]]

 [[ 41.83 ]]

 [[ 42.05 ]]

 [[ 41.91 ]]

 [[ 41.69 ]]

 [[ 41.27 ]]

 [[ 41.92 ]]

 [[ 42.31 ]]

 [[ 40.79 ]]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
{'features': tensor([[34.7700, 35.2850],
        [35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300]], dtype=torch.float64), 'value': tensor([[[35.2850]],

        [[35.3400]],

        [[40.3000]],

        [[40.0900]],

        [[40.4000]],

        [[40.1000]],

        [[41.8200]],

        [[41.9100]],

        [[39.2500]],

        [[38.7500]],

        [[39.1500]],

        [[38.9400]],

        [[38.5650]],

        [[37.9700]],

        [[37.6300]],

        [[38.5900]],

        [[39.1600]],

        [[39.0300]],

        [[39.0800]],

        [[38.0700]],

        [[37.0100]],

        [[37.0100]],

        [[37.1600]],

        [[37.8700]],

        [[37.2900]],

        [[39.5000]],

        [[40.0000]],

        [[39.8000]],

        [[39.3500]],

        [[39.7890]],

        [[39.8400]],

        [[39.7300]]], dtype=torch.float64)}
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 580, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 483, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 401, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 222, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 72, in Train
    xs, ys = Variable(xs), Variable(ys)
TypeError: Variable data has to be a tensor, but got str
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 06:55:47
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[ 35.285  40.3    40.4    41.82   39.25   39.15   38.565  37.63   39.16
  39.08   37.01   37.16   37.29   40.     39.35   39.84   39.97   39.31
  38.71   39.13   38.69   43.74   45.08   43.59   39.2    40.51   37.53
  34.82   35.21   35.35   35.75   36.59   37.85   37.81   36.63   36.87
  36.52   35.8    39.     38.42   37.6    38.06   40.81   42.08   42.87
  39.7    38.97   38.64   39.24   37.09   38.91   36.85   36.6    36.78
  38.9    39.68   34.47   34.97   34.09   33.82   32.82   32.14   31.65
  31.69   32.64   33.09   33.     32.39   31.61   31.09   31.2    32.09
  33.82   34.15   36.36   36.68   34.75   34.6    34.86   34.82   35.02
  34.19   34.54   35.77   38.29   37.28   37.28   44.4    42.73   41.65
  42.05   41.69   41.92   40.79   40.74   41.29   42.47   41.47   41.83
  41.97   42.46   42.43   42.51   42.15   41.65   43.82   44.15   42.06
  42.64   41.31   44.87   43.8    43.61   43.52   40.1    40.44   42.42
  44.29   45.78   45.34   44.62   43.43   45.13   44.44   44.43   43.97
  41.8    42.49   41.47   42.56   42.78   42.65   44.03   43.63   44.18
  42.92   43.85   42.57   43.44   44.13   43.89   42.79   42.73   43.16
  43.72   45.57   46.35   44.92   46.29   45.77   45.66   43.38   44.02
  45.29   44.28   43.34   44.58   46.35   46.54   46.47   46.55   48.46
  47.12   47.     46.67   45.47   45.94   45.99   46.68   48.49   47.64
  44.62   46.72   44.91   45.74   46.36   45.35   44.41   46.37   47.3
  49.54   48.4    47.19   46.49   47.34   46.67   47.68 ]
train data size is: 187
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 581, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 484, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 402, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 221, in TrainSimpleRnn
    output_size = y_train.shape[1]
IndexError: tuple index out of range
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 06:58:19
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[ 40.3    40.09   40.4    40.1    41.82   41.91   39.25   38.75   39.15
  38.94   38.565  37.97   37.63   38.59   39.16   39.03   39.08   38.07
  37.01   37.01   37.16   37.87   37.29   39.5    40.     39.8    39.35
  39.789  39.84   39.73   39.97   39.37   39.31   38.98   38.71   40.08
  39.13   39.02   38.69   40.65   43.74   44.74   45.08   43.98   43.59
  42.56   39.2    40.65   40.51   39.5    37.53   37.27   34.82   35.53
  35.21   35.21   35.35   35.76   35.75   35.91   36.59   37.7    37.85
  37.88   37.81   36.86   36.63   36.34   36.87   37.18   36.52   34.73
  35.8    37.93   39.     37.92   38.42   37.39   37.6    37.04   38.06
  39.     40.81   42.21   42.08   43.14   42.87   40.69   39.7    39.49
  38.97   39.13   38.64   38.86   39.24   37.69   37.09   37.89   38.91
  37.86   36.85   36.87   36.6    36.67   36.78   37.29   38.9    40.1
  39.68   33.61   34.47   34.34   34.97   34.38   34.09   33.85   33.82
  32.87   32.82   32.3    32.14   32.33   31.65   31.38   31.69   32.22
  32.64   32.76   33.09   33.45   33.     32.49   32.39   31.58   31.61
  30.96   31.09   31.42   31.2    31.8    32.09   33.65   33.82   34.36
  34.15   35.24   36.36   36.31   36.68   36.85   34.75   35.89   34.6
  34.68   34.86   34.87   34.82   34.96   35.02   34.     34.19   34.46
  34.54   36.67   35.77   39.68   38.29   37.46   37.28   38.11   37.28
  37.57   44.4    43.69   42.73   42.75   41.65   41.83   42.05   41.91
  41.69   41.27   41.92   42.31   40.79   41.31   40.74 ]
train data size is: 187
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 581, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 484, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 402, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 221, in TrainSimpleRnn
    output_size = y_train.shape[1]
IndexError: tuple index out of range
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 07:00:47
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[[ 35.34 ]
 [ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
{'features': tensor([[34.7700, 35.2850],
        [35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300]], dtype=torch.float64), 'value': tensor([[35.3400],
        [40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700]], dtype=torch.float64)}
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 72, in Train
    xs, ys = Variable(xs), Variable(ys)
TypeError: Variable data has to be a tensor, but got str
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 07:11:50
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[[ 35.34 ]
 [ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
{'features': tensor([[34.7700, 35.2850],
        [35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300]], dtype=torch.float64), 'value': tensor([[35.3400],
        [40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700]], dtype=torch.float64)}
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 72, in Train
    xs, ys = xs.values(), ys.values()
AttributeError: 'str' object has no attribute 'values'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 07:12:58
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[[ 35.34 ]
 [ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 70, in Train
    xs, ys = data.features, data.value
AttributeError: 'dict' object has no attribute 'features'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/12/18 07:14:07
INFO - ******************MLNX*******************
[[ 34.77   35.285]
 [ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]]
[[ 35.34 ]
 [ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
{'features': tensor([[34.7700, 35.2850],
        [35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300]], dtype=torch.float64), 'value': tensor([[35.3400],
        [40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700]], dtype=torch.float64)}
tensor([[34.7700, 35.2850],
        [35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300]], dtype=torch.float64) tensor([[35.3400],
        [40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700]], dtype=torch.float64)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 75, in Train
    y_pred = model(xs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 34, in forward
    out, self.h_0 = self.rnn(x, self.h_0)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\rnn.py", line 192, in forward
    output, hidden = func(input, self.all_weights, hx, batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 324, in forward
    return func(input, *fargs, **fkwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 244, in forward
    nexth, output = func(input, hidden, weight, batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 87, in forward
    hy, output = inner(input, hidden[l], weight[l], batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 116, in forward
    hidden = inner(input[i], hidden, *weight)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 17, in RNNReLUCell
    hy = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1024, in linear
    return torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of type torch.FloatTensor but found type torch.DoubleTensor for argument #4 'mat1'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:13:58
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]], dtype=torch.float64) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]], dtype=torch.float64)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 73, in Train
    y_pred = model(xs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 34, in forward
    out, self.h_0 = self.rnn(x, self.h_0)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\rnn.py", line 192, in forward
    output, hidden = func(input, self.all_weights, hx, batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 324, in forward
    return func(input, *fargs, **fkwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 244, in forward
    nexth, output = func(input, hidden, weight, batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 87, in forward
    hy, output = inner(input, hidden[l], weight[l], batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 116, in forward
    hidden = inner(input[i], hidden, *weight)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 17, in RNNReLUCell
    hy = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1024, in linear
    return torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of type torch.FloatTensor but found type torch.DoubleTensor for argument #4 'mat1'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:17:45
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]], dtype=torch.float64) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]], dtype=torch.float64)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 76, in Train
    loss = criterion(y_pred, ys)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\loss.py", line 421, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1716, in mse_loss
    return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1674, in _pointwise_loss
    return lambd_optimized(input, target, reduction)
RuntimeError: input and target shapes do not match: input [1 x 32 x 1], target [32 x 1] at c:\programdata\miniconda3\conda-bld\pytorch-cpu_1532498166916\work\aten\src\thnn\generic/MSECriterion.c:12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:19:44
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
train data for SimpleRnn x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]])
y_pred is: 
tensor([[[-0.7539],
         [-0.8144],
         [-0.8535],
         [-0.8132],
         [-0.8382],
         [-0.8449],
         [-0.8716],
         [-0.8568],
         [-0.8015],
         [-0.8012],
         [-0.8117],
         [-0.8070],
         [-0.7988],
         [-0.7883],
         [-0.7850],
         [-0.8053],
         [-0.8144],
         [-0.8103],
         [-0.8100],
         [-0.7867],
         [-0.7702],
         [-0.7727],
         [-0.7774],
         [-0.7903],
         [-0.7804],
         [-0.8252],
         [-0.8306],
         [-0.8239],
         [-0.8155],
         [-0.8253],
         [-0.8252],
         [-0.8226]]], grad_fn=<ThAddBackward>)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 79, in Train
    loss = criterion(y_pred, ys)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\loss.py", line 421, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1716, in mse_loss
    return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1674, in _pointwise_loss
    return lambd_optimized(input, target, reduction)
RuntimeError: input and target shapes do not match: input [1 x 32 x 1], target [32 x 1] at c:\programdata\miniconda3\conda-bld\pytorch-cpu_1532498166916\work\aten\src\thnn\generic/MSECriterion.c:12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:21:49
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
train data for SimpleRnn x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]])
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
TypeError: reshape() got an unexpected keyword argument 'order'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 77, in Train
    y_pred = np.reshape(y_pred, (32,1))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 232, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 67, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 47, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\numeric.py", line 531, in asarray
    return array(a, dtype, copy=False, order=order)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\tensor.py", line 397, in __array__
    return self.cpu().numpy()
RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:23:57
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]], dtype=torch.float64)
y_pred is: 
tensor([[[0.4682],
         [0.4065],
         [0.3305],
         [0.3072],
         [0.3010],
         [0.3086],
         [0.3242],
         [0.2848],
         [0.2629],
         [0.2918],
         [0.2892],
         [0.2828],
         [0.2772],
         [0.2723],
         [0.2831],
         [0.2946],
         [0.2900],
         [0.2885],
         [0.2800],
         [0.2609],
         [0.2643],
         [0.2721],
         [0.2781],
         [0.2761],
         [0.2893],
         [0.3100],
         [0.2942],
         [0.2917],
         [0.2926],
         [0.2979],
         [0.2938],
         [0.2970]]], grad_fn=<ThAddBackward>)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 81, in Train
    loss = criterion(y_pred, ys)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\loss.py", line 421, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1716, in mse_loss
    return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1674, in _pointwise_loss
    return lambd_optimized(input, target, reduction)
RuntimeError: Expected object of type torch.FloatTensor but found type torch.DoubleTensor for argument #2 'target'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:25:48
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]])
y_pred is: 
tensor([[-0.8142],
        [-0.5780],
        [-0.7313],
        [-0.9368],
        [-0.9048],
        [-0.9129],
        [-0.8685],
        [-0.8020],
        [-0.8104],
        [-0.8040],
        [-0.7805],
        [-0.7859],
        [-0.7778],
        [-0.7705],
        [-0.7917],
        [-0.7914],
        [-0.7972],
        [-0.8078],
        [-0.7816],
        [-0.7588],
        [-0.7590],
        [-0.7525],
        [-0.7672],
        [-0.7532],
        [-0.8204],
        [-0.8041],
        [-0.8196],
        [-0.8180],
        [-0.8258],
        [-0.8176],
        [-0.8158],
        [-0.8244]], grad_fn=<ThAddBackward>)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 83, in Train
    loss.backward(retain_graph=True)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\autograd\__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function MseLossBackward returned an invalid gradient at index 0 - expected shape [1, 32, 1] but got [32, 1]
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:27:44
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[40.3000],
        [40.0900],
        [40.4000],
        [40.1000],
        [41.8200],
        [41.9100],
        [39.2500],
        [38.7500],
        [39.1500],
        [38.9400],
        [38.5650],
        [37.9700],
        [37.6300],
        [38.5900],
        [39.1600],
        [39.0300],
        [39.0800],
        [38.0700],
        [37.0100],
        [37.0100],
        [37.1600],
        [37.8700],
        [37.2900],
        [39.5000],
        [40.0000],
        [39.8000],
        [39.3500],
        [39.7890],
        [39.8400],
        [39.7300],
        [39.9700],
        [39.3700]])
y_pred is: 
tensor([[[-0.1215],
         [ 0.6583],
         [ 1.5703],
         [ 1.8855],
         [ 1.9288],
         [ 1.8997],
         [ 1.8998],
         [ 1.9705],
         [ 1.9594],
         [ 1.8933],
         [ 1.8447],
         [ 1.8310],
         [ 1.8354],
         [ 1.8212],
         [ 1.7851],
         [ 1.7805],
         [ 1.8080],
         [ 1.8335],
         [ 1.8551],
         [ 1.8442],
         [ 1.7966],
         [ 1.7545],
         [ 1.7305],
         [ 1.7518],
         [ 1.7434],
         [ 1.7831],
         [ 1.8390],
         [ 1.8793],
         [ 1.8803],
         [ 1.8717],
         [ 1.8724],
         [ 1.8743]]], grad_fn=<ThAddBackward>)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 81, in Train
    loss = criterion(y_pred, ys)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\loss.py", line 421, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1716, in mse_loss
    return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1674, in _pointwise_loss
    return lambd_optimized(input, target, reduction)
RuntimeError: input and target shapes do not match: input [1 x 32 x 1], target [32 x 1] at c:\programdata\miniconda3\conda-bld\pytorch-cpu_1532498166916\work\aten\src\thnn\generic/MSECriterion.c:12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:29:27
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[0.0370],
         [0.4431],
         [0.7732],
         [0.9511],
         [0.9199],
         [0.9286],
         [0.9386],
         [0.9499],
         [0.9632],
         [0.9348],
         [0.9043],
         [0.8993],
         [0.8988],
         [0.8918],
         [0.8817],
         [0.8772],
         [0.8853],
         [0.8981],
         [0.9021],
         [0.8971],
         [0.8871],
         [0.8660],
         [0.8573],
         [0.8601],
         [0.8681],
         [0.8771],
         [0.8955],
         [0.9163],
         [0.9196],
         [0.9150],
         [0.9146],
         [0.9183]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:84: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[ 37.2018],
         [ 76.7887],
         [123.8497],
         [174.4080],
         [225.8568],
         [277.3787],
         [328.1802],
         [377.3844],
         [425.0040],
         [471.9647],
         [518.6978],
         [565.0463],
         [610.0280],
         [652.9368],
         [693.2127],
         [729.9764],
         [763.0394],
         [793.0248],
         [820.3483],
         [844.6178],
         [865.7001],
         [883.4772],
         [898.1978],
         [910.4406],
         [920.8316],
         [929.7050],
         [937.4787],
         [944.4412],
         [950.7532],
         [956.6524],
         [962.5618],
         [968.7211]]], grad_fn=<ThAddBackward>)
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[488.5045],
         [ 68.8576],
         [ 15.8629],
         [ 58.5155],
         [ 33.2050],
         [ 17.6693],
         [ 11.9114],
         [ 13.3530],
         [ 15.7203],
         [ 16.5859],
         [ 17.2451],
         [ 18.3673],
         [ 18.7695],
         [ 18.0687],
         [ 17.1078],
         [ 16.5201],
         [ 16.4800],
         [ 16.8773],
         [ 17.7288],
         [ 18.8942],
         [ 20.0782],
         [ 20.5174],
         [ 20.2106],
         [ 19.7756],
         [ 18.9201],
         [ 17.6740],
         [ 16.9515],
         [ 17.0131],
         [ 17.4577],
         [ 17.7730],
         [ 17.7853],
         [ 17.8229]]], grad_fn=<ThAddBackward>)
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[-3.7876],
         [-3.3050],
         [-2.4676],
         [-1.3110],
         [-0.8224],
         [-2.2354],
         [-2.4410],
         [-2.1433],
         [-1.9118],
         [-1.7796],
         [-1.8624],
         [-1.8812],
         [-1.8680],
         [-2.0975],
         [-2.4368],
         [-2.2521],
         [-1.8181],
         [-1.4801],
         [-1.6314],
         [-1.9992],
         [-1.9719],
         [-1.8582],
         [-1.8097],
         [-1.8016],
         [-1.7775],
         [-1.7565],
         [-1.7038],
         [-1.7145],
         [-1.7512],
         [-1.6966],
         [-1.5904],
         [-1.5722]]], grad_fn=<ThAddBackward>)
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[2.5615],
         [2.6451],
         [3.7536],
         [3.4950],
         [2.3363],
         [2.2912],
         [2.5551],
         [2.7726],
         [2.6791],
         [2.4906],
         [2.5491],
         [2.6395],
         [2.6813],
         [2.7160],
         [2.8935],
         [2.9372],
         [2.7945],
         [2.7043],
         [2.8973],
         [3.2336],
         [3.1418],
         [2.8560],
         [2.8820],
         [2.7772],
         [2.8269],
         [2.9219],
         [2.8379],
         [2.8354],
         [2.8992],
         [2.8767],
         [2.8481],
         [2.8697]]], grad_fn=<ThAddBackward>)
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
TypeError: reshape() got an unexpected keyword argument 'order'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 75, in Train
    ys.data = np.reshape(ys.data, (1,32,1))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 232, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 67, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 47, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
ValueError: cannot reshape array of size 27 into shape (1,32,1)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:32:36
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 76, in Train
    print(ys.get_shape())
AttributeError: 'Tensor' object has no attribute 'get_shape'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:33:34
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
torch.Size([32, 1])
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[-0.5587],
         [-0.4685],
         [-0.5809],
         [-0.6846],
         [-0.6483],
         [-0.6657],
         [-0.6497],
         [-0.6536],
         [-0.6376],
         [-0.6323],
         [-0.6207],
         [-0.6187],
         [-0.6174],
         [-0.6110],
         [-0.6087],
         [-0.6130],
         [-0.6174],
         [-0.6221],
         [-0.6214],
         [-0.6112],
         [-0.6026],
         [-0.5967],
         [-0.5956],
         [-0.5995],
         [-0.6068],
         [-0.6162],
         [-0.6286],
         [-0.6324],
         [-0.6332],
         [-0.6320],
         [-0.6317],
         [-0.6337]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:86: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: batch num: 1
torch.Size([32, 1])
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[10.7505],
         [12.5616],
         [13.2659],
         [13.8061],
         [14.1058],
         [14.3850],
         [14.4802],
         [14.4998],
         [14.5286],
         [14.7997],
         [15.3113],
         [15.7938],
         [16.1195],
         [16.2191],
         [16.1905],
         [15.9731],
         [15.5428],
         [15.3380],
         [15.1901],
         [14.9542],
         [14.6067],
         [14.2627],
         [13.8275],
         [13.5752],
         [13.4009],
         [13.2951],
         [13.2573],
         [13.2759],
         [13.3018],
         [13.3535],
         [13.4821],
         [13.6798]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
torch.Size([32, 1])
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[  53.5118],
         [  93.3985],
         [ 137.7306],
         [ 184.8391],
         [ 233.2018],
         [ 281.9272],
         [ 330.5984],
         [ 378.5107],
         [ 424.7231],
         [ 468.9090],
         [ 511.5492],
         [ 553.3462],
         [ 594.1711],
         [ 633.6686],
         [ 671.6547],
         [ 707.8593],
         [ 742.2487],
         [ 774.8878],
         [ 806.2037],
         [ 836.6398],
         [ 866.6702],
         [ 896.3211],
         [ 925.4511],
         [ 954.0388],
         [ 981.4914],
         [1007.0928],
         [1030.7316],
         [1052.5219],
         [1072.5559],
         [1090.9661],
         [1107.8369],
         [1123.4325]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
torch.Size([32, 1])
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[1143.1565],
         [ 885.7433],
         [ 703.8635],
         [ 569.9478],
         [ 472.2792],
         [ 401.8137],
         [ 350.4724],
         [ 313.2950],
         [ 286.5137],
         [ 267.4012],
         [ 254.0229],
         [ 245.3218],
         [ 240.7051],
         [ 238.8585],
         [ 236.7027],
         [ 232.0202],
         [ 227.0304],
         [ 222.7135],
         [ 219.3466],
         [ 216.4861],
         [ 213.9779],
         [ 211.8076],
         [ 209.7919],
         [ 207.6363],
         [ 205.5080],
         [ 203.3824],
         [ 201.4785],
         [ 199.8274],
         [ 198.1112],
         [ 196.5097],
         [ 195.4321],
         [ 195.0729]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
torch.Size([32, 1])
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[133.4957],
         [ 68.3045],
         [ 42.9430],
         [ 38.9304],
         [ 41.0989],
         [ 42.8591],
         [ 43.0737],
         [ 42.4122],
         [ 41.8122],
         [ 41.3460],
         [ 41.3872],
         [ 41.6580],
         [ 41.8847],
         [ 42.3488],
         [ 43.2357],
         [ 44.5449],
         [ 45.4373],
         [ 45.8279],
         [ 46.0306],
         [ 46.9404],
         [ 48.0509],
         [ 48.6161],
         [ 48.8628],
         [ 48.3168],
         [ 47.1033],
         [ 46.6503],
         [ 46.0886],
         [ 45.9297],
         [ 46.1022],
         [ 46.2545],
         [ 46.3066],
         [ 46.3882]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
torch.Size([27, 1])
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
TypeError: reshape() got an unexpected keyword argument 'order'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 77, in Train
    ys.data = np.reshape(ys.data, (1,32,1))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 232, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 67, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 47, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
ValueError: cannot reshape array of size 27 into shape (1,32,1)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:34:28
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
32
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[ 0.7395],
         [ 0.1423],
         [-0.0710],
         [-0.2198],
         [-0.2776],
         [-0.3010],
         [-0.3101],
         [-0.3879],
         [-0.3380],
         [-0.2800],
         [-0.3107],
         [-0.3179],
         [-0.3113],
         [-0.3071],
         [-0.2790],
         [-0.2811],
         [-0.3097],
         [-0.3054],
         [-0.3201],
         [-0.3305],
         [-0.2968],
         [-0.2857],
         [-0.2823],
         [-0.3014],
         [-0.2636],
         [-0.2691],
         [-0.3278],
         [-0.3167],
         [-0.3006],
         [-0.3029],
         [-0.3165],
         [-0.3042]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:87: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: batch num: 1
32
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[   44.2473],
         [   97.8232],
         [  161.8465],
         [  238.7587],
         [  332.4512],
         [  444.0674],
         [  574.3402],
         [  725.1350],
         [  899.6208],
         [ 1102.1642],
         [ 1335.1707],
         [ 1602.5433],
         [ 1908.0596],
         [ 2256.1328],
         [ 2651.8887],
         [ 3100.4492],
         [ 3608.7666],
         [ 4185.3154],
         [ 4838.8389],
         [ 5578.7891],
         [ 6416.5420],
         [ 7364.7407],
         [ 8438.2217],
         [ 9654.0713],
         [11031.4688],
         [12592.0635],
         [14360.6123],
         [16364.9951],
         [18636.7754],
         [21211.9023],
         [24131.3125],
         [27441.0918]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
32
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[12194.5752],
         [  615.9258],
         [  -86.1418],
         [  798.7940],
         [  332.6801],
         [    5.8526],
         [   26.5605],
         [   49.5022],
         [   53.6839],
         [   47.9343],
         [   47.0789],
         [   49.5347],
         [   51.4969],
         [   51.5356],
         [   50.8292],
         [   50.1757],
         [   49.7828],
         [   49.9389],
         [   50.9305],
         [   52.6902],
         [   54.8302],
         [   56.2127],
         [   56.8785],
         [   57.2211],
         [   56.2984],
         [   54.3150],
         [   52.8150],
         [   52.1820],
         [   52.0683],
         [   51.9850],
         [   51.8503],
         [   52.0098]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
32
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[ -6.7774],
         [-11.8738],
         [ -6.7310],
         [ -0.6731],
         [  1.2934],
         [ -1.2771],
         [ -2.6672],
         [ -1.9515],
         [ -1.0741],
         [ -0.9036],
         [ -1.0429],
         [ -0.8103],
         [ -0.4387],
         [ -0.7893],
         [ -2.7660],
         [ -3.3176],
         [ -1.8428],
         [ -0.6230],
         [ -0.7317],
         [ -1.3837],
         [ -1.5923],
         [ -1.3488],
         [ -1.3153],
         [ -1.4020],
         [ -1.3847],
         [ -1.3161],
         [ -1.1386],
         [ -1.1672],
         [ -1.3353],
         [ -1.2094],
         [ -0.8755],
         [ -0.7098]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
32
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[-0.5877],
         [ 6.5951],
         [ 9.1700],
         [ 3.7842],
         [ 2.3134],
         [ 5.1860],
         [ 6.3444],
         [ 4.8037],
         [ 4.0308],
         [ 4.7034],
         [ 5.4361],
         [ 5.1797],
         [ 4.8368],
         [ 5.1159],
         [ 5.7600],
         [ 5.8517],
         [ 5.3560],
         [ 5.1974],
         [ 5.6363],
         [ 6.3240],
         [ 6.0730],
         [ 5.5216],
         [ 5.6680],
         [ 5.3196],
         [ 5.1923],
         [ 5.5280],
         [ 5.3833],
         [ 5.4230],
         [ 5.5952],
         [ 5.5291],
         [ 5.4549],
         [ 5.5234]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
27
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[ 9.2517],
         [14.3685],
         [12.3254],
         [ 9.1182],
         [10.6222],
         [12.9252],
         [13.1189],
         [12.3178],
         [11.1386],
         [11.2268],
         [12.3292],
         [12.3221],
         [11.4890],
         [13.8689],
         [16.0366],
         [13.6321],
         [11.7545],
         [12.7870],
         [13.7618],
         [13.4789],
         [12.9118],
         [12.8815],
         [13.0296],
         [13.2030],
         [13.4751],
         [12.9271],
         [12.4478]]], grad_fn=<ThAddBackward>)
[  7.39457011e-01   1.42282188e-01  -7.09729269e-02  -2.19837904e-01
  -2.77637839e-01  -3.00996333e-01  -3.10119420e-01  -3.87937725e-01
  -3.38036478e-01  -2.80011773e-01  -3.10684085e-01  -3.17923486e-01
  -3.11290145e-01  -3.07065547e-01  -2.79035628e-01  -2.81135023e-01
  -3.09671164e-01  -3.05414081e-01  -3.20053160e-01  -3.30495894e-01
  -2.96776414e-01  -2.85721004e-01  -2.82326281e-01  -3.01354975e-01
  -2.63574719e-01  -2.69143343e-01  -3.27782214e-01  -3.16671848e-01
  -3.00553262e-01  -3.02895486e-01  -3.16481531e-01  -3.04160357e-01
   4.42473335e+01   9.78232117e+01   1.61846542e+02   2.38758713e+02
   3.32451202e+02   4.44067383e+02   5.74340210e+02   7.25135010e+02
   8.99620789e+02   1.10216418e+03   1.33517065e+03   1.60254333e+03
   1.90805957e+03   2.25613281e+03   2.65188867e+03   3.10044922e+03
   3.60876660e+03   4.18531543e+03   4.83883887e+03   5.57878906e+03
   6.41654199e+03   7.36474072e+03   8.43822168e+03   9.65407129e+03
   1.10314688e+04   1.25920635e+04   1.43606123e+04   1.63649951e+04
   1.86367754e+04   2.12119023e+04   2.41313125e+04   2.74410918e+04
   1.21945752e+04   6.15925781e+02  -8.61417618e+01   7.98794006e+02
   3.32680054e+02   5.85264587e+00   2.65605087e+01   4.95022392e+01
   5.36839485e+01   4.79343491e+01   4.70788612e+01   4.95346870e+01
   5.14969139e+01   5.15355797e+01   5.08291893e+01   5.01757088e+01
   4.97828407e+01   4.99388695e+01   5.09304886e+01   5.26901894e+01
   5.48302345e+01   5.62126808e+01   5.68784828e+01   5.72211266e+01
   5.62983589e+01   5.43150444e+01   5.28150444e+01   5.21819534e+01
   5.20683174e+01   5.19849777e+01   5.18503075e+01   5.20097847e+01
  -6.77743387e+00  -1.18737736e+01  -6.73104334e+00  -6.73116505e-01
   1.29342330e+00  -1.27712953e+00  -2.66723084e+00  -1.95151722e+00
  -1.07406271e+00  -9.03620064e-01  -1.04293978e+00  -8.10251534e-01
  -4.38671440e-01  -7.89258540e-01  -2.76595426e+00  -3.31762815e+00
  -1.84279501e+00  -6.23011410e-01  -7.31715024e-01  -1.38374209e+00
  -1.59230936e+00  -1.34876955e+00  -1.31530106e+00  -1.40196884e+00
  -1.38469923e+00  -1.31605399e+00  -1.13864219e+00  -1.16715217e+00
  -1.33525705e+00  -1.20942962e+00  -8.75527203e-01  -7.09820330e-01
  -5.87717652e-01   6.59513235e+00   9.16998100e+00   3.78423285e+00
   2.31342673e+00   5.18597031e+00   6.34443045e+00   4.80370092e+00
   4.03082800e+00   4.70340776e+00   5.43605185e+00   5.17970657e+00
   4.83677244e+00   5.11585093e+00   5.75996161e+00   5.85165834e+00
   5.35596609e+00   5.19744253e+00   5.63628531e+00   6.32396078e+00
   6.07301521e+00   5.52158689e+00   5.66801310e+00   5.31955099e+00
   5.19228411e+00   5.52803564e+00   5.38331366e+00   5.42299318e+00
   5.59520149e+00   5.52908134e+00   5.45488501e+00   5.52340555e+00
   9.25168228e+00   1.43684683e+01   1.23253546e+01   9.11819935e+00
   1.06221533e+01   1.29251747e+01   1.31189356e+01   1.23177538e+01
   1.11385708e+01   1.12267637e+01   1.23292131e+01   1.23221140e+01
   1.14890413e+01   1.38688974e+01   1.60366173e+01   1.36321144e+01
   1.17545300e+01   1.27869987e+01   1.37617884e+01   1.34788914e+01
   1.29117527e+01   1.28815441e+01   1.30296011e+01   1.32029886e+01
   1.34751368e+01   1.29271355e+01   1.24478159e+01]
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
32
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[12.0501],
         [18.6133],
         [19.7120],
         [14.6978],
         [14.3952],
         [18.3684],
         [18.9759],
         [15.5112],
         [14.3781],
         [16.7093],
         [17.3063],
         [15.5039],
         [14.9023],
         [15.8081],
         [16.3538],
         [16.1950],
         [15.9498],
         [16.0543],
         [15.9264],
         [15.2243],
         [15.0282],
         [15.5203],
         [15.8477],
         [15.5035],
         [15.8757],
         [16.9230],
         [16.7108],
         [15.8738],
         [16.1413],
         [16.8206],
         [16.5758],
         [16.1974]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
32
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[12.6452],
         [17.3841],
         [17.7205],
         [13.4248],
         [14.6988],
         [17.6216],
         [15.5218],
         [13.8092],
         [16.3981],
         [18.5826],
         [17.3011],
         [16.5182],
         [17.7237],
         [17.7712],
         [16.4643],
         [15.3053],
         [16.0272],
         [16.9801],
         [15.5298],
         [14.1793],
         [14.9875],
         [14.7598],
         [13.5453],
         [14.0103],
         [14.5109],
         [13.9921],
         [13.9237],
         [14.4140],
         [14.3935],
         [14.3373],
         [14.9602],
         [15.2310]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
32
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[14.2129],
         [17.9304],
         [16.5107],
         [13.5890],
         [15.4934],
         [16.9388],
         [15.2719],
         [14.6676],
         [15.3271],
         [15.3950],
         [15.8058],
         [16.4188],
         [15.9676],
         [15.8503],
         [16.1965],
         [15.9219],
         [15.6260],
         [16.0181],
         [16.6394],
         [17.0222],
         [17.5421],
         [17.6825],
         [17.8778],
         [18.1828],
         [17.2821],
         [16.5222],
         [16.9735],
         [16.8477],
         [16.2958],
         [16.3401],
         [16.5578],
         [16.6153]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
32
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[13.7652],
         [17.9558],
         [17.7073],
         [13.9493],
         [15.4980],
         [17.3162],
         [15.5079],
         [14.5566],
         [15.9819],
         [16.1216],
         [15.1937],
         [16.0695],
         [17.2603],
         [16.5491],
         [14.3506],
         [14.6189],
         [15.7981],
         [14.5534],
         [13.9256],
         [14.8317],
         [14.7842],
         [14.0443],
         [13.9759],
         [14.2106],
         [13.8918],
         [13.5215],
         [13.8077],
         [13.7071],
         [13.2334],
         [13.4755],
         [13.8516],
         [13.7492]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
32
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[15.9271],
         [18.4996],
         [16.0205],
         [14.0339],
         [16.9417],
         [17.8124],
         [14.6620],
         [14.3014],
         [16.8707],
         [16.2386],
         [14.0390],
         [15.0236],
         [16.8865],
         [15.7962],
         [14.9603],
         [16.7301],
         [17.5845],
         [16.2393],
         [16.3344],
         [18.2641],
         [18.1476],
         [16.9212],
         [17.7975],
         [18.2918],
         [17.3682],
         [16.8317],
         [17.2601],
         [17.5994],
         [16.9503],
         [16.7203],
         [17.4224],
         [17.4857]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
27
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ 12.0500679   18.61325836  19.7119503   14.69781971  14.39518547
  18.36839485  18.97587967  15.51116467  14.37806702  16.70934677
  17.30628014  15.50385475  14.902318    15.80807114  16.35379791
  16.19501877  15.94976234  16.05433273  15.9264307   15.22428703
  15.02824783  15.52025318  15.84769058  15.50352287  15.87568283
  16.92297745  16.71077347  15.873806    16.14125824  16.82059479
  16.57575798  16.19736671  12.64517975  17.38410759  17.72049713
  13.42481613  14.69879055  17.62162781  15.52180386  13.80917168
  16.39810562  18.58259583  17.30111885  16.51822662  17.72372246
  17.77117729  16.46429825  15.30534935  16.02724457  16.98013878
  15.52984619  14.17934227  14.98754311  14.75977421  13.54525185
  14.01034164  14.51089954  13.99207401  13.92365074  14.41396236
  14.39353371  14.33726215  14.96021271  15.23102188  14.21287346
  17.9303894   16.5106945   13.58896732  15.49340439  16.93881035
  15.27194691  14.66761112  15.32711887  15.39499283  15.8057642
  16.41881943  15.96764374  15.85026646  16.19648552  15.92193699
  15.62596416  16.01808357  16.63941193  17.02219009  17.54209137
  17.68253708  17.87776566  18.18283463  17.28206253  16.52219391
  16.97348595  16.84774208  16.29582405  16.34013748  16.55777931
  16.61528397  13.76521206  17.95581436  17.70729828  13.94932365
  15.49802876  17.31620026  15.50794888  14.55662155  15.98190022
  16.1216011   15.19372845  16.06953239  17.26026917  16.54909325
  14.35062313  14.61889839  15.79811954  14.55342293  13.92557049
  14.83172321  14.78417969  14.04430485  13.97589207  14.21055508
  13.89175129  13.52149105  13.80767536  13.70712948  13.23343182
  13.47549057  13.85159874  13.74918556  15.92712402  18.49957848
  16.02052498  14.033885    16.94173622  17.81241989  14.66196823
  14.30143166  16.87066269  16.23856735  14.03896141  15.0235815
  16.88653946  15.79615021  14.96033955  16.73007774  17.58451653
  16.23934746  16.33440781  18.26407623  18.14758301  16.92118645
  17.79747772  18.2918396   17.36824989  16.8316555   17.26005936
  17.5994072   16.95033264  16.72031021  17.42237091  17.48570633
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan]
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
32
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
32
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
32
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
32
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
32
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
27
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan]
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
32
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
32
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
32
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
32
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
32
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
27
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan]
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:38:39
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[-0.9489],
         [-0.9069],
         [-1.1419],
         [-1.1809],
         [-1.1976],
         [-1.1995],
         [-1.2299],
         [-1.2225],
         [-1.1668],
         [-1.1357],
         [-1.1463],
         [-1.1473],
         [-1.1298],
         [-1.1136],
         [-1.1136],
         [-1.1360],
         [-1.1535],
         [-1.1528],
         [-1.1409],
         [-1.1150],
         [-1.0875],
         [-1.0827],
         [-1.0970],
         [-1.1060],
         [-1.1194],
         [-1.1589],
         [-1.1832],
         [-1.1733],
         [-1.1636],
         [-1.1707],
         [-1.1753],
         [-1.1745]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:87: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[25.6568],
         [42.5090],
         [54.4281],
         [62.1024],
         [67.8300],
         [71.7595],
         [74.4406],
         [76.0680],
         [77.2817],
         [78.8197],
         [80.9408],
         [83.2561],
         [85.1454],
         [86.3804],
         [86.9249],
         [86.5696],
         [85.6098],
         [84.6257],
         [83.7749],
         [82.6505],
         [81.2509],
         [79.5349],
         [77.7431],
         [76.1390],
         [74.8927],
         [73.9427],
         [73.3281],
         [72.9932],
         [72.8519],
         [72.9009],
         [73.2635],
         [73.8987]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[ 8.0177],
         [-2.2566],
         [ 1.3139],
         [ 2.4730],
         [ 0.6063],
         [ 0.3541],
         [ 0.8403],
         [ 1.0161],
         [ 0.7565],
         [ 0.7461],
         [ 0.9862],
         [ 1.1282],
         [ 0.9595],
         [ 0.8297],
         [ 0.8201],
         [ 0.8778],
         [ 0.8622],
         [ 0.9127],
         [ 0.9882],
         [ 1.0822],
         [ 1.1205],
         [ 1.0549],
         [ 1.0134],
         [ 0.9992],
         [ 0.8889],
         [ 0.7728],
         [ 0.8163],
         [ 0.9099],
         [ 0.9309],
         [ 0.8935],
         [ 0.8990],
         [ 0.9341]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[ 7.3581],
         [10.0698],
         [10.5492],
         [10.7858],
         [11.0872],
         [10.9360],
         [10.8243],
         [10.8504],
         [10.8281],
         [10.8032],
         [10.8807],
         [11.1376],
         [11.4722],
         [11.5868],
         [10.8299],
         [10.2530],
         [10.2730],
         [10.3986],
         [10.2622],
         [10.1148],
         [10.0762],
         [10.0568],
         [ 9.9149],
         [ 9.7862],
         [ 9.7020],
         [ 9.6270],
         [ 9.6023],
         [ 9.5320],
         [ 9.4172],
         [ 9.4026],
         [ 9.5062],
         [ 9.6151]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[ 26.8958],
         [ 40.0719],
         [ 51.2953],
         [ 61.8685],
         [ 71.8975],
         [ 80.7456],
         [ 88.6918],
         [ 95.9688],
         [102.4653],
         [108.3840],
         [113.9351],
         [119.0595],
         [123.8848],
         [128.4949],
         [133.1938],
         [137.8197],
         [142.2599],
         [146.3751],
         [150.4244],
         [154.6113],
         [158.6643],
         [162.4703],
         [166.0752],
         [168.9528],
         [171.5120],
         [173.7233],
         [175.6279],
         [177.3610],
         [179.0249],
         [180.5534],
         [181.9625],
         [183.3122]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[161.7214],
         [129.8489],
         [107.2163],
         [ 90.1970],
         [ 80.9499],
         [ 75.0672],
         [ 71.9432],
         [ 70.5527],
         [ 69.0073],
         [ 67.6126],
         [ 67.0320],
         [ 66.6181],
         [ 66.1740],
         [ 68.2645],
         [ 71.2117],
         [ 72.4603],
         [ 72.9049],
         [ 72.9833],
         [ 72.8324],
         [ 72.8112],
         [ 72.8439],
         [ 72.7427],
         [ 72.4667],
         [ 72.4118],
         [ 72.6898],
         [ 72.4559],
         [ 72.0552]]], grad_fn=<ThAddBackward>)
[  -0.94891942   -0.90687835   -1.14190209   -1.1809181    -1.19756496
   -1.19952524   -1.22987282   -1.22249138   -1.16675305   -1.13569295
   -1.14625919   -1.14729702   -1.12975168   -1.11355543   -1.1135987
   -1.13603342   -1.15353525   -1.15279031   -1.14092374   -1.1150049
   -1.08745134   -1.08270621   -1.09703958   -1.1060431    -1.1194104
   -1.15885842   -1.18321395   -1.17333162   -1.16361034   -1.17073119
   -1.17530787   -1.17453194   25.65682983   42.50899887   54.42810059
   62.10237885   67.82996368   71.75946808   74.44057465   76.06802368
   77.28170013   78.81967926   80.9407959    83.25611877   85.14539337
   86.38037872   86.92487335   86.56958771   85.60984039   84.62572479
   83.77494812   82.65048981   81.25092316   79.5348587    77.74310303
   76.13904572   74.89270782   73.9426651    73.32813263   72.99318695
   72.85193634   72.90085602   73.26346588   73.89870453    8.01768303
   -2.25663114    1.31394374    2.47295976    0.60627031    0.35408777
    0.84028596    1.01606596    0.75650829    0.74611181    0.98621643
    1.12815654    0.95946872    0.82973349    0.82010376    0.87776935
    0.86221159    0.91274357    0.98817307    1.08219504    1.12046528
    1.0549134     1.01342213    0.99923742    0.88891745    0.77281225
    0.81634438    0.90991551    0.9309414     0.89353716    0.89896667
    0.9340539     7.35810709   10.06979752   10.54923725   10.78581047
   11.08723259   10.93602276   10.82430458   10.85036469   10.8281002
   10.80323601   10.88070011   11.13760376   11.47222519   11.58676147
   10.82987118   10.25298786   10.2729969    10.3985939    10.26220131
   10.1147995    10.07619762   10.05683517    9.91494656    9.78615379
    9.70204163    9.62704277    9.60231781    9.53195763    9.41719246
    9.40264511    9.50619698    9.61507988   26.89580917   40.07193756
   51.29532623   61.86848831   71.89752197   80.74555206   88.69178009
   95.96881866  102.46527863  108.38398743  113.935112    119.05950165
  123.88483429  128.49487305  133.19380188  137.81968689  142.25991821
  146.37510681  150.42442322  154.61125183  158.66429138  162.47026062
  166.07522583  168.95283508  171.51200867  173.7233429   175.62794495
  177.36100769  179.02488708  180.5533905   181.96250916  183.31216431
  161.72135925  129.84887695  107.21632385   90.19699097   80.94994354
   75.06719208   71.94319916   70.55271149   69.00728607   67.61263275
   67.03195953   66.61814117   66.17403412   68.2644577    71.21172333
   72.46028137   72.90488434   72.98325348   72.83242798   72.811203
   72.84391022   72.74265289   72.46669769   72.41176605   72.68979645
   72.45592499   72.05517578]
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[39.6171],
         [33.1731],
         [33.7306],
         [33.0965],
         [32.9624],
         [33.6330],
         [34.2723],
         [33.4831],
         [32.3171],
         [32.1442],
         [32.2152],
         [31.9912],
         [31.6253],
         [31.2808],
         [31.4437],
         [31.9738],
         [32.1905],
         [32.1788],
         [31.8610],
         [31.1377],
         [30.6800],
         [30.6787],
         [30.9675],
         [31.0293],
         [31.5959],
         [32.5635],
         [32.8179],
         [32.5961],
         [32.5915],
         [32.7629],
         [32.7633],
         [32.7957]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[37.5437],
         [39.3897],
         [38.8794],
         [38.6082],
         [39.1852],
         [39.3588],
         [38.9032],
         [38.6557],
         [39.4134],
         [41.6887],
         [43.6439],
         [44.3270],
         [43.9971],
         [43.3440],
         [42.6627],
         [40.6789],
         [39.7211],
         [40.2414],
         [39.8180],
         [38.3857],
         [37.3486],
         [36.0871],
         [35.2667],
         [35.3555],
         [35.2757],
         [35.2842],
         [35.5513],
         [35.7436],
         [35.8171],
         [36.1926],
         [37.0192],
         [37.6216]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[57.8443],
         [63.1986],
         [62.2885],
         [61.8338],
         [61.5204],
         [61.4890],
         [62.0524],
         [61.9487],
         [60.2853],
         [59.5416],
         [61.5258],
         [63.9953],
         [64.3519],
         [64.0425],
         [63.6578],
         [63.1039],
         [62.7082],
         [63.0005],
         [64.2993],
         [66.3617],
         [68.7962],
         [70.0682],
         [70.8995],
         [71.5610],
         [70.1058],
         [67.7349],
         [66.5542],
         [65.9062],
         [65.5147],
         [65.2116],
         [65.0086],
         [65.3405]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[64.2233],
         [64.6698],
         [64.5798],
         [65.6954],
         [65.8845],
         [64.3181],
         [63.4697],
         [63.2716],
         [63.0715],
         [63.1616],
         [63.6637],
         [65.2951],
         [67.5102],
         [68.2296],
         [63.3146],
         [59.2368],
         [59.4767],
         [59.9177],
         [59.7332],
         [59.1679],
         [58.7243],
         [58.4930],
         [57.7110],
         [56.9212],
         [56.4198],
         [55.8912],
         [55.8470],
         [55.4716],
         [54.7285],
         [54.7070],
         [55.3555],
         [56.0874]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[40.3595],
         [44.2715],
         [44.1292],
         [43.3345],
         [43.1489],
         [42.7795],
         [42.0704],
         [41.6936],
         [41.2880],
         [41.0030],
         [41.3849],
         [41.3857],
         [41.6662],
         [42.2666],
         [43.5539],
         [44.5100],
         [44.8725],
         [45.0193],
         [45.7182],
         [47.1890],
         [47.6594],
         [47.8006],
         [48.1983],
         [46.6511],
         [46.3323],
         [46.2355],
         [45.4079],
         [45.6973],
         [45.8465],
         [45.7250],
         [45.8343],
         [45.9713]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[21.8360],
         [32.4540],
         [27.9768],
         [28.8021],
         [30.6677],
         [30.2480],
         [32.2725],
         [32.6778],
         [31.1090],
         [31.2310],
         [31.7412],
         [31.3688],
         [31.2538],
         [35.8996],
         [36.9627],
         [35.1999],
         [35.5058],
         [34.8365],
         [34.5728],
         [34.9786],
         [34.8435],
         [34.6477],
         [34.3747],
         [34.6926],
         [35.1538],
         [34.1738],
         [34.1043]]], grad_fn=<ThAddBackward>)
[ 39.61712646  33.17307281  33.73055649  33.09645081  32.96236801
  33.63301849  34.27230835  33.48313904  32.31710815  32.1441803
  32.21517944  31.99116135  31.62529182  31.28076935  31.44365692
  31.97377014  32.19052505  32.1788063   31.86100578  31.13768005
  30.67997169  30.67873573  30.96748352  31.02933884  31.59592056
  32.56354141  32.81787109  32.59608841  32.5914917   32.76292419
  32.76334763  32.7957077   37.54368591  39.3897171   38.87937927
  38.60820007  39.18520737  39.35876846  38.9031601   38.65568542
  39.41342163  41.68871307  43.64391327  44.32703018  43.99713898
  43.34399796  42.66268921  40.67890549  39.721138    40.24143219
  39.81802368  38.38571167  37.34858322  36.08709717  35.26667023
  35.35549164  35.27568817  35.28424454  35.55132294  35.74363327
  35.81714249  36.19261932  37.01916885  37.62160873  57.84427643
  63.19856262  62.28847885  61.83381653  61.52044678  61.48901749
  62.05243683  61.94870758  60.28534698  59.54164124  61.52577209
  63.99525833  64.35186768  64.04249573  63.65776825  63.10388184
  62.7081604   63.00054169  64.29930115  66.36174774  68.79621887
  70.06819916  70.89949799  71.56100464  70.10575104  67.73487854
  66.55418396  65.90621185  65.51473999  65.21160126  65.00862122
  65.34051514  64.22329712  64.66981506  64.57975769  65.69540405
  65.88454437  64.31811523  63.46972275  63.27160263  63.07154083
  63.16162491  63.66365814  65.29512787  67.51016998  68.22963715
  63.31463242  59.2367897   59.47674179  59.91773987  59.73319626
  59.16785812  58.7242775   58.49295425  57.71104813  56.92121506
  56.41982651  55.89124298  55.84699631  55.47157288  54.72847366
  54.70703888  55.35552216  56.08738708  40.35950851  44.27145004
  44.1292305   43.33446503  43.14892197  42.77953339  42.07040405
  41.69363785  41.28804016  41.00297165  41.38490677  41.38568878
  41.66617203  42.26663589  43.55389023  44.50995636  44.87250137  45.01931
  45.71821976  47.18902969  47.65937042  47.80063248  48.19829178
  46.65114212  46.33227158  46.23547363  45.40793991  45.69734192
  45.84654999  45.72497177  45.83432007  45.97132111  21.83601952
  32.45399094  27.97679138  28.80207062  30.6677475   30.24798203
  32.27248001  32.67780304  31.10897446  31.23098183  31.74123764
  31.36878014  31.25383949  35.8996315   36.96274948  35.19987869
  35.50580978  34.83648682  34.57279587  34.97862244  34.84353638
  34.64770126  34.37467575  34.69263077  35.15380478  34.17376328
  34.10427856]
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[20.2230],
         [33.0869],
         [28.0221],
         [29.7301],
         [29.5522],
         [30.3988],
         [30.7848],
         [28.8554],
         [28.3089],
         [28.8360],
         [28.5915],
         [28.3247],
         [27.9569],
         [27.6749],
         [28.3157],
         [28.8069],
         [28.6494],
         [28.6791],
         [28.0637],
         [27.2270],
         [27.2503],
         [27.4009],
         [27.8209],
         [27.5439],
         [28.8061],
         [29.5280],
         [29.0903],
         [28.9207],
         [29.1394],
         [29.2634],
         [29.1175],
         [29.3010]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[33.2287],
         [30.7742],
         [31.9539],
         [31.1181],
         [32.3454],
         [31.7103],
         [31.3809],
         [31.3734],
         [32.6631],
         [35.2091],
         [35.9233],
         [36.0562],
         [35.3503],
         [34.8984],
         [34.2759],
         [31.6373],
         [32.5628],
         [32.9407],
         [31.6955],
         [30.4616],
         [30.1014],
         [28.4600],
         [28.6533],
         [28.8642],
         [28.4773],
         [28.8158],
         [29.0306],
         [29.0693],
         [29.1408],
         [29.6954],
         [30.5331],
         [30.6764]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[44.3536],
         [36.1811],
         [40.0079],
         [37.5994],
         [38.1982],
         [38.2617],
         [38.8953],
         [38.0620],
         [36.4421],
         [37.2161],
         [39.5747],
         [40.5053],
         [39.5390],
         [39.8288],
         [39.1762],
         [38.9644],
         [38.7927],
         [39.4382],
         [40.6356],
         [42.1764],
         [43.7327],
         [43.5715],
         [44.4716],
         [44.4932],
         [42.2050],
         [41.1614],
         [41.1018],
         [40.5166],
         [40.6133],
         [40.2783],
         [40.3087],
         [40.8226]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[41.8939],
         [39.0900],
         [41.6418],
         [41.7988],
         [41.0938],
         [39.7827],
         [40.0446],
         [39.6759],
         [39.7223],
         [39.8942],
         [40.3684],
         [42.0237],
         [43.3117],
         [42.8040],
         [36.6939],
         [37.0995],
         [37.9297],
         [37.6388],
         [37.6281],
         [37.0016],
         [36.9396],
         [36.8133],
         [35.9225],
         [35.7424],
         [35.3947],
         [35.0521],
         [35.3725],
         [34.6503],
         [34.2983],
         [34.7172],
         [35.2177],
         [35.6455]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[28.8750],
         [33.6115],
         [31.2136],
         [32.3119],
         [30.8499],
         [31.5589],
         [30.1881],
         [30.6873],
         [29.8164],
         [30.1085],
         [30.3890],
         [30.1493],
         [30.7162],
         [31.0456],
         [32.4363],
         [32.5823],
         [32.9420],
         [32.8602],
         [33.8187],
         [34.9552],
         [34.6667],
         [35.0944],
         [35.2788],
         [33.1103],
         [34.4359],
         [33.2521],
         [33.0806],
         [33.6424],
         [33.3575],
         [33.4201],
         [33.5464],
         [33.5952]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[27.6700],
         [30.7145],
         [29.5173],
         [30.2997],
         [31.8583],
         [30.9879],
         [34.6702],
         [32.8479],
         [31.9643],
         [32.5326],
         [32.8891],
         [32.0636],
         [32.5270],
         [39.3494],
         [36.8177],
         [36.3652],
         [36.9304],
         [35.4261],
         [35.9657],
         [36.2040],
         [35.8265],
         [35.7972],
         [35.3714],
         [36.1157],
         [36.3328],
         [34.7154],
         [35.6779]]], grad_fn=<ThAddBackward>)
[ 20.22296715  33.08694077  28.0220623   29.7301445   29.5521946
  30.39879799  30.78484154  28.85544968  28.30890846  28.83601761
  28.59151268  28.32466316  27.95687675  27.67493629  28.31565285
  28.80692863  28.64942169  28.67905998  28.06370354  27.22695923
  27.25032043  27.40087128  27.82086945  27.5439167   28.8060627
  29.52795219  29.0902729   28.92065811  29.13937187  29.26340675
  29.1174984   29.30099487  33.22869873  30.77423286  31.95385361
  31.11810493  32.34536743  31.71034622  31.38092613  31.37336349
  32.66314316  35.20909882  35.92333984  36.05622482  35.35027313
  34.8984375   34.27590942  31.63730621  32.56281281  32.94074249
  31.69553757  30.46160889  30.10139275  28.46002007  28.65330887
  28.86418915  28.47732544  28.8157692   29.03058243  29.06931686
  29.14082336  29.69541931  30.53307724  30.67636871  44.35355759
  36.18114471  40.00787354  37.59941483  38.19816971  38.26167679
  38.89531326  38.06200409  36.44205856  37.21609879  39.57473755
  40.5052948   39.53899765  39.82881927  39.17621613  38.96444321
  38.79270935  39.43818283  40.63560867  42.17642975  43.73265839
  43.57148361  44.47160721  44.49321747  42.20502853  41.16136551
  41.10181427  40.51663589  40.61333466  40.27832413  40.30868149
  40.82262802  41.89392853  39.09002304  41.64183044  41.79880905
  41.09375763  39.78267288  40.04462433  39.67589188  39.72230911
  39.89418411  40.36844254  42.02365875  43.31171036  42.80398178
  36.69392776  37.09952545  37.92974854  37.63879013  37.62806702
  37.00163651  36.93955612  36.81332016  35.92250061  35.7424469
  35.39471436  35.05213165  35.3724823   34.65031052  34.29829025
  34.71719742  35.21774673  35.64545059  28.87503242  33.61148453
  31.21362305  32.31187439  30.84993172  31.55893517  30.18810463
  30.68730545  29.81635284  30.10850525  30.38904953  30.14929962
  30.71622467  31.04556656  32.43627548  32.5823288   32.94200516
  32.8602066   33.81866837  34.95524216  34.66669846  35.09441757
  35.27876663  33.11033249  34.43591309  33.25207901  33.08056641
  33.6423645   33.35752106  33.42012787  33.54639053  33.59524155
  27.67000008  30.71445656  29.51734543  30.29967117  31.85825348
  30.98788071  34.67016602  32.84791565  31.96429634  32.53264618
  32.88913727  32.06356049  32.52699661  39.34940338  36.81766129
  36.36520386  36.93037415  35.4261055   35.96573639  36.20404434
  35.82648468  35.79717255  35.37139511  36.11572266  36.33280945
  34.71543884  35.67793274]
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[20.7549],
         [41.2392],
         [24.4778],
         [38.1080],
         [27.2625],
         [38.0894],
         [29.5158],
         [34.2262],
         [28.2636],
         [34.2968],
         [28.6375],
         [33.1407],
         [28.4906],
         [31.9417],
         [29.8264],
         [32.7733],
         [30.0777],
         [32.4866],
         [29.3756],
         [30.4762],
         [29.2176],
         [30.5728],
         [30.0798],
         [30.2827],
         [32.0020],
         [32.2496],
         [31.5428],
         [31.7801],
         [31.8689],
         [32.0386],
         [31.7226],
         [32.2408]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ 20.75494385  41.23916626  24.47776031  38.10804749  27.26246262
  38.08939362  29.51582527  34.22618866  28.26356697  34.29677582
  28.6374836   33.14066315  28.49058723  31.94168854  29.82644653
  32.77327728  30.07769966  32.48656845  29.37557793  30.47622871
  29.21755219  30.57283974  30.0797596   30.28272438  32.00201416
  32.24961472  31.54284668  31.78009796  31.86890984  32.0385704
  31.72260094  32.24082565          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan          nan          nan          nan
          nan          nan]
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan]
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan]
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan
  nan  nan  nan  nan  nan  nan  nan]
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan],
         [nan]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:46:19
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[-0.8406],
         [-1.0386],
         [-1.2162],
         [-1.2988],
         [-1.3373],
         [-1.3395],
         [-1.3727],
         [-1.3774],
         [-1.3381],
         [-1.2978],
         [-1.2957],
         [-1.2957],
         [-1.2834],
         [-1.2657],
         [-1.2561],
         [-1.2712],
         [-1.2908],
         [-1.2958],
         [-1.2924],
         [-1.2714],
         [-1.2421],
         [-1.2294],
         [-1.2328],
         [-1.2452],
         [-1.2480],
         [-1.2854],
         [-1.3164],
         [-1.3215],
         [-1.3120],
         [-1.3166],
         [-1.3222],
         [-1.3220]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:87: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[-0.8537],
         [-0.7455],
         [-0.7261],
         [-0.7213],
         [-0.7194],
         [-0.7241],
         [-0.7314],
         [-0.7219],
         [-0.7184],
         [-0.7416],
         [-0.7919],
         [-0.8265],
         [-0.8347],
         [-0.8309],
         [-0.8122],
         [-0.7820],
         [-0.7563],
         [-0.7451],
         [-0.7433],
         [-0.7313],
         [-0.7126],
         [-0.6717],
         [-0.6597],
         [-0.6452],
         [-0.6446],
         [-0.6446],
         [-0.6466],
         [-0.6485],
         [-0.6544],
         [-0.6580],
         [-0.6669],
         [-0.6830]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[-0.2694],
         [-0.1797],
         [-0.1770],
         [-0.1773],
         [-0.1696],
         [-0.1729],
         [-0.1695],
         [-0.1718],
         [-0.1724],
         [-0.1834],
         [-0.1784],
         [-0.1717],
         [-0.1806],
         [-0.1899],
         [-0.1742],
         [-0.1812],
         [-0.1735],
         [-0.1858],
         [-0.1798],
         [-0.1935],
         [-0.1969],
         [-0.2011],
         [-0.2150],
         [-0.2057],
         [-0.2079],
         [-0.2058],
         [-0.1954],
         [-0.1861],
         [-0.1898],
         [-0.1838],
         [-0.1874],
         [-0.1850]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[0.1844],
         [0.2410],
         [0.2436],
         [0.2552],
         [0.2704],
         [0.2523],
         [0.2471],
         [0.2536],
         [0.2487],
         [0.2497],
         [0.2493],
         [0.2483],
         [0.2626],
         [0.2737],
         [0.2233],
         [0.2099],
         [0.2428],
         [0.2389],
         [0.2440],
         [0.2366],
         [0.2372],
         [0.2358],
         [0.2402],
         [0.2272],
         [0.2327],
         [0.2269],
         [0.2251],
         [0.2323],
         [0.2233],
         [0.2204],
         [0.2238],
         [0.2278]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[0.4988],
         [0.5535],
         [0.5646],
         [0.5661],
         [0.5597],
         [0.5574],
         [0.5527],
         [0.5445],
         [0.5431],
         [0.5361],
         [0.5409],
         [0.5406],
         [0.5407],
         [0.5495],
         [0.5590],
         [0.5737],
         [0.5739],
         [0.5807],
         [0.5855],
         [0.6038],
         [0.6091],
         [0.6089],
         [0.6172],
         [0.6057],
         [0.5940],
         [0.6048],
         [0.5852],
         [0.5920],
         [0.5918],
         [0.5897],
         [0.5907],
         [0.5928]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[0.8442],
         [0.8932],
         [0.9044],
         [0.9073],
         [0.9229],
         [0.9455],
         [0.9567],
         [1.0070],
         [0.9768],
         [0.9773],
         [0.9841],
         [0.9858],
         [0.9745],
         [1.0286],
         [1.1099],
         [1.0815],
         [1.0924],
         [1.0941],
         [1.0802],
         [1.0835],
         [1.0822],
         [1.0773],
         [1.0727],
         [1.0724],
         [1.0831],
         [1.0736],
         [1.0588]]], grad_fn=<ThAddBackward>)
[-0.84055662 -1.03862667 -1.21618879 -1.29879165 -1.33731747 -1.33951104
 -1.37266278 -1.37744749 -1.33810389 -1.29783034 -1.29570484 -1.29570961
 -1.28336334 -1.26570892 -1.25609541 -1.27123833 -1.2907542  -1.29577065
 -1.2924124  -1.27137423 -1.24205053 -1.22937584 -1.23275661 -1.24521327
 -1.24803019 -1.28537154 -1.31641698 -1.32149994 -1.31202364 -1.31662548
 -1.32215798 -1.32204676 -0.85372627 -0.74550283 -0.72609079 -0.7213189
 -0.71935499 -0.72406244 -0.73140228 -0.72185409 -0.71842134 -0.74162042
 -0.79190618 -0.82647812 -0.83467698 -0.83090043 -0.81221867 -0.78196037
 -0.75625712 -0.7450828  -0.74331838 -0.73130345 -0.7126348  -0.67172164
 -0.65970904 -0.64517939 -0.64456052 -0.6445936  -0.64661014 -0.64854085
 -0.65439236 -0.65799397 -0.66688162 -0.68300003 -0.26941285 -0.17966342
 -0.17703566 -0.17730057 -0.16957647 -0.17290506 -0.16954464 -0.17182976
 -0.17243636 -0.18344852 -0.17836946 -0.17166141 -0.18058646 -0.18988961
 -0.17420757 -0.18123838 -0.17353597 -0.18578872 -0.17977962 -0.1935195
 -0.19693375 -0.20107886 -0.21502396 -0.20568052 -0.20792103 -0.20583501
 -0.19536322 -0.18608248 -0.18979222 -0.18382224 -0.1874333  -0.18497372
  0.18440303  0.24097607  0.24362436  0.2552079   0.27044749  0.25225025
  0.24713767  0.25357878  0.24868456  0.24965787  0.24931702  0.24832276
  0.26258186  0.27368906  0.22329098  0.20993775  0.24278495  0.23885706
  0.24400744  0.23661998  0.23717976  0.23575723  0.24017537  0.22719085
  0.23265889  0.22686607  0.22512871  0.23225576  0.22325724  0.22041532
  0.22379079  0.2277652   0.49883878  0.5534693   0.56464213  0.56609726
  0.55974269  0.55738544  0.55266792  0.54451394  0.5431264   0.53614116
  0.54090255  0.54060894  0.54066908  0.54948503  0.55900955  0.57371467
  0.57393396  0.58074564  0.58552814  0.60379565  0.60912061  0.60889196
  0.61721879  0.60568905  0.5939787   0.6047709   0.58520144  0.59200048
  0.5917967   0.58974522  0.59068716  0.59278518  0.84422499  0.89319974
  0.90439481  0.907327    0.92286569  0.94546145  0.9567154   1.0070349
  0.97679752  0.97726017  0.98408556  0.98579162  0.97450542  1.02861607
  1.10989404  1.08154082  1.09241796  1.09407687  1.08019876  1.08352733
  1.08216071  1.07734919  1.07271576  1.07236052  1.08306885  1.07359242
  1.05875516]
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[1.2271],
         [1.3155],
         [1.3762],
         [1.3684],
         [1.3918],
         [1.4026],
         [1.4380],
         [1.4154],
         [1.3710],
         [1.3626],
         [1.3599],
         [1.3501],
         [1.3396],
         [1.3248],
         [1.3248],
         [1.3458],
         [1.3531],
         [1.3539],
         [1.3487],
         [1.3219],
         [1.3026],
         [1.2992],
         [1.3047],
         [1.3125],
         [1.3189],
         [1.3671],
         [1.3729],
         [1.3735],
         [1.3719],
         [1.3805],
         [1.3792],
         [1.3807]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[1.6582],
         [1.7253],
         [1.7224],
         [1.7076],
         [1.7141],
         [1.7312],
         [1.7138],
         [1.7092],
         [1.7166],
         [1.7873],
         [1.8715],
         [1.9148],
         [1.9306],
         [1.9099],
         [1.8882],
         [1.8207],
         [1.7527],
         [1.7716],
         [1.7538],
         [1.7125],
         [1.6581],
         [1.6135],
         [1.5608],
         [1.5657],
         [1.5544],
         [1.5588],
         [1.5641],
         [1.5737],
         [1.5770],
         [1.5890],
         [1.6152],
         [1.6442]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[1.9073],
         [1.9786],
         [1.9694],
         [1.9383],
         [1.9220],
         [1.9174],
         [1.9337],
         [1.9371],
         [1.8972],
         [1.8572],
         [1.9007],
         [1.9804],
         [2.0115],
         [1.9993],
         [1.9927],
         [1.9695],
         [1.9618],
         [1.9591],
         [1.9961],
         [2.0514],
         [2.1278],
         [2.1799],
         [2.2023],
         [2.2290],
         [2.1993],
         [2.1249],
         [2.0794],
         [2.0574],
         [2.0428],
         [2.0370],
         [2.0266],
         [2.0345]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[2.2848],
         [2.3054],
         [2.2895],
         [2.3191],
         [2.3439],
         [2.3078],
         [2.2674],
         [2.2523],
         [2.2433],
         [2.2444],
         [2.2550],
         [2.2951],
         [2.3682],
         [2.4171],
         [2.3050],
         [2.1526],
         [2.1215],
         [2.1176],
         [2.1309],
         [2.1134],
         [2.0961],
         [2.0843],
         [2.0673],
         [2.0357],
         [2.0174],
         [1.9964],
         [1.9889],
         [1.9828],
         [1.9591],
         [1.9486],
         [1.9619],
         [1.9872]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[2.2312],
         [2.3098],
         [2.3391],
         [2.3472],
         [2.3294],
         [2.3093],
         [2.2834],
         [2.2550],
         [2.2345],
         [2.2129],
         [2.2165],
         [2.2208],
         [2.2288],
         [2.2524],
         [2.2937],
         [2.3557],
         [2.3885],
         [2.4111],
         [2.4281],
         [2.4846],
         [2.5342],
         [2.5533],
         [2.5728],
         [2.5435],
         [2.4993],
         [2.4943],
         [2.4561],
         [2.4534],
         [2.4551],
         [2.4557],
         [2.4574],
         [2.4627]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[2.6741],
         [2.7131],
         [2.7265],
         [2.7350],
         [2.7816],
         [2.8448],
         [2.9092],
         [3.0280],
         [3.0073],
         [2.9810],
         [2.9753],
         [2.9818],
         [2.9660],
         [3.0974],
         [3.3325],
         [3.3673],
         [3.3694],
         [3.3376],
         [3.2987],
         [3.2960],
         [3.2963],
         [3.2899],
         [3.2731],
         [3.2676],
         [3.2920],
         [3.2813],
         [3.2458]]], grad_fn=<ThAddBackward>)
[ 1.22705483  1.31546235  1.37624407  1.36840546  1.39181411  1.40262723
  1.43797791  1.41539931  1.37100756  1.36255419  1.35987854  1.35012937
  1.33957195  1.32478178  1.32477927  1.34583139  1.35310459  1.35389423
  1.34870386  1.32194924  1.30255258  1.29922426  1.30474889  1.3125304
  1.31888223  1.36709511  1.37290144  1.37352824  1.37194204  1.38052821
  1.3792429   1.38073027  1.65823162  1.725263    1.72239852  1.70760024
  1.71412647  1.73115253  1.71377456  1.7092253   1.71658432  1.78729713
  1.87150657  1.91476166  1.93055594  1.90987575  1.8882184   1.82066631
  1.75268829  1.77161181  1.75378168  1.71245658  1.65809691  1.61349511
  1.56084001  1.56568015  1.55435872  1.55876422  1.56411397  1.57367599
  1.57699156  1.58895874  1.61518657  1.64423621  1.90728664  1.97859585
  1.96942019  1.93830359  1.92203081  1.91742814  1.93372071  1.9370501
  1.8971808   1.85718918  1.90067959  1.98035836  2.01153541  1.99930239
  1.99274755  1.96945453  1.96175587  1.95911074  1.99609756  2.05138564
  2.12781     2.17993116  2.20229149  2.2289896   2.19929004  2.12489796
  2.07936859  2.05735517  2.04281068  2.03700566  2.02664137  2.03453493
  2.28478551  2.30540538  2.28954291  2.31907082  2.34391379  2.30778074
  2.26740289  2.25234604  2.24326921  2.24438024  2.25499177  2.29505181
  2.36821294  2.41711688  2.3049562   2.15258789  2.1215117   2.1176393
  2.13086724  2.11344886  2.0960691   2.08430743  2.06726432  2.03568649
  2.01741314  1.99636161  1.98891759  1.98277831  1.95905256  1.94864571
  1.96193624  1.98721004  2.23120499  2.3098464   2.33907056  2.34719253
  2.32944083  2.30925059  2.28341579  2.25500894  2.23447061  2.21294689
  2.21649623  2.22076249  2.22878814  2.25238323  2.29373527  2.35573983
  2.38849854  2.41112638  2.4280901   2.48464942  2.53421998  2.55325413
  2.5727818   2.54349017  2.49931836  2.49432158  2.45605969  2.45343065
  2.45513272  2.45571184  2.4574244   2.4627018   2.67413211  2.71311688
  2.72652578  2.73498297  2.78163719  2.84482145  2.9092164   3.02800798
  3.00734425  2.9810164   2.9753325   2.9817698   2.96596146  3.09742212
  3.33252382  3.3673234   3.36942434  3.33763933  3.29874086  3.29602027
  3.29631662  3.28986406  3.2731297   3.26762366  3.29195261  3.28133154
  3.24577475]
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[3.2033],
         [3.2849],
         [3.4400],
         [3.4935],
         [3.5438],
         [3.5823],
         [3.6484],
         [3.6044],
         [3.5136],
         [3.4766],
         [3.4551],
         [3.4353],
         [3.4079],
         [3.3724],
         [3.3716],
         [3.4109],
         [3.4348],
         [3.4476],
         [3.4291],
         [3.3675],
         [3.3183],
         [3.2996],
         [3.3106],
         [3.3219],
         [3.3607],
         [3.4555],
         [3.4898],
         [3.5000],
         [3.4995],
         [3.5104],
         [3.5100],
         [3.5167]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[3.8089],
         [3.8756],
         [3.8820],
         [3.8574],
         [3.8762],
         [3.8986],
         [3.8747],
         [3.8632],
         [3.8886],
         [4.0325],
         [4.2088],
         [4.3271],
         [4.3672],
         [4.3354],
         [4.2828],
         [4.1366],
         [4.0196],
         [4.0104],
         [3.9597],
         [3.8798],
         [3.7804],
         [3.6665],
         [3.5594],
         [3.5380],
         [3.5061],
         [3.5099],
         [3.5232],
         [3.5412],
         [3.5524],
         [3.5811],
         [3.6397],
         [3.6996]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[4.0324],
         [4.1443],
         [4.1478],
         [4.1042],
         [4.0701],
         [4.0560],
         [4.0775],
         [4.0776],
         [4.0115],
         [3.9552],
         [4.0134],
         [4.1308],
         [4.1976],
         [4.2184],
         [4.2106],
         [4.1696],
         [4.1507],
         [4.1449],
         [4.2039],
         [4.3041],
         [4.4480],
         [4.5531],
         [4.6312],
         [4.6881],
         [4.6384],
         [4.5271],
         [4.4388],
         [4.3687],
         [4.3287],
         [4.3078],
         [4.2877],
         [4.3004]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[4.6042],
         [4.6321],
         [4.6297],
         [4.6655],
         [4.6883],
         [4.6489],
         [4.5987],
         [4.5547],
         [4.5266],
         [4.5236],
         [4.5403],
         [4.6080],
         [4.7292],
         [4.8194],
         [4.6730],
         [4.4243],
         [4.3276],
         [4.2724],
         [4.2777],
         [4.2577],
         [4.2336],
         [4.2072],
         [4.1664],
         [4.1164],
         [4.0757],
         [4.0321],
         [4.0138],
         [3.9906],
         [3.9525],
         [3.9345],
         [3.9470],
         [3.9841]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[4.3079],
         [4.4401],
         [4.5153],
         [4.5430],
         [4.5232],
         [4.4888],
         [4.4424],
         [4.3877],
         [4.3450],
         [4.3015],
         [4.2964],
         [4.3019],
         [4.3147],
         [4.3544],
         [4.4279],
         [4.5336],
         [4.6079],
         [4.6606],
         [4.7043],
         [4.7989],
         [4.8881],
         [4.9429],
         [4.9875],
         [4.9478],
         [4.8793],
         [4.8529],
         [4.7880],
         [4.7699],
         [4.7672],
         [4.7673],
         [4.7709],
         [4.7795]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[5.0657],
         [5.1309],
         [5.1593],
         [5.1808],
         [5.2611],
         [5.3635],
         [5.4994],
         [5.6912],
         [5.7085],
         [5.6737],
         [5.6627],
         [5.6621],
         [5.6448],
         [5.8458],
         [6.2165],
         [6.3711],
         [6.4061],
         [6.3669],
         [6.3046],
         [6.2850],
         [6.2806],
         [6.2693],
         [6.2423],
         [6.2309],
         [6.2615],
         [6.2478],
         [6.2003]]], grad_fn=<ThAddBackward>)
[ 3.20325327  3.2849474   3.44001007  3.49349713  3.5437572   3.58227539
  3.64842534  3.60440326  3.5135572   3.47661018  3.45513153  3.43528914
  3.40786195  3.37236238  3.37156224  3.41088128  3.4347508   3.44756818
  3.42906642  3.36748338  3.3182745   3.29962277  3.31062841  3.32194948
  3.36068082  3.45554709  3.48978925  3.49995327  3.49946785  3.5104177
  3.51003718  3.51672149  3.8088944   3.87555814  3.88204217  3.85738087
  3.87623119  3.89864135  3.87469268  3.86320949  3.88863206  4.03249979
  4.20876122  4.32708263  4.36721754  4.33542538  4.28284693  4.13658905
  4.01959133  4.01037979  3.95966077  3.87980843  3.78037739  3.66652775
  3.5594151   3.53797841  3.50605464  3.50989699  3.52324557  3.54121184
  3.55239797  3.58111501  3.63974929  3.69961596  4.03239727  4.144279
  4.14784527  4.10418892  4.07009506  4.05595922  4.0774641   4.07762146
  4.01154709  3.95523238  4.0134306   4.13083458  4.19758892  4.21835995
  4.21055079  4.16957903  4.15067148  4.14486313  4.20385361  4.30414629
  4.44795418  4.55310678  4.63122845  4.6880827   4.63839626  4.52710247
  4.43881512  4.36873674  4.32869196  4.30780792  4.2877264   4.30038071
  4.60420036  4.63206291  4.62967682  4.66552925  4.68832588  4.6489048
  4.5987196   4.55469227  4.52660656  4.52359438  4.54033661  4.60795498
  4.72917509  4.81941986  4.67298794  4.42434978  4.32759857  4.27242708
  4.27766895  4.25772285  4.23356152  4.20718861  4.1663537   4.11639595
  4.07570648  4.03207445  4.01376963  3.99061346  3.95246625  3.93445849
  3.94698334  3.98409891  4.30785656  4.44010496  4.51529408  4.54301453
  4.52322006  4.48881006  4.44240904  4.38774061  4.34502363  4.30154753
  4.29642487  4.30185175  4.31470728  4.3543663   4.42791796  4.53360939
  4.60794687  4.66062593  4.70425606  4.79889011  4.88813925  4.94289255
  4.98754692  4.94780874  4.87928438  4.85285711  4.78796911  4.76993036
  4.76722288  4.76728821  4.77090788  4.77949286  5.06570196  5.13087511
  5.15932608  5.18083382  5.26112318  5.3635397   5.49940872  5.69116163
  5.70847225  5.67370796  5.66272402  5.66208982  5.64477015  5.84584761
  6.21653891  6.37109327  6.40609026  6.36687279  6.30460978  6.28500891
  6.28057146  6.26933908  6.24233103  6.23092508  6.2615366   6.24782228
  6.20033216]
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[6.0594],
         [6.1353],
         [6.3571],
         [6.5085],
         [6.5978],
         [6.6759],
         [6.7860],
         [6.7656],
         [6.6273],
         [6.5264],
         [6.4809],
         [6.4442],
         [6.3944],
         [6.3297],
         [6.3101],
         [6.3595],
         [6.4120],
         [6.4409],
         [6.4217],
         [6.3312],
         [6.2328],
         [6.1811],
         [6.1866],
         [6.2059],
         [6.2692],
         [6.4099],
         [6.5067],
         [6.5378],
         [6.5424],
         [6.5587],
         [6.5674],
         [6.5766]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[6.9757],
         [7.0906],
         [7.1301],
         [7.1134],
         [7.1362],
         [7.1761],
         [7.1596],
         [7.1303],
         [7.1629],
         [7.3749],
         [7.6777],
         [7.9189],
         [8.0263],
         [8.0098],
         [7.9320],
         [7.7223],
         [7.4983],
         [7.4229],
         [7.3540],
         [7.2151],
         [7.0367],
         [6.8325],
         [6.6336],
         [6.5427],
         [6.4886],
         [6.4713],
         [6.4855],
         [6.5123],
         [6.5351],
         [6.5778],
         [6.6698],
         [6.7782]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[7.2614],
         [7.4577],
         [7.5209],
         [7.4839],
         [7.4306],
         [7.4019],
         [7.4243],
         [7.4352],
         [7.3488],
         [7.2417],
         [7.2976],
         [7.4827],
         [7.6208],
         [7.6726],
         [7.6706],
         [7.6228],
         [7.5800],
         [7.5649],
         [7.6395],
         [7.8024],
         [8.0404],
         [8.2460],
         [8.3986],
         [8.5142],
         [8.4887],
         [8.3251],
         [8.1560],
         [8.0269],
         [7.9400],
         [7.8871],
         [7.8455],
         [7.8461]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[8.2710],
         [8.3375],
         [8.3536],
         [8.4236],
         [8.4857],
         [8.4402],
         [8.3494],
         [8.2767],
         [8.2241],
         [8.2050],
         [8.2221],
         [8.3208],
         [8.5131],
         [8.6839],
         [8.5398],
         [8.1477],
         [7.9173],
         [7.8024],
         [7.7758],
         [7.7302],
         [7.6770],
         [7.6291],
         [7.5636],
         [7.4738],
         [7.3977],
         [7.3184],
         [7.2730],
         [7.2325],
         [7.1659],
         [7.1230],
         [7.1316],
         [7.1854]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[7.6628],
         [7.9014],
         [8.0697],
         [8.1579],
         [8.1614],
         [8.1219],
         [8.0533],
         [7.9650],
         [7.8873],
         [7.8087],
         [7.7830],
         [7.7800],
         [7.7951],
         [7.8495],
         [7.9628],
         [8.1283],
         [8.2695],
         [8.3745],
         [8.4649],
         [8.6166],
         [8.7744],
         [8.8885],
         [8.9788],
         [8.9559],
         [8.8626],
         [8.8046],
         [8.7041],
         [8.6564],
         [8.6383],
         [8.6308],
         [8.6326],
         [8.6432]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[ 9.0910],
         [ 9.2341],
         [ 9.3277],
         [ 9.3884],
         [ 9.5170],
         [ 9.6891],
         [ 9.9172],
         [10.2277],
         [10.3306],
         [10.3185],
         [10.3088],
         [10.3119],
         [10.2872],
         [10.5600],
         [11.1141],
         [11.4443],
         [11.5836],
         [11.5827],
         [11.5086],
         [11.4709],
         [11.4568],
         [11.4377],
         [11.3967],
         [11.3696],
         [11.4045],
         [11.3947],
         [11.3251]]], grad_fn=<ThAddBackward>)
[  6.05940151   6.13530064   6.3570714    6.50853395   6.59776068
   6.67593718   6.78595209   6.76555014   6.62732077   6.52637768
   6.48094511   6.44417238   6.39438248   6.32974482   6.31005144
   6.3594985    6.41196537   6.4408555    6.42165613   6.33116293
   6.23277235   6.18114233   6.18662024   6.20586157   6.26924086
   6.4098897    6.50674009   6.53776026   6.54244423   6.55865479
   6.56740904   6.57662535   6.97572088   7.09055758   7.13009977
   7.11344099   7.13618803   7.17609215   7.15964508   7.13026047
   7.1628685    7.3749485    7.67768431   7.91885185   8.02630043
   8.00980091   7.93195915   7.72234201   7.49827576   7.42285252
   7.35399103   7.21507502   7.03666973   6.83251715   6.63360834
   6.54270363   6.48857689   6.47132492   6.48545742   6.51227903
   6.53509188   6.57776594   6.66976738   6.778234     7.26135731
   7.45766687   7.52088594   7.48390961   7.43063498   7.40189505
   7.4243145    7.4351964    7.34878397   7.24170256   7.29755783
   7.48267221   7.62079716   7.67255735   7.67057133   7.62278223
   7.58001566   7.56490278   7.63954449   7.8023572    8.0403738
   8.2460022    8.39861298   8.51420784   8.48868179   8.32510567
   8.15598774   8.02688408   7.93998528   7.88705683   7.84547663
   7.84613466   8.27097511   8.33747959   8.35363293   8.42355251
   8.48573017   8.4402113    8.34939766   8.27671719   8.22406006
   8.20499992   8.2220602    8.32077599   8.5131197    8.68390369
   8.53984165   8.14768314   7.91734028   7.80242634   7.77579689
   7.7301569    7.67702007   7.62908077   7.56363487   7.4738245
   7.39772224   7.31844711   7.27296925   7.23245239   7.16586399
   7.12300968   7.13163376   7.18540192   7.66280079   7.90135288
   8.06973934   8.15786266   8.16144753   8.12191868   8.05331135
   7.96501684   7.8872757    7.80868912   7.7830267    7.77999306
   7.79510927   7.84947968   7.96281433   8.12833881   8.26953793
   8.37447262   8.46490383   8.61656761   8.77438354   8.88851547
   8.97884655   8.95588875   8.86260414   8.80458355   8.70408916
   8.65636539   8.63831997   8.63084316   8.63263226   8.6431942
   9.09101105   9.23412514   9.32772446   9.38835907   9.51695442
   9.68909359   9.91715813  10.22766495  10.33059883  10.31846046
  10.30882072  10.31192207  10.28723049  10.5600338   11.11405945
  11.4442873   11.58356476  11.58274364  11.50858784  11.47085762
  11.45684052  11.43774319  11.39670372  11.36964607  11.40447426
  11.3947258   11.32512093]
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[11.2203],
         [11.2575],
         [11.5707],
         [11.8266],
         [12.0104],
         [12.1639],
         [12.3614],
         [12.3921],
         [12.2151],
         [12.0464],
         [11.9474],
         [11.8674],
         [11.7755],
         [11.6631],
         [11.6042],
         [11.6546],
         [11.7327],
         [11.7878],
         [11.7789],
         [11.6590],
         [11.4980],
         [11.3917],
         [11.3673],
         [11.3838],
         [11.4625],
         [11.6732],
         [11.8533],
         [11.9420],
         [11.9749],
         [12.0117],
         [12.0355],
         [12.0544]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[12.7092],
         [12.9654],
         [13.0945],
         [13.1196],
         [13.1624],
         [13.2359],
         [13.2311],
         [13.1940],
         [13.2254],
         [13.5154],
         [13.9895],
         [14.4269],
         [14.6913],
         [14.7525],
         [14.6840],
         [14.3984],
         [14.0279],
         [13.8478],
         [13.6942],
         [13.4617],
         [13.1529],
         [12.8044],
         [12.4374],
         [12.2280],
         [12.0825],
         [12.0128],
         [12.0003],
         [12.0279],
         [12.0609],
         [12.1240],
         [12.2614],
         [12.4418]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[13.2557],
         [13.6693],
         [13.8757],
         [13.9036],
         [13.8571],
         [13.8169],
         [13.8403],
         [13.8579],
         [13.7471],
         [13.5833],
         [13.6126],
         [13.8606],
         [14.1006],
         [14.2299],
         [14.2663],
         [14.2235],
         [14.1625],
         [14.1289],
         [14.2193],
         [14.4504],
         [14.8220],
         [15.1921],
         [15.4973],
         [15.7410],
         [15.7838],
         [15.5977],
         [15.3344],
         [15.1019],
         [14.9230],
         [14.7966],
         [14.6996],
         [14.6681]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[15.3741],
         [15.5839],
         [15.6724],
         [15.7997],
         [15.9237],
         [15.9069],
         [15.7903],
         [15.6642],
         [15.5663],
         [15.5112],
         [15.5133],
         [15.6401],
         [15.9184],
         [16.2135],
         [16.0931],
         [15.5941],
         [15.1557],
         [14.8935],
         [14.7694],
         [14.6553],
         [14.5425],
         [14.4407],
         [14.3216],
         [14.1732],
         [14.0235],
         [13.8773],
         [13.7722],
         [13.6814],
         [13.5687],
         [13.4750],
         [13.4566],
         [13.5173]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[14.3149],
         [14.7821],
         [15.1416],
         [15.3764],
         [15.4710],
         [15.4654],
         [15.3851],
         [15.2627],
         [15.1247],
         [14.9871],
         [14.9088],
         [14.8750],
         [14.8835],
         [14.9479],
         [15.1136],
         [15.3682],
         [15.6265],
         [15.8378],
         [16.0271],
         [16.2864],
         [16.5700],
         [16.8112],
         [17.0031],
         [17.0346],
         [16.9520],
         [16.8480],
         [16.7045],
         [16.5978],
         [16.5366],
         [16.5082],
         [16.4958],
         [16.5017]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[17.2959],
         [17.6409],
         [17.8734],
         [18.0417],
         [18.2841],
         [18.5740],
         [18.9816],
         [19.4848],
         [19.7842],
         [19.8788],
         [19.9134],
         [19.9452],
         [19.9397],
         [20.3079],
         [21.0978],
         [21.7601],
         [22.1446],
         [22.2744],
         [22.2641],
         [22.2345],
         [22.2194],
         [22.1966],
         [22.1357],
         [22.0873],
         [22.1144],
         [22.1032],
         [22.0188]]], grad_fn=<ThAddBackward>)
[ 11.22025108  11.25748444  11.57066345  11.82664871  12.0103569
  12.16385746  12.36140728  12.39207458  12.21505737  12.0464468
  11.94738197  11.86743736  11.77551651  11.66309738  11.60423851
  11.6545639   11.7327137   11.78780174  11.77885246  11.65904713
  11.49795723  11.39171982  11.36732674  11.38376808  11.46245193
  11.67318726  11.85326004  11.94204712  11.97492695  12.01171017
  12.03546047  12.05436325  12.70920944  12.96536255  13.09445381
  13.11962223  13.16243076  13.23592758  13.23110199  13.19403839
  13.22538948  13.51535034  13.98950481  14.42687035  14.69128799
  14.75252628  14.68399239  14.39836979  14.02793598  13.84776497
  13.69423389  13.46167183  13.15292549  12.80435658  12.4374342
  12.22803688  12.08251381  12.01275063  12.00026131  12.02794743
  12.06094742  12.12398338  12.26138115  12.44183922  13.25566101
  13.66930199  13.87568283  13.90355492  13.85709572  13.81685257
  13.84026337  13.85787392  13.74713516  13.58334351  13.61257744
  13.86057281  14.10062599  14.22990036  14.26630592  14.22354507
  14.16254807  14.12892914  14.21934891  14.45040894  14.82201576
  15.19208717  15.49725151  15.7410183   15.78379822  15.59771347
  15.33444977  15.10193634  14.92295647  14.79662704  14.69956779
  14.66806793  15.37405777  15.5838995   15.67235374  15.79973125
  15.92367458  15.90692902  15.79028988  15.66419888  15.56634521
  15.51120472  15.51330853  15.64007759  15.91837406  16.21352768
  16.09311104  15.59412384  15.15573978  14.89349461  14.76942825
  14.65531349  14.54245758  14.44073868  14.32163143  14.17316723
  14.02346897  13.87732792  13.77216434  13.68139172  13.56866741
  13.47500896  13.45657444  13.51734447  14.31489277  14.7821188
  15.14163303  15.37638092  15.47100735  15.46536636  15.38510704
  15.26266766  15.12471485  14.98707962  14.90880775  14.8750391
  14.88349819  14.94786549  15.11363697  15.36816978  15.62650204
  15.83782673  16.02709389  16.28642273  16.57000923  16.81116104
  17.00309563  17.03464317  16.95197105  16.84799957  16.70454788
  16.59779167  16.53663635  16.5081749   16.49576569  16.50168228
  17.29593658  17.64089775  17.87339783  18.04170418  18.284132
  18.57402802  18.98164749  19.48483658  19.78417015  19.87882614
  19.91340637  19.94523048  19.93966484  20.30793571  21.09776688
  21.76014709  22.14456558  22.27436256  22.26408195  22.23449516
  22.21940804  22.19663048  22.13574409  22.087286    22.11439323
  22.10318184  22.01881599]
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[22.1439],
         [22.2112],
         [22.6184],
         [23.0721],
         [23.4385],
         [23.7616],
         [24.1257],
         [24.2865],
         [24.1420],
         [23.9052],
         [23.7245],
         [23.5758],
         [23.4034],
         [23.1980],
         [23.0594],
         [23.0672],
         [23.1556],
         [23.2427],
         [23.2472],
         [23.0975],
         [22.8588],
         [22.6538],
         [22.5625],
         [22.5393],
         [22.6336],
         [22.9082],
         [23.2129],
         [23.4190],
         [23.5339],
         [23.6293],
         [23.7021],
         [23.7599]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[24.9413],
         [25.5615],
         [25.9634],
         [26.1720],
         [26.3517],
         [26.5245],
         [26.6011],
         [26.5916],
         [26.6551],
         [27.0571],
         [27.7701],
         [28.5443],
         [29.1246],
         [29.4167],
         [29.4635],
         [29.1482],
         [28.6427],
         [28.2680],
         [27.9568],
         [27.5414],
         [26.9977],
         [26.3559],
         [25.6906],
         [25.1884],
         [24.8129],
         [24.5666],
         [24.4331],
         [24.3931],
         [24.3976],
         [24.4642],
         [24.6506],
         [24.9269]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[26.4013],
         [27.3179],
         [27.9302],
         [28.2398],
         [28.3588],
         [28.4147],
         [28.5040],
         [28.5733],
         [28.4599],
         [28.2314],
         [28.2304],
         [28.5346],
         [28.9093],
         [29.1798],
         [29.3159],
         [29.3320],
         [29.2823],
         [29.2492],
         [29.3640],
         [29.6971],
         [30.2637],
         [30.8999],
         [31.5020],
         [32.0273],
         [32.2778],
         [32.1657],
         [31.8511],
         [31.4968],
         [31.1787],
         [30.9107],
         [30.6849],
         [30.5563]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[31.8332],
         [32.3903],
         [32.7501],
         [33.1091],
         [33.4384],
         [33.5554],
         [33.4853],
         [33.3525],
         [33.2135],
         [33.1116],
         [33.0811],
         [33.2314],
         [33.6208],
         [34.0956],
         [34.0676],
         [33.4399],
         [32.7708],
         [32.2560],
         [31.9145],
         [31.6043],
         [31.3180],
         [31.0610],
         [30.7930],
         [30.4871],
         [30.1794],
         [29.8709],
         [29.6181],
         [29.3932],
         [29.1454],
         [28.9288],
         [28.8193],
         [28.8383]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[30.2592],
         [31.2236],
         [32.0562],
         [32.7128],
         [33.1402],
         [33.3723],
         [33.4355],
         [33.3700],
         [33.2280],
         [33.0362],
         [32.8945],
         [32.8028],
         [32.7699],
         [32.8305],
         [33.0484],
         [33.4313],
         [33.8645],
         [34.2771],
         [34.6737],
         [35.1761],
         [35.7288],
         [36.2415],
         [36.6973],
         [36.9264],
         [36.9527],
         [36.9017],
         [36.7356],
         [36.5830],
         [36.4656],
         [36.3849],
         [36.3312],
         [36.3100]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[37.4718],
         [38.0335],
         [38.4842],
         [38.8538],
         [39.3150],
         [39.8598],
         [40.5718],
         [41.4749],
         [42.1385],
         [42.5384],
         [42.8090],
         [43.0243],
         [43.1431],
         [43.7326],
         [44.9479],
         [46.1045],
         [46.9830],
         [47.5179],
         [47.7973],
         [47.9753],
         [48.1039],
         [48.1746],
         [48.1611],
         [48.1301],
         [48.1825],
         [48.1802],
         [48.0711]]], grad_fn=<ThAddBackward>)
[ 22.14394379  22.21120644  22.61837769  23.0720768   23.43852425
  23.76164436  24.12565041  24.28649521  24.14196396  23.90517998
  23.72453117  23.57582664  23.4033699   23.19795799  23.05942535
  23.06723595  23.15558434  23.24274445  23.24722862  23.09753609
  22.85884476  22.65379524  22.5624752   22.53930092  22.63358116
  22.90822411  23.21294212  23.41896057  23.53385925  23.62929344
  23.70211029  23.75986671  24.94125557  25.56150246  25.96341515
  26.17196655  26.35165405  26.52453041  26.60114479  26.59158516
  26.65513229  27.05713081  27.77013779  28.54434967  29.12460709
  29.41668701  29.46350288  29.14822578  28.64274788  28.2679615
  27.95682526  27.54141617  26.99770927  26.35591888  25.69056129
  25.18843842  24.8129406   24.56662941  24.43306541  24.39306831
  24.39758301  24.46420097  24.65058708  24.92685127  26.40128326
  27.31792259  27.93021202  28.23976326  28.358778    28.41471481
  28.50403214  28.57333565  28.4599247   28.23141479  28.23037338
  28.53458405  28.90932846  29.17975998  29.31589317  29.33203697
  29.28227043  29.24921036  29.36395454  29.69708061  30.26365471
  30.89989471  31.50196075  32.0273056   32.27776337  32.16572952
  31.85108185  31.49676323  31.17867851  30.91071129  30.68493462
  30.55634499  31.83317757  32.3903389   32.75008774  33.10910416
  33.43836594  33.55542374  33.48532486  33.35252762  33.21345901
  33.11159134  33.08112335  33.23139191  33.62081909  34.0956459
  34.0676384   33.43989182  32.77082825  32.25599289  31.91447449
  31.60430336  31.31801033  31.06098557  30.79302788  30.48707199
  30.17935371  29.8708725   29.6181469   29.3932457   29.14536476
  28.92883682  28.81928062  28.83827019  30.25916862  31.22364807
  32.05617523  32.71278     33.14024734  33.37226868  33.43552399
  33.37002563  33.22803879  33.03619385  32.89452362  32.80279541
  32.76985168  32.83047867  33.0483551   33.43125534  33.86445999
  34.27711487  34.67366409  35.1760788   35.72879791  36.24149704
  36.69731522  36.92644882  36.95268631  36.90167999  36.73562622
  36.58296204  36.46564102  36.38485336  36.33115768  36.30996323
  37.47176743  38.03352737  38.48423004  38.85379791  39.31500626
  39.85980606  40.57177353  41.47494125  42.1385498   42.5383873
  42.80903244  43.02425003  43.14313507  43.73255157  44.94792175
  46.10454178  46.98302078  47.51786041  47.79726028  47.97525406
  48.10389328  48.17458725  48.16110611  48.13008499  48.18253708
  48.18024063  48.0710907 ]
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[47.9701],
         [47.7050],
         [47.9349],
         [48.3491],
         [48.7821],
         [49.2398],
         [49.8108],
         [50.1500],
         [50.0898],
         [49.8541],
         [49.6189],
         [49.3842],
         [49.0967],
         [48.7465],
         [48.4663],
         [48.3798],
         [48.4208],
         [48.4961],
         [48.4892],
         [48.2829],
         [47.9353],
         [47.5951],
         [47.3808],
         [47.2608],
         [47.3125],
         [47.6473],
         [48.0757],
         [48.4280],
         [48.6852],
         [48.9148],
         [49.1059],
         [49.2678]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[50.1629],
         [50.5392],
         [50.7936],
         [50.9045],
         [51.0372],
         [51.2051],
         [51.2803],
         [51.2649],
         [51.3418],
         [51.8700],
         [52.8595],
         [54.0192],
         [55.0120],
         [55.6716],
         [55.9962],
         [55.7968],
         [55.2523],
         [54.7589],
         [54.2780],
         [53.6261],
         [52.7816],
         [51.7541],
         [50.6427],
         [49.6828],
         [48.8836],
         [48.2684],
         [47.8392],
         [47.5739],
         [47.4153],
         [47.3833],
         [47.5508],
         [47.8832]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[48.6773],
         [49.1588],
         [49.4730],
         [49.5757],
         [49.5298],
         [49.4574],
         [49.4599],
         [49.4686],
         [49.2773],
         [48.9504],
         [48.8725],
         [49.1954],
         [49.6544],
         [50.0429],
         [50.2825],
         [50.3783],
         [50.3709],
         [50.3636],
         [50.5247],
         [50.9654],
         [51.7333],
         [52.6439],
         [53.5648],
         [54.4192],
         [54.9513],
         [55.0136],
         [54.7562],
         [54.3691],
         [53.9579],
         [53.5687],
         [53.2130],
         [52.9671]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[52.8184],
         [52.3786],
         [51.9292],
         [51.6935],
         [51.5875],
         [51.3744],
         [51.0369],
         [50.6700],
         [50.3381],
         [50.0768],
         [49.9281],
         [50.0222],
         [50.4383],
         [51.0069],
         [51.0152],
         [50.3254],
         [49.4682],
         [48.7631],
         [48.2325],
         [47.7484],
         [47.2977],
         [46.8904],
         [46.4760],
         [46.0257],
         [45.5646],
         [45.1061],
         [44.7108],
         [44.3514],
         [43.9756],
         [43.6357],
         [43.4275],
         [43.3852]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[43.3400],
         [43.3343],
         [43.4201],
         [43.5211],
         [43.5331],
         [43.4464],
         [43.2543],
         [42.9867],
         [42.6671],
         [42.3319],
         [42.0710],
         [41.8879],
         [41.7964],
         [41.8198],
         [42.0477],
         [42.4717],
         [42.9851],
         [43.4809],
         [43.9744],
         [44.5863],
         [45.2659],
         [45.9131],
         [46.4910],
         [46.8068],
         [46.8958],
         [46.8516],
         [46.6886],
         [46.5114],
         [46.3700],
         [46.2701],
         [46.2001],
         [46.1676]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[45.7401],
         [45.2768],
         [44.8920],
         [44.6273],
         [44.6331],
         [44.8426],
         [45.3743],
         [46.1443],
         [46.7576],
         [47.1018],
         [47.3253],
         [47.4962],
         [47.5876],
         [48.2077],
         [49.4834],
         [50.7750],
         [51.7530],
         [52.3447],
         [52.6556],
         [52.8359],
         [52.9605],
         [53.0254],
         [52.9992],
         [52.9589],
         [53.0039],
         [52.9952],
         [52.8830]]], grad_fn=<ThAddBackward>)
[ 47.97006226  47.70499039  47.9348793   48.34905624  48.78211975
  49.23975372  49.81077957  50.14999008  50.08983994  49.85413361
  49.61889648  49.38422775  49.09674072  48.74645233  48.466259
  48.37976456  48.42078781  48.49606323  48.48916626  48.28286362
  47.9352684   47.59510422  47.38082504  47.26083755  47.31246567
  47.64734268  48.07567215  48.42804337  48.68518448  48.91483688
  49.10591888  49.26781845  50.16292953  50.53920364  50.7936058
  50.90449142  51.03717422  51.20512009  51.28034592  51.26488876
  51.34181976  51.86997986  52.859478    54.01919174  55.01203918
  55.67155838  55.99618912  55.79682922  55.25230408  54.75886917
  54.27800369  53.62607193  52.78155899  51.7541008   50.64266586
  49.68284988  48.88363647  48.26837158  47.83922577  47.57392502
  47.41526413  47.38330841  47.55076981  47.88324356  48.67730331
  49.15884781  49.47302628  49.57565308  49.52978897  49.45741653
  49.4598732   49.46863174  49.27726364  48.95039749  48.87254333
  49.19544601  49.6543808   50.04285049  50.28251648  50.37826157
  50.37092209  50.36358643  50.52474594  50.96538925  51.73333359
  52.64391708  53.56476212  54.41923523  54.95134735  55.01356888
  54.75621414  54.36907578  53.95785141  53.56871414  53.21300888
  52.96707535  52.81838226  52.3785553   51.92916107  51.69349289
  51.58753204  51.37440491  51.03690338  50.67003632  50.3381424
  50.0767746   49.92811966  50.02224731  50.43825912  51.00689697
  51.01515198  50.32539368  49.46821594  48.76306534  48.23248291
  47.74843597  47.29769135  46.89044189  46.47599792  46.0256958
  45.56462097  45.10614777  44.7108078   44.35137177  43.97563171
  43.63569641  43.42750931  43.38517761  43.33996582  43.33430862
  43.42010117  43.52111435  43.5330925   43.44635391  43.25429153
  42.98670578  42.66714859  42.33185959  42.07101059  41.88787079
  41.79639053  41.81981277  42.04767609  42.47174454  42.98506165
  43.48085785  43.97444153  44.58629227  45.26587296  45.91305161
  46.49097061  46.80679321  46.89580917  46.85161209  46.68857193
  46.51138687  46.37004852  46.2701149   46.20007706  46.16756821
  45.74007797  45.27679825  44.89201736  44.62725449  44.63305664
  44.842556    45.37433243  46.14429092  46.75764084  47.10177231
  47.32527542  47.49615097  47.58761215  48.20767212  49.48337173
  50.77501678  51.75304794  52.34474182  52.65564346  52.83585739
  52.96045685  53.02536392  52.99917984  52.95888138  53.00387192
  52.99518204  52.88300705]
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[50.9614],
         [49.5567],
         [48.9023],
         [48.6833],
         [48.6473],
         [48.7746],
         [49.1010],
         [49.2591],
         [49.0541],
         [48.6879],
         [48.3498],
         [48.0448],
         [47.7066],
         [47.3210],
         [47.0208],
         [46.9262],
         [46.9768],
         [47.0672],
         [47.0676],
         [46.8608],
         [46.5041],
         [46.1490],
         [45.9316],
         [45.8149],
         [45.8922],
         [46.2469],
         [46.7083],
         [47.0820],
         [47.3463],
         [47.5720],
         [47.7559],
         [47.9082]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[47.2817],
         [46.7613],
         [46.2835],
         [45.8473],
         [45.5832],
         [45.4569],
         [45.3305],
         [45.1638],
         [45.1393],
         [45.5965],
         [46.5294],
         [47.6307],
         [48.5396],
         [49.0980],
         [49.3103],
         [49.0037],
         [48.3850],
         [47.8315],
         [47.3386],
         [46.6986],
         [45.8873],
         [44.9015],
         [43.8652],
         [42.9863],
         [42.2939],
         [41.7894],
         [41.4676],
         [41.3001],
         [41.2272],
         [41.2647],
         [41.4851],
         [41.8526]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[41.4840],
         [41.2737],
         [41.0217],
         [40.6982],
         [40.3478],
         [40.0679],
         [39.9317],
         [39.8478],
         [39.5934],
         [39.2404],
         [39.1586],
         [39.4854],
         [39.9310],
         [40.2809],
         [40.4679],
         [40.5089],
         [40.4562],
         [40.4209],
         [40.5611],
         [40.9820],
         [41.7099],
         [42.5436],
         [43.3548],
         [44.0743],
         [44.4538],
         [44.3781],
         [44.0243],
         [43.5805],
         [43.1636],
         [42.7944],
         [42.4838],
         [42.2888]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[41.1980],
         [40.2570],
         [39.4643],
         [39.0153],
         [38.7824],
         [38.4953],
         [38.1278],
         [37.7672],
         [37.4688],
         [37.2569],
         [37.1620],
         [37.2988],
         [37.7273],
         [38.2621],
         [38.2106],
         [37.4994],
         [36.6845],
         [36.0765],
         [35.6730],
         [35.3266],
         [35.0061],
         [34.7215],
         [34.4217],
         [34.0846],
         [33.7358],
         [33.3920],
         [33.1100],
         [32.8582],
         [32.5864],
         [32.3464],
         [32.2281],
         [32.2597]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.6284],
         [31.3048],
         [31.1447],
         [31.0649],
         [30.9475],
         [30.7780],
         [30.5450],
         [30.2743],
         [29.9847],
         [29.7042],
         [29.5119],
         [29.3978],
         [29.3657],
         [29.4318],
         [29.6738],
         [30.0728],
         [30.5183],
         [30.9148],
         [31.2893],
         [31.7655],
         [32.2890],
         [32.7646],
         [33.1668],
         [33.3185],
         [33.2791],
         [33.1506],
         [32.9455],
         [32.7635],
         [32.6390],
         [32.5643],
         [32.5212],
         [32.5114]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[31.9114],
         [31.4182],
         [31.0491],
         [30.8303],
         [30.8835],
         [31.1122],
         [31.6104],
         [32.2850],
         [32.7422],
         [32.9229],
         [33.0115],
         [33.0793],
         [33.0938],
         [33.6428],
         [34.7744],
         [35.8141],
         [36.4828],
         [36.7788],
         [36.8505],
         [36.8626],
         [36.8726],
         [36.8592],
         [36.7832],
         [36.7173],
         [36.7506],
         [36.7315],
         [36.6153]]], grad_fn=<ThAddBackward>)
[ 50.96141052  49.55673599  48.90231323  48.68334198  48.64725494
  48.77459717  49.10101318  49.25912476  49.05405426  48.68792343
  48.34976196  48.04483032  47.70659256  47.320961    47.02078629
  46.92620087  46.97679138  47.06720734  47.06761169  46.86082077
  46.50407791  46.14899445  45.93162537  45.81486511  45.89223099
  46.24687958  46.70832062  47.08200836  47.34634781  47.57200241
  47.75590515  47.90822983  47.28167725  46.76127243  46.28347015
  45.84727097  45.58317947  45.45688248  45.33053589  45.16384506
  45.13929367  45.59653854  46.52941513  47.63073349  48.53963089
  49.09804916  49.31029129  49.00368118  48.38503647  47.83153152
  47.33864975  46.69863129  45.88726425  44.90151596  43.8652153
  42.98632812  42.29385757  41.78943253  41.46762085  41.30012894
  41.22721863  41.26473618  41.48511124  41.85264206  41.48398972
  41.27367401  41.02170181  40.69823074  40.34779358  40.06786346
  39.93166733  39.84781265  39.5933609   39.24042892  39.15858459
  39.48535538  39.93095016  40.28087234  40.46785736  40.50893402
  40.45615768  40.42085648  40.56106186  40.98198318  41.70988464
  42.5436058   43.35484695  44.0742836   44.4537735   44.37812424
  44.02434158  43.58054733  43.1635704   42.79442215  42.48381424
  42.28879929  41.19803619  40.25703049  39.46429062  39.01528168
  38.78243637  38.49529266  38.12784958  37.7672081   37.46875381
  37.25688934  37.16204453  37.29875183  37.72733307  38.26207352
  38.21064377  37.49944305  36.68450928  36.07647324  35.67298508
  35.32661057  35.00606537  34.72153473  34.42174149  34.08455276
  33.73577499  33.39200592  33.11003494  32.85824585  32.58637619
  32.34637833  32.22813034  32.25972366  31.62838936  31.3047924
  31.14465523  31.06488609  30.94747925  30.77796936  30.54495239
  30.2743187   29.98469925  29.70418739  29.51193237  29.39778137
  29.36568642  29.4317627   29.67382812  30.07282066  30.51834106
  30.91481209  31.2892971   31.76554108  32.2889595   32.76455307
  33.16677094  33.31851578  33.2790947   33.15056229  32.94550705
  32.76350784  32.6390152   32.56431198  32.5211525   32.51143646
  31.91141129  31.41816902  31.04907417  30.83028603  30.88348198
  31.11224556  31.61035728  32.28504181  32.74217224  32.92285538
  33.01153564  33.07928848  33.09381866  33.64279938  34.77444839
  35.81407547  36.48279953  36.77879333  36.85047913  36.86257172
  36.872612    36.85921478  36.78320312  36.71726227  36.75060272
  36.73147964  36.61531448]
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[35.0055],
         [34.0077],
         [33.7539],
         [33.8399],
         [33.9925],
         [34.2192],
         [34.5767],
         [34.7173],
         [34.4926],
         [34.1497],
         [33.8740],
         [33.6494],
         [33.3985],
         [33.1068],
         [32.9055],
         [32.8991],
         [33.0065],
         [33.1197],
         [33.1239],
         [32.9253],
         [32.6051],
         [32.3186],
         [32.1821],
         [32.1373],
         [32.2611],
         [32.6230],
         [33.0373],
         [33.3260],
         [33.4967],
         [33.6370],
         [33.7464],
         [33.8348]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[33.7332],
         [33.5877],
         [33.4204],
         [33.2331],
         [33.1639],
         [33.1821],
         [33.1540],
         [33.0610],
         [33.0930],
         [33.5712],
         [34.4501],
         [35.4064],
         [36.1108],
         [36.4575],
         [36.4979],
         [36.0897],
         [35.4610],
         [34.9735],
         [34.5830],
         [34.0627],
         [33.3959],
         [32.5911],
         [31.7730],
         [31.1319],
         [30.6698],
         [30.3688],
         [30.2131],
         [30.1701],
         [30.1829],
         [30.2727],
         [30.5123],
         [30.8611]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[31.2322],
         [31.4824],
         [31.5712],
         [31.4942],
         [31.3302],
         [31.1967],
         [31.1728],
         [31.1683],
         [30.9730],
         [30.6795],
         [30.6595],
         [31.0215],
         [31.4546],
         [31.7566],
         [31.8879],
         [31.8829],
         [31.8037],
         [31.7582],
         [31.8926],
         [32.2906],
         [32.9590],
         [33.6886],
         [34.3620],
         [34.9302],
         [35.1654],
         [34.9828],
         [34.5827],
         [34.1492],
         [33.7792],
         [33.4749],
         [33.2331],
         [33.1026]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[33.1327],
         [32.9131],
         [32.6752],
         [32.6304],
         [32.6791],
         [32.5816],
         [32.3492],
         [32.0943],
         [31.8819],
         [31.7399],
         [31.6988],
         [31.8710],
         [32.3105],
         [32.8258],
         [32.7375],
         [32.0122],
         [31.2316],
         [30.6923],
         [30.3662],
         [30.0879],
         [29.8269],
         [29.5957],
         [29.3446],
         [29.0537],
         [28.7528],
         [28.4579],
         [28.2247],
         [28.0187],
         [27.7886],
         [27.5889],
         [27.5077],
         [27.5670]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[27.9752],
         [28.2912],
         [28.5983],
         [28.8358],
         [28.9270],
         [28.8939],
         [28.7520],
         [28.5435],
         [28.2995],
         [28.0535],
         [27.8898],
         [27.7991],
         [27.7846],
         [27.8638],
         [28.1141],
         [28.5164],
         [28.9564],
         [29.3402],
         [29.6977],
         [30.1571],
         [30.6619],
         [31.1145],
         [31.4920],
         [31.6187],
         [31.5559],
         [31.4142],
         [31.2017],
         [31.0194],
         [30.8993],
         [30.8305],
         [30.7929],
         [30.7878]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[31.1243],
         [31.1985],
         [31.2423],
         [31.3050],
         [31.5503],
         [31.9150],
         [32.5109],
         [33.2665],
         [33.7787],
         [33.9909],
         [34.0971],
         [34.1765],
         [34.1961],
         [34.7602],
         [35.9328],
         [37.0152],
         [37.7123],
         [38.0229],
         [38.0970],
         [38.1071],
         [38.1148],
         [38.0987],
         [38.0188],
         [37.9486],
         [37.9817],
         [37.9630],
         [37.8422]]], grad_fn=<ThAddBackward>)
[ 35.00550842  34.00771332  33.75392151  33.83992767  33.9924736
  34.21924973  34.57666397  34.71734619  34.49263382  34.14973831
  33.87397766  33.64935684  33.3984642   33.10677719  32.90551376
  32.89909744  33.00649261  33.1197319   33.123909    32.92527771
  32.60511398  32.31856155  32.18214035  32.13726425  32.26105118
  32.6229744   33.03728104  33.32598495  33.49668121  33.63700104
  33.74640656  33.83482742  33.73321152  33.58769989  33.42036438
  33.23311615  33.16391373  33.18213272  33.15400314  33.06104279
  33.0929718   33.5711937   34.45011902  35.40644073  36.11079025
  36.45751572  36.4979248   36.08965302  35.46100616  34.97350311
  34.58303833  34.06269073  33.39594269  32.59112549  31.772995
  31.13186646  30.66980362  30.36875153  30.21305275  30.17008591
  30.18286705  30.27270889  30.51226807  30.86106873  31.2322216
  31.48238754  31.57117462  31.49415779  31.33018684  31.19667816
  31.17282486  31.16834831  30.97296906  30.67952919  30.65953636
  31.02145195  31.45464325  31.75660706  31.88789368  31.88292503
  31.80373764  31.75820923  31.89263535  32.29058838  32.95904541
  33.68860245  34.36199951  34.9302063   35.16542053  34.98278046
  34.58271027  34.14915848  33.77920532  33.47491837  33.23310089
  33.10261154  33.13274002  32.9131279   32.67523956  32.63039017
  32.67909622  32.58163834  32.34915543  32.09429932  31.88192177
  31.73985672  31.69877625  31.87099075  32.31052017  32.82584     32.73748016
  32.01221848  31.23162079  30.69229317  30.36618042  30.08786011
  29.82690239  29.59569168  29.34461594  29.05371475  28.75279808
  28.45791245  28.22469711  28.01874352  27.78863144  27.58887672
  27.50766754  27.56695175  27.9751873   28.2912426   28.5983181
  28.83577156  28.92698669  28.89390182  28.7519722   28.54351616
  28.29946136  28.0534935   27.8898468   27.79911804  27.78455162
  27.86377907  28.11413383  28.51642609  28.95635223  29.34020424
  29.69768143  30.15705872  30.66190338  31.11449051  31.49203491
  31.61872101  31.55588531  31.41417122  31.2016716   31.01935196
  30.89933205  30.83053589  30.79285049  30.78784561  31.12432671
  31.19851875  31.24225426  31.30496979  31.55025101  31.91501236
  32.5109024   33.26647568  33.77873993  33.99088287  34.09706116
  34.1764679   34.19606781  34.76018906  35.9327774   37.01520538
  37.71230316  38.02290344  38.09703827  38.10711288  38.11479187
  38.09873962  38.01882935  37.94864655  37.98165131  37.96297836
  37.84223175]
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[37.1442],
         [36.6567],
         [36.7842],
         [37.1485],
         [37.5016],
         [37.8744],
         [38.3476],
         [38.5725],
         [38.3880],
         [38.0518],
         [37.7673],
         [37.5273],
         [37.2563],
         [36.9393],
         [36.7130],
         [36.6925],
         [36.7986],
         [36.9177],
         [36.9270],
         [36.7218],
         [36.3804],
         [36.0662],
         [35.9061],
         [35.8467],
         [35.9668],
         [36.3464],
         [36.7935],
         [37.1165],
         [37.3147],
         [37.4773],
         [37.6044],
         [37.7064]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[38.4607],
         [38.8334],
         [39.0419],
         [39.1029],
         [39.1961],
         [39.3274],
         [39.3742],
         [39.3255],
         [39.3861],
         [39.9140],
         [40.8909],
         [41.9837],
         [42.8318],
         [43.2959],
         [43.4122],
         [43.0226],
         [42.3516],
         [41.7971],
         [41.3355],
         [40.7355],
         [39.9703],
         [39.0477],
         [38.0925],
         [37.3172],
         [36.7374],
         [36.3416],
         [36.1167],
         [36.0296],
         [36.0173],
         [36.0984],
         [36.3494],
         [36.7322]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.5670],
         [38.1239],
         [38.4455],
         [38.5214],
         [38.4453],
         [38.3563],
         [38.3584],
         [38.3728],
         [38.1779],
         [37.8581],
         [37.8136],
         [38.1883],
         [38.6721],
         [39.0359],
         [39.2220],
         [39.2508],
         [39.1851],
         [39.1404],
         [39.2835],
         [39.7182],
         [40.4670],
         [41.3149],
         [42.1237],
         [42.8297],
         [43.1778],
         [43.0519],
         [42.6496],
         [42.1730],
         [41.7395],
         [41.3679],
         [41.0621],
         [40.8795]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.9430],
         [40.7202],
         [40.4590],
         [40.3927],
         [40.4340],
         [40.3274],
         [40.0701],
         [39.7767],
         [39.5194],
         [39.3349],
         [39.2622],
         [39.4266],
         [39.8984],
         [40.4829],
         [40.4481],
         [39.6911],
         [38.8170],
         [38.1632],
         [37.7316],
         [37.3641],
         [37.0261],
         [36.7268],
         [36.4126],
         [36.0563],
         [35.6878],
         [35.3238],
         [35.0250],
         [34.7603],
         [34.4735],
         [34.2195],
         [34.0952],
         [34.1314]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[34.3197],
         [34.5199],
         [34.7574],
         [34.9626],
         [35.0372],
         [34.9881],
         [34.8235],
         [34.5820],
         [34.2970],
         [34.0043],
         [33.7967],
         [33.6710],
         [33.6326],
         [33.7030],
         [33.9655],
         [34.4076],
         [34.9082],
         [35.3626],
         [35.7920],
         [36.3307],
         [36.9254],
         [37.4707],
         [37.9369],
         [38.1336],
         [38.1078],
         [37.9782],
         [37.7549],
         [37.5495],
         [37.4047],
         [37.3148],
         [37.2611],
         [37.2461]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[37.0573],
         [36.8094],
         [36.6082],
         [36.5025],
         [36.6435],
         [36.9604],
         [37.5546],
         [38.3549],
         [38.9269],
         [39.1886],
         [39.3292],
         [39.4334],
         [39.4677],
         [40.0655],
         [41.3310],
         [42.5408],
         [43.3619],
         [43.7702],
         [43.9070],
         [43.9534],
         [43.9822],
         [43.9776],
         [43.9004],
         [43.8268],
         [43.8590],
         [43.8408],
         [43.7128]]], grad_fn=<ThAddBackward>)
[ 37.1442337   36.6566658   36.7841568   37.14852905  37.50164413
  37.87443542  38.34762573  38.57252502  38.38796616  38.0518074
  37.76726913  37.52725983  37.25626373  36.93933105  36.71296692
  36.69245148  36.7986145   36.91767883  36.9270401   36.72182465
  36.38037109  36.06617737  35.90609741  35.84674454  35.9667511
  36.34642792  36.79354095  37.11650467  37.31465912  37.47727203
  37.60436249  37.70644379  38.46072769  38.83344269  39.04185867
  39.10289764  39.19607925  39.3274498   39.37421036  39.32545471
  39.38611603  39.91404343  40.89087296  41.98370743  42.83176804
  43.29589844  43.41215897  43.02264786  42.35160828  41.79711151
  41.3355217   40.73546219  39.97031784  39.04766464  38.09246445
  37.31721878  36.73735046  36.34156799  36.11674118  36.02959061
  36.01734543  36.09840012  36.34941483  36.73223495  37.56696701
  38.12393188  38.4455452   38.52138901  38.44529343  38.35625839
  38.3584404   38.3727684   38.17790985  37.85808182  37.8135643
  38.18829727  38.67205429  39.03593063  39.22200775  39.25080109
  39.18513107  39.14037704  39.28349304  39.71815872  40.46700668
  41.31490707  42.12373352  42.82968521  43.17776108  43.05187225
  42.64963913  42.17298508  41.7395134   41.36785507  41.06209946
  40.87953568  40.94295502  40.7202034   40.45902252  40.39271164
  40.43397141  40.32742691  40.07011032  39.77667618  39.51935196
  39.33493805  39.2622261   39.42664719  39.89837646  40.48294067
  40.44812393  39.69105148  38.81699753  38.16318893  37.73164749
  37.36414719  37.02605438  36.72679901  36.41256714  36.05630112
  35.68782425  35.3237915   35.02498245  34.76025009  34.47347641
  34.21954346  34.0952301   34.13140869  34.31973648  34.51992798
  34.75738144  34.96264648  35.03722382  34.98813629  34.82351685
  34.58201599  34.29698944  34.00432587  33.79666901  33.67103195
  33.63262177  33.70303345  33.96554565  34.40762329  34.90822601
  35.36264801  35.79202271  36.33068848  36.92540359  37.47066498
  37.9369278   38.13357925  38.10780334  37.97821808  37.75493622
  37.5494957   37.40466309  37.31477356  37.26112366  37.24605942
  37.05728912  36.80935287  36.60824585  36.50246048  36.64350891
  36.96043396  37.55456924  38.35490036  38.92692566  39.18855667
  39.32917786  39.4333992   39.46770477  40.06549454  41.33103561
  42.54075241  43.36185455  43.77019119  43.90704727  43.95344925
  43.98220825  43.97763062  43.90040207  43.82676315  43.85898209
  43.84080124  43.71279526]
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[42.0500],
         [40.9549],
         [40.6383],
         [40.7094],
         [40.8767],
         [41.1367],
         [41.5503],
         [41.7422],
         [41.5258],
         [41.1551],
         [40.8382],
         [40.5695],
         [40.2719],
         [39.9282],
         [39.6779],
         [39.6429],
         [39.7450],
         [39.8661],
         [39.8778],
         [39.6668],
         [39.3094],
         [38.9746],
         [38.7966],
         [38.7257],
         [38.8422],
         [39.2341],
         [39.7048],
         [40.0529],
         [40.2719],
         [40.4518],
         [40.5929],
         [40.7060]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.3302],
         [40.0279],
         [39.7314],
         [39.4452],
         [39.3048],
         [39.2825],
         [39.2263],
         [39.1089],
         [39.1253],
         [39.6291],
         [40.5946],
         [41.6787],
         [42.5156],
         [42.9651],
         [43.0664],
         [42.6615],
         [41.9763],
         [41.4163],
         [40.9542],
         [40.3539],
         [39.5881],
         [38.6663],
         [37.7130],
         [36.9443],
         [36.3733],
         [35.9872],
         [35.7721],
         [35.6938],
         [35.6885],
         [35.7751],
         [36.0313],
         [36.4187]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.2984],
         [36.2805],
         [36.1691],
         [35.9480],
         [35.6765],
         [35.4636],
         [35.3880],
         [35.3519],
         [35.1259],
         [34.7928],
         [34.7475],
         [35.1191],
         [35.5871],
         [35.9256],
         [36.0864],
         [36.0951],
         [36.0179],
         [35.9680],
         [36.1080],
         [36.5332],
         [37.2591],
         [38.0676],
         [38.8248],
         [39.4747],
         [39.7697],
         [39.6049],
         [39.1866],
         [38.7149],
         [38.2996],
         [37.9526],
         [37.6727],
         [37.5143]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[37.0309],
         [36.5023],
         [36.0284],
         [35.8277],
         [35.7803],
         [35.6159],
         [35.3289],
         [35.0296],
         [34.7813],
         [34.6126],
         [34.5552],
         [34.7266],
         [35.1874],
         [35.7391],
         [35.6679],
         [34.9070],
         [34.0723],
         [33.4809],
         [33.1147],
         [32.8044],
         [32.5167],
         [32.2626],
         [31.9898],
         [31.6741],
         [31.3476],
         [31.0263],
         [30.7694],
         [30.5434],
         [30.2927],
         [30.0734],
         [29.9794],
         [30.0361]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[30.0898],
         [30.2071],
         [30.3731],
         [30.5197],
         [30.5493],
         [30.4718],
         [30.2956],
         [30.0581],
         [29.7896],
         [29.5222],
         [29.3431],
         [29.2433],
         [29.2241],
         [29.3050],
         [29.5646],
         [29.9860],
         [30.4484],
         [30.8540],
         [31.2306],
         [31.7128],
         [32.2440],
         [32.7207],
         [33.1192],
         [33.2577],
         [33.1928],
         [33.0457],
         [32.8217],
         [32.6281],
         [32.4999],
         [32.4260],
         [32.3853],
         [32.3795]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[32.3574],
         [32.2195],
         [32.1070],
         [32.0661],
         [32.2479],
         [32.5796],
         [33.1609],
         [33.9206],
         [34.4333],
         [34.6388],
         [34.7374],
         [34.8119],
         [34.8256],
         [35.3975],
         [36.6052],
         [37.7174],
         [38.4279],
         [38.7405],
         [38.8081],
         [38.8108],
         [38.8129],
         [38.7922],
         [38.7072],
         [38.6327],
         [38.6658],
         [38.6474],
         [38.5220]]], grad_fn=<ThAddBackward>)
[ 42.05002975  40.95490646  40.63834763  40.70940018  40.87671661
  41.13673401  41.55025482  41.74222183  41.52582169  41.15507507
  40.83816147  40.56948853  40.27187729  39.92824936  39.67785263
  39.64288712  39.7449913   39.86610413  39.87776566  39.66684341
  39.30939484  38.97458267  38.79657745  38.72565079  38.84216309
  39.23413849  39.70477676  40.05286789  40.2718544   40.45183182
  40.59291458  40.70600128  40.33018494  40.02790833  39.73139191
  39.44519424  39.30475616  39.28248215  39.22634888  39.10894775
  39.12528992  39.62905884  40.59455109  41.67869568  42.51556396
  42.96511459  43.06643295  42.66150665  41.97626495  41.41630936
  40.95419693  40.3539238   39.58808136  38.66631699  37.71296692
  36.94427109  36.37328339  35.98723221  35.77205276  35.69377899
  35.68849945  35.77510071  36.03134918  36.41873169  36.29839325
  36.28047943  36.16905212  35.94802475  35.67650986  35.46362686
  35.38799286  35.35191727  35.12588501  34.79279327  34.74753189
  35.1190567   35.58706665  35.92560959  36.08636475  36.09513092
  36.0178566   35.96800995  36.10801697  36.53321075  37.25909042
  38.0676384   38.82484055  39.47465515  39.76967239  39.60492706
  39.1866188   38.71487045  38.29962158  37.95258331  37.67271423
  37.51427841  37.0308609   36.50231171  36.02835464  35.82770538
  35.7803421   35.61591339  35.3289299   35.02959442  34.78131485
  34.61260223  34.55516052  34.72655487  35.18738556  35.73912811
  35.66793823  34.90704727  34.07228851  33.48094559  33.11465836
  32.80441284  32.51673126  32.26257706  31.9898262   31.67412567
  31.34756088  31.0263195   30.76938248  30.54338074  30.29265213
  30.07336807  29.97935677  30.03607368  30.08977699  30.20706177
  30.37306404  30.51967239  30.54933548  30.47179031  30.29564667
  30.05805016  29.78961563  29.52221489  29.34314728  29.24327278
  29.22413635  29.30495834  29.5646019   29.98604393  30.44836617
  30.85396576  31.23060226  31.71282578  32.24398804  32.72067261
  33.11919403  33.25765991  33.19284058  33.0456543   32.82170868
  32.62807083  32.49990463  32.42598724  32.38532257  32.37953186
  32.35742569  32.21947098  32.10703278  32.06606674  32.24793625
  32.57959366  33.16091537  33.92061615  34.4332695   34.63877869
  34.73735809  34.81190109  34.82557678  35.39748001  36.60519791
  37.71741486  38.42792892  38.74052429  38.80811691  38.81076431
  38.81288528  38.79217148  38.7071991   38.63266373  38.66582108
  38.6473732   38.52197647]
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[37.4003],
         [36.6709],
         [36.6421],
         [36.9068],
         [37.1918],
         [37.5186],
         [37.9639],
         [38.1645],
         [37.9522],
         [37.5951],
         [37.3000],
         [37.0554],
         [36.7820],
         [36.4636],
         [36.2392],
         [36.2260],
         [36.3403],
         [36.4644],
         [36.4746],
         [36.2650],
         [35.9177],
         [35.6018],
         [35.4451],
         [35.3918],
         [35.5186],
         [35.9093],
         [36.3639],
         [36.6869],
         [36.8803],
         [37.0383],
         [37.1608],
         [37.2585]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[37.6425],
         [37.7909],
         [37.8288],
         [37.7704],
         [37.7844],
         [37.8650],
         [37.8751],
         [37.8008],
         [37.8467],
         [38.3712],
         [39.3457],
         [40.4238],
         [41.2426],
         [41.6692],
         [41.7500],
         [41.3298],
         [40.6390],
         [40.0856],
         [39.6349],
         [39.0466],
         [38.2945],
         [37.3903],
         [36.4585],
         [35.7159],
         [35.1709],
         [34.8084],
         [34.6128],
         [34.5495],
         [34.5545],
         [34.6476],
         [34.9075],
         [35.2944]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.1177],
         [36.6638],
         [36.9679],
         [37.0238],
         [36.9324],
         [36.8348],
         [36.8347],
         [36.8482],
         [36.6496],
         [36.3272],
         [36.2903],
         [36.6775],
         [37.1656],
         [37.5220],
         [37.6958],
         [37.7111],
         [37.6356],
         [37.5855],
         [37.7291],
         [38.1662],
         [38.9150],
         [39.7535],
         [40.5423],
         [41.2224],
         [41.5387],
         [41.3797],
         [40.9544],
         [40.4688],
         [40.0374],
         [39.6749],
         [39.3812],
         [39.2126]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.6221],
         [39.6097],
         [39.5054],
         [39.5522],
         [39.6733],
         [39.6159],
         [39.3879],
         [39.1146],
         [38.8728],
         [38.7016],
         [38.6410],
         [38.8187],
         [39.3054],
         [39.9001],
         [39.8562],
         [39.0746],
         [38.1870],
         [37.5338],
         [37.1118],
         [36.7543],
         [36.4252],
         [36.1345],
         [35.8273],
         [35.4759],
         [35.1125],
         [34.7537],
         [34.4616],
         [34.2040],
         [33.9225],
         [33.6741],
         [33.5577],
         [33.6034]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[34.0688],
         [34.4429],
         [34.8119],
         [35.1073],
         [35.2399],
         [35.2260],
         [35.0814],
         [34.8499],
         [34.5698],
         [34.2789],
         [34.0740],
         [33.9523],
         [33.9184],
         [33.9951],
         [34.2672],
         [34.7223],
         [35.2345],
         [35.6970],
         [36.1319],
         [36.6786],
         [37.2823],
         [37.8336],
         [38.3032],
         [38.4963],
         [38.4608],
         [38.3226],
         [38.0897],
         [37.8776],
         [37.7298],
         [37.6391],
         [37.5858],
         [37.5720]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[37.6545],
         [37.5702],
         [37.4916],
         [37.4717],
         [37.6782],
         [38.0482],
         [38.6883],
         [39.5367],
         [40.1420],
         [40.4191],
         [40.5685],
         [40.6786],
         [40.7147],
         [41.3277],
         [42.6452],
         [43.8950],
         [44.7403],
         [45.1638],
         [45.3036],
         [45.3494],
         [45.3771],
         [45.3707],
         [45.2896],
         [45.2123],
         [45.2450],
         [45.2265],
         [45.0935]]], grad_fn=<ThAddBackward>)
[ 37.40028763  36.67087936  36.64214325  36.90675354  37.19176483
  37.51860809  37.96388245  38.16445541  37.95222092  37.59508514
  37.30001831  37.05542755  36.78199005  36.46360779  36.23915863
  36.22598648  36.3403244   36.46440125  36.47464371  36.26496124
  35.91767502  35.60175323  35.44512177  35.39181519  35.51858139
  35.90932465  36.36391068  36.68690491  36.88029861  37.03830719
  37.16083527  37.25850296  37.64249039  37.79088211  37.8288002
  37.77039719  37.78439713  37.86499023  37.87509918  37.80077744
  37.84666061  38.37119675  39.34570694  40.42380905  41.24262619
  41.6692276   41.75001907  41.32984543  40.63903809  40.08559036
  39.63487625  39.04656982  38.29452133  37.39027023  36.45850754
  35.7158699   35.1709404   34.80837631  34.61280441  34.54954147
  34.55448914  34.6476326   34.90746689  35.29443741  36.11772919
  36.66378021  36.96785736  37.0237999   36.93238068  36.83478165
  36.83470917  36.84817886  36.64955902  36.32722855  36.29030228
  36.67753601  37.16560364  37.52202225  37.69583893  37.71114731
  37.63558578  37.58547592  37.72907257  38.16616821  38.91498566
  39.75349426  40.54225159  41.22241211  41.53868866  41.37967682
  40.954422    40.46876526  40.03738022  39.6749115   39.38119507
  39.2126236   39.6221199   39.60966873  39.50539017  39.55221176
  39.67330933  39.61592484  39.38790512  39.11460495  38.8727684
  38.70159149  38.6409874   38.8187294   39.30543518  39.90012741
  39.85622025  39.07461548  38.18697739  37.53380966  37.11183548
  36.75427246  36.42523575  36.13451004  35.82732391  35.4759407
  35.11249924  34.75367737  34.46158981  34.20397568  33.92248535
  33.67411423  33.55772781  33.60338211  34.06881714  34.44292068
  34.8118515   35.10734177  35.23987198  35.22595215  35.08137131
  34.84993744  34.56977844  34.27892685  34.07404327  33.95230103
  33.91837692  33.9950943   34.26722336  34.72233963  35.2345047
  35.69702148  36.13189697  36.6786232   37.2823143   37.8335762
  38.30324554  38.49634171  38.46082687  38.32258224  38.08967972
  37.87764359  37.72981644  37.63909912  37.58575821  37.57196426
  37.6545105   37.57020569  37.49163818  37.47171402  37.67818451
  38.04816818  38.6883049   39.53673553  40.14200211  40.41912079  40.56847
  40.67858887  40.71468353  41.32771683  42.64523697  43.89497757
  44.74028778  45.1638031   45.30361176  45.34937286  45.37708282
  45.37067413  45.28964996  45.21229553  45.24501419  45.22647858
  45.09347534]
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[43.6718],
         [42.7073],
         [42.5101],
         [42.6670],
         [42.9031],
         [43.2197],
         [43.6822],
         [43.9068],
         [43.6982],
         [43.3214],
         [42.9949],
         [42.7155],
         [42.4049],
         [42.0455],
         [41.7817],
         [41.7422],
         [41.8470],
         [41.9731],
         [41.9867],
         [41.7684],
         [41.3954],
         [41.0441],
         [40.8547],
         [40.7780],
         [40.8965],
         [41.3042],
         [41.7967],
         [42.1636],
         [42.3961],
         [42.5871],
         [42.7368],
         [42.8565]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[42.7251],
         [42.5685],
         [42.3753],
         [42.1538],
         [42.0564],
         [42.0660],
         [42.0293],
         [41.9211],
         [41.9458],
         [42.4759],
         [43.4949],
         [44.6473],
         [45.5464],
         [46.0406],
         [46.1662],
         [45.7559],
         [45.0402],
         [44.4468],
         [43.9523],
         [43.3133],
         [42.4992],
         [41.5198],
         [40.5020],
         [39.6752],
         [39.0557],
         [38.6326],
         [38.3922],
         [38.2994],
         [38.2869],
         [38.3733],
         [38.6397],
         [39.0471]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[39.1378],
         [39.2571],
         [39.2437],
         [39.0806],
         [38.8385],
         [38.6386],
         [38.5714],
         [38.5412],
         [38.3098],
         [37.9597],
         [37.9054],
         [38.2903],
         [38.7862],
         [39.1531],
         [39.3355],
         [39.3551],
         [39.2802],
         [39.2292],
         [39.3755],
         [39.8228],
         [40.5926],
         [41.4601],
         [42.2799],
         [42.9900],
         [43.3285],
         [43.1775],
         [42.7478],
         [42.2501],
         [41.8036],
         [41.4260],
         [41.1184],
         [40.9392]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.2800],
         [39.6403],
         [39.0747],
         [38.8119],
         [38.7261],
         [38.5313],
         [38.2148],
         [37.8873],
         [37.6139],
         [37.4253],
         [37.3552],
         [37.5271],
         [38.0067],
         [38.5904],
         [38.5317],
         [37.7423],
         [36.8616],
         [36.2246],
         [35.8210],
         [35.4798],
         [35.1652],
         [34.8874],
         [34.5919],
         [34.2516],
         [33.8998],
         [33.5528],
         [33.2727],
         [33.0262],
         [32.7548],
         [32.5163],
         [32.4092],
         [32.4618]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.1448],
         [32.0554],
         [32.0742],
         [32.1251],
         [32.0904],
         [31.9678],
         [31.7579],
         [31.4929],
         [31.2018],
         [30.9150],
         [30.7218],
         [30.6127],
         [30.5887],
         [30.6698],
         [30.9361],
         [31.3734],
         [31.8552],
         [32.2787],
         [32.6729],
         [33.1759],
         [33.7307],
         [34.2299],
         [34.6480],
         [34.7975],
         [34.7341],
         [34.5838],
         [34.3519],
         [34.1499],
         [34.0152],
         [33.9368],
         [33.8933],
         [33.8863]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.5135],
         [33.1735],
         [32.9146],
         [32.7775],
         [32.8994],
         [33.1999],
         [33.7649],
         [34.5262],
         [35.0416],
         [35.2393],
         [35.3335],
         [35.4057],
         [35.4164],
         [35.9922],
         [37.2283],
         [38.3686],
         [39.0882],
         [39.4029],
         [39.4678],
         [39.4678],
         [39.4681],
         [39.4456],
         [39.3580],
         [39.2813],
         [39.3152],
         [39.2965],
         [39.1679]]], grad_fn=<ThAddBackward>)
[ 43.67178345  42.70727158  42.51008606  42.6669693   42.90308762
  43.21965027  43.6822052   43.90681458  43.69820404  43.32135773
  42.99485779  42.7154541   42.40488052  42.04547119  41.78165436
  41.74219894  41.84695053  41.97306442  41.98672485  41.76839066
  41.39543915  41.04407501  40.85474396  40.7780304   40.89648819
  41.30421448  41.79666138  42.16356659  42.39606094  42.58707428
  42.736763    42.85652924  42.72509003  42.56847382  42.37532043
  42.15380859  42.056427    42.06601334  42.02933121  41.92111206
  41.94578171  42.47585297  43.49491501  44.64729309  45.54642868
  46.0406456   46.16617584  45.75588989  45.04018402  44.44681931
  43.9522934   43.31332397  42.49920273  41.5197525   40.50203705
  39.67518234  39.05573654  38.63256454  38.39218521  38.29936218
  38.28686142  38.37330627  38.63969421  39.04708481  39.13776016
  39.25710678  39.2436676   39.0806427   38.83848572  38.63859177
  38.57136917  38.54121017  38.30978012  37.95966721  37.90542221
  38.29028702  38.78621292  39.15309525  39.33549118  39.3551445
  39.28017044  39.22917938  39.37546158  39.82278061  40.59255981
  41.46007156  42.27985001  42.99000168  43.32853699  43.17750931
  42.74778366  42.25005722  41.80358505  41.42599869  41.1183815
  40.9392128   40.28002548  39.64025879  39.07470703  38.81186295
  38.72608185  38.53133774  38.21475983  37.88729095  37.61390686
  37.42527771  37.35522842  37.52706146  38.00669479  38.59035873
  38.53169632  37.74231339  36.86161804  36.22457886  35.82102966
  35.47980499  35.1651535   34.88738632  34.59187317  34.25163269
  33.89975739  33.55280304  33.27269363  33.02620316  32.75479889
  32.51631165  32.40917206  32.46177673  32.14477921  32.05537796
  32.07423401  32.1251297   32.0904007   31.96780205  31.75786781
  31.49294853  31.2018261   30.91499138  30.72176552  30.61269569
  30.58868599  30.66980934  30.93605804  31.37338829  31.85518074
  32.27866745  32.67293549  33.1758728   33.73073196  34.22986603
  34.64797211  34.79754257  34.73406982  34.58384323  34.35188675
  34.1498642   34.01521301  33.93676758  33.89325714  33.88634109
  33.5135231   33.17352295  32.91456223  32.77747726  32.89942551
  33.19994354  33.7649231   34.52619934  35.04157257  35.239254
  35.33354187  35.40567017  35.41641998  35.99216461  37.22831345
  38.36857986  39.08815384  39.40293503  39.46778488  39.46779251
  39.46813965  39.44564819  39.35797882  39.2812767   39.31520081
  39.29650879  39.16793823]
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[37.6620],
         [36.7206],
         [36.5537],
         [36.7290],
         [36.9568],
         [37.2462],
         [37.6663],
         [37.8473],
         [37.6145],
         [37.2441],
         [36.9433],
         [36.6968],
         [36.4226],
         [36.1041],
         [35.8821],
         [35.8745],
         [35.9940],
         [36.1209],
         [36.1308],
         [35.9175],
         [35.5668],
         [35.2507],
         [35.0975],
         [35.0484],
         [35.1784],
         [35.5765],
         [36.0365],
         [36.3571],
         [36.5460],
         [36.7005],
         [36.8199],
         [36.9148]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[36.9746],
         [36.9343],
         [36.8344],
         [36.6843],
         [36.6400],
         [36.6844],
         [36.6696],
         [36.5784],
         [36.6146],
         [37.1335],
         [38.1024],
         [39.1666],
         [39.9600],
         [40.3563],
         [40.4105],
         [39.9713],
         [39.2731],
         [38.7266],
         [38.2880],
         [37.7117],
         [36.9733],
         [36.0865],
         [35.1769],
         [34.4617],
         [33.9444],
         [33.6067],
         [33.4319],
         [33.3844],
         [33.3999],
         [33.4999],
         [33.7628],
         [34.1500]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[34.6857],
         [35.0534],
         [35.2202],
         [35.1804],
         [35.0274],
         [34.8935],
         [34.8729],
         [34.8733],
         [34.6656],
         [34.3419],
         [34.3108],
         [34.7017],
         [35.1856],
         [35.5275],
         [35.6826],
         [35.6840],
         [35.6004],
         [35.5475],
         [35.6905],
         [36.1223],
         [36.8586],
         [37.6754],
         [38.4305],
         [39.0727],
         [39.3521],
         [39.1632],
         [38.7252],
         [38.2423],
         [37.8236],
         [37.4784],
         [37.2028],
         [37.0514]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[37.1844],
         [37.0126],
         [36.7955],
         [36.7706],
         [36.8427],
         [36.7507],
         [36.5034],
         [36.2253],
         [35.9884],
         [35.8268],
         [35.7762],
         [35.9589],
         [36.4410],
         [37.0194],
         [36.9488],
         [36.1568],
         [35.2897],
         [34.6749],
         [34.2947],
         [33.9735],
         [33.6761],
         [33.4137],
         [33.1319],
         [32.8049],
         [32.4669],
         [32.1341],
         [31.8680],
         [31.6344],
         [31.3748],
         [31.1478],
         [31.0510],
         [31.1108]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.6249],
         [32.0237],
         [32.4028],
         [32.6958],
         [32.8207],
         [32.8002],
         [32.6533],
         [32.4254],
         [32.1545],
         [31.8772],
         [31.6884],
         [31.5817],
         [31.5595],
         [31.6437],
         [31.9162],
         [32.3642],
         [32.8591],
         [33.2958],
         [33.7030],
         [34.2203],
         [34.7916],
         [35.3068],
         [35.7393],
         [35.8983],
         [35.8382],
         [35.6874],
         [35.4514],
         [35.2443],
         [35.1052],
         [35.0234],
         [34.9776],
         [34.9695]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.3989],
         [35.5228],
         [35.6008],
         [35.6884],
         [35.9671],
         [36.3830],
         [37.0459],
         [37.9016],
         [38.4978],
         [38.7553],
         [38.8896],
         [38.9886],
         [39.0160],
         [39.6238],
         [40.9330],
         [42.1574],
         [42.9617],
         [43.3441],
         [43.4512],
         [43.4754],
         [43.4895],
         [43.4743],
         [43.3877],
         [43.3084],
         [43.3418],
         [43.3226],
         [43.1888]]], grad_fn=<ThAddBackward>)
[ 37.66198349  36.72064209  36.55374908  36.72903824  36.95684433
  37.24623871  37.66633987  37.84725189  37.61454391  37.24407959
  36.94328308  36.69680023  36.42264938  36.10412598  35.88209152
  35.87447357  35.99404907  36.12088013  36.13076782  35.91752625
  35.56681061  35.25074005  35.09749985  35.04844666  35.17842102
  35.57647324  36.03649139  36.35707092  36.54598236  36.70051956
  36.81988907  36.91478729  36.97464752  36.93434906  36.83441925
  36.68433762  36.6399765   36.68435669  36.66962433  36.57839584
  36.61463928  37.13350296  38.10240936  39.1665535   39.96001434
  40.35634995  40.41054153  39.97129059  39.2730751   38.72656631
  38.28801727  37.71165848  36.97333527  36.08652115  35.17687607
  34.46173859  33.9444046   33.60666656  33.43185806  33.38438416
  33.39992142  33.49985123  33.7628479   34.14995956  34.68566132
  35.05344772  35.22018051  35.18040466  35.02739716  34.89346313
  34.87293243  34.87329483  34.66559219  34.3419342   34.3107872
  34.70167923  35.18558884  35.52747726  35.68263626  35.68401337
  35.60042572  35.54754257  35.69050217  36.12229538  36.85864639
  37.67542648  38.43045425  39.0727005   39.3520546   39.16320038
  38.72519302  38.24227905  37.82363129  37.47842789  37.20279694
  37.05142593  37.18435669  37.01264954  36.79551697  36.77057648
  36.84274673  36.75068665  36.50338364  36.22532654  35.98838425
  35.82679367  35.77615356  35.95887375  36.440979    37.01935577
  36.94882202  36.15675354  35.28966522  34.67492676  34.29470825
  33.97350693  33.6761322   33.41365433  33.13186646  32.80494308
  32.46685028  32.13405228  31.86797523  31.63436317  31.37478447
  31.1478157   31.05103111  31.11081886  31.6248951   32.02373886
  32.40280151  32.69583511  32.82071304  32.80015945  32.65328979
  32.42538452  32.15446091  31.87719536  31.68844414  31.58167648
  31.55954742  31.64374733  31.91618538  32.36422348  32.85905838
  33.29580307  33.70302963  34.22029495  34.79158783  35.30680084
  35.73933792  35.89829254  35.83816528  35.68743896  35.45141602
  35.24432755  35.1052475   35.02342606  34.9776268   34.96952057
  35.39893341  35.5227623   35.60083389  35.68842697  35.96709442
  36.38298798  37.04587555  37.90158463  38.49776459  38.75530243
  38.88962173  38.98864365  39.01602554  39.62380219  40.93296051
  42.15740967  42.96170425  43.34407806  43.45121002  43.4753685
  43.48952484  43.47431946  43.38772583  43.30838776  43.3418045
  43.32263947  43.188797  ]
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[42.1468],
         [41.4218],
         [41.4194],
         [41.7127],
         [42.0401],
         [42.4167],
         [42.9167],
         [43.1600],
         [42.9570],
         [42.5840],
         [42.2634],
         [41.9904],
         [41.6853],
         [41.3308],
         [41.0736],
         [41.0427],
         [41.1547],
         [41.2847],
         [41.2986],
         [41.0774],
         [40.7022],
         [40.3524],
         [40.1682],
         [40.0973],
         [40.2219],
         [40.6367],
         [41.1317],
         [41.4951],
         [41.7217],
         [41.9071],
         [42.0517],
         [42.1670]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[42.3785],
         [42.4265],
         [42.3861],
         [42.2676],
         [42.2415],
         [42.3002],
         [42.2948],
         [42.2058],
         [42.2453],
         [42.7944],
         [43.8367],
         [45.0103],
         [45.9212],
         [46.4172],
         [46.5384],
         [46.1153],
         [45.3842],
         [44.7817],
         [44.2813],
         [43.6342],
         [42.8099],
         [41.8186],
         [40.7897],
         [39.9567],
         [39.3349],
         [38.9123],
         [38.6748],
         [38.5860],
         [38.5774],
         [38.6682],
         [38.9412],
         [39.3563]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[39.7434],
         [40.0428],
         [40.1619],
         [40.0849],
         [39.8968],
         [39.7310],
         [39.6874],
         [39.6731],
         [39.4468],
         [39.0944],
         [39.0425],
         [39.4396],
         [39.9506],
         [40.3298],
         [40.5199],
         [40.5420],
         [40.4661],
         [40.4140],
         [40.5639],
         [41.0233],
         [41.8143],
         [42.7060],
         [43.5501],
         [44.2827],
         [44.6335],
         [44.4810],
         [44.0411],
         [43.5297],
         [43.0697],
         [42.6803],
         [42.3629],
         [42.1774]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[41.7794],
         [41.2851],
         [40.8218],
         [40.6283],
         [40.5907],
         [40.4236],
         [40.1184],
         [39.7923],
         [39.5153],
         [39.3221],
         [39.2496],
         [39.4265],
         [39.9230],
         [40.5299],
         [40.4775],
         [39.6685],
         [38.7562],
         [38.0884],
         [37.6604],
         [37.2986],
         [36.9659],
         [36.6723],
         [36.3612],
         [36.0042],
         [35.6350],
         [35.2706],
         [34.9750],
         [34.7147],
         [34.4292],
         [34.1778],
         [34.0622],
         [34.1126]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[33.9845],
         [34.0106],
         [34.1190],
         [34.2325],
         [34.2382],
         [34.1381],
         [33.9364],
         [33.6691],
         [33.3687],
         [33.0686],
         [32.8629],
         [32.7443],
         [32.7148],
         [32.7961],
         [33.0726],
         [33.5289],
         [34.0358],
         [34.4871],
         [34.9084],
         [35.4426],
         [36.0317],
         [36.5647],
         [37.0146],
         [37.1850],
         [37.1299],
         [36.9796],
         [36.7397],
         [36.5273],
         [36.3832],
         [36.2973],
         [36.2487],
         [36.2390]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.7478],
         [35.3358],
         [35.0220],
         [34.8469],
         [34.9516],
         [35.2483],
         [35.8290],
         [36.6194],
         [37.1605],
         [37.3795],
         [37.4863],
         [37.5664],
         [37.5812],
         [38.1812],
         [39.4716],
         [40.6595],
         [41.4277],
         [41.7795],
         [41.8641],
         [41.8740],
         [41.8795],
         [41.8590],
         [41.7697],
         [41.6902],
         [41.7242],
         [41.7047],
         [41.5716]]], grad_fn=<ThAddBackward>)
[ 42.14680099  41.42177963  41.41944504  41.71269608  42.04008865
  42.41671371  42.91666031  43.16004562  42.95703506  42.583992
  42.26338959  41.99039459  41.68534088  41.33079147  41.07355499
  41.04273224  41.15469742  41.28473282  41.29862213  41.07738113
  40.70222473  40.35237122  40.16823196  40.09728622  40.22187042
  40.63667679  41.13174438  41.49510193  41.72165298  41.90714645
  42.05170059  42.16701889  42.37850952  42.42647934  42.38611984
  42.26762009  42.24146652  42.30020142  42.29480362  42.20576477
  42.24532318  42.79442215  43.83673477  45.01034164  45.92123032
  46.41719818  46.53844833  46.115345    45.38418961  44.78168869
  44.28125381  43.63420486  42.80994797  41.81864548  40.78970337
  39.956707    39.33491898  38.91231155  38.67479324  38.58599472
  38.57736969  38.66822433  38.94121552  39.35629272  39.7433815
  40.04276276  40.16194534  40.08491135  39.89677048  39.73098755
  39.68744659  39.67314529  39.44675827  39.09438324  39.04247665
  39.43960571  39.95064926  40.32978058  40.5198822   40.54195023
  40.46607208  40.41398239  40.56394577  41.02330399  41.81425095
  42.70601273  43.55012131  44.28273392  44.63352585  44.48102951
  44.04110336  43.5296936   43.0697403   42.68034744  42.362854
  42.17744827  41.77941513  41.28507996  40.82182693  40.62833023
  40.5906601   40.42356491  40.11835098  39.79227066  39.51531982
  39.32212448  39.24964523  39.42652512  39.92303848  40.52987289
  40.47746658  39.66851425  38.75624084  38.08841324  37.66038895
  37.29858017  36.96587753  36.67229462  36.36120224  36.00420761
  35.6350441   35.2706337   34.9750061   34.71465302  34.42916489
  34.17780685  34.06220627  34.1125679   33.98445129  34.01060104
  34.11895752  34.23253632  34.23815155  34.13805008  33.93638611
  33.66914749  33.36874771  33.06856155  32.86289597  32.74429321
  32.71479034  32.79611206  33.07255173  33.52888107  34.03576279
  34.48708344  34.90837479  35.44261932  36.03174591  36.56470871
  37.01457214  37.18501663  37.12992096  36.97956085  36.73970795
  36.52727509  36.38318253  36.29733276  36.24868774  36.23897552
  35.74780273  35.33584976  35.02201462  34.84685516  34.95155334
  35.2483139   35.8289566   36.61936188  37.16047287  37.3794632
  37.48628616  37.5664444   37.58119965  38.18119049  39.47164536
  40.65953827  41.4276886   41.77953339  41.86407471  41.8739624
  41.87945557  41.85896301  41.76972198  41.69020081  41.72420502
  41.70474243  41.57158661]
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[39.9146],
         [38.8691],
         [38.6409],
         [38.7755],
         [38.9845],
         [39.2727],
         [39.7034],
         [39.8907],
         [39.6536],
         [39.2704],
         [38.9552],
         [38.6946],
         [38.4058],
         [38.0713],
         [37.8357],
         [37.8217],
         [37.9414],
         [38.0715],
         [38.0829],
         [37.8638],
         [37.5005],
         [37.1700],
         [37.0059],
         [36.9503],
         [37.0829],
         [37.4926],
         [37.9686],
         [38.3069],
         [38.5095],
         [38.6748],
         [38.8024],
         [38.9039]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[38.8805],
         [38.7922],
         [38.6549],
         [38.4765],
         [38.4144],
         [38.4485],
         [38.4262],
         [38.3284],
         [38.3635],
         [38.8987],
         [39.8985],
         [41.0002],
         [41.8300],
         [42.2533],
         [42.3219],
         [41.8762],
         [41.1572],
         [40.5885],
         [40.1286],
         [39.5270],
         [38.7583],
         [37.8348],
         [36.8852],
         [36.1342],
         [35.5873],
         [35.2273],
         [35.0379],
         [34.9823],
         [34.9942],
         [35.0950],
         [35.3662],
         [35.7661]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.2471],
         [36.5887],
         [36.7347],
         [36.6775],
         [36.5089],
         [36.3625],
         [36.3348],
         [36.3307],
         [36.1147],
         [35.7795],
         [35.7451],
         [36.1453],
         [36.6406],
         [36.9946],
         [37.1602],
         [37.1651],
         [37.0810],
         [37.0269],
         [37.1739],
         [37.6201],
         [38.3801],
         [39.2229],
         [40.0076],
         [40.6779],
         [40.9739],
         [40.7877],
         [40.3406],
         [39.8428],
         [39.4084],
         [39.0485],
         [38.7603],
         [38.6002]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.6625],
         [38.4431],
         [38.1879],
         [38.1396],
         [38.1983],
         [38.0943],
         [37.8346],
         [37.5445],
         [37.2969],
         [37.1272],
         [37.0722],
         [37.2588],
         [37.7537],
         [38.3466],
         [38.2772],
         [37.4692],
         [36.5781],
         [35.9414],
         [35.5447],
         [35.2097],
         [34.9002],
         [34.6273],
         [34.3349],
         [33.9965],
         [33.6466],
         [33.3019],
         [33.0255],
         [32.7827],
         [32.5135],
         [32.2781],
         [32.1762],
         [32.2355]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.4445],
         [32.6655],
         [32.9143],
         [33.1203],
         [33.1856],
         [33.1245],
         [32.9495],
         [32.7018],
         [32.4170],
         [32.1300],
         [31.9358],
         [31.8263],
         [31.8035],
         [31.8889],
         [32.1667],
         [32.6202],
         [33.1200],
         [33.5613],
         [33.9717],
         [34.4948],
         [35.0711],
         [35.5897],
         [36.0253],
         [36.1824],
         [36.1182],
         [35.9634],
         [35.7230],
         [35.5130],
         [35.3727],
         [35.2905],
         [35.2449],
         [35.2374]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.0724],
         [34.8411],
         [34.6590],
         [34.5732],
         [34.7389],
         [35.0760],
         [35.6854],
         [36.4935],
         [37.0419],
         [37.2635],
         [37.3711],
         [37.4515],
         [37.4659],
         [38.0711],
         [39.3685],
         [40.5569],
         [41.3220],
         [41.6672],
         [41.7454],
         [41.7512],
         [41.7544],
         [41.7322],
         [41.6415],
         [41.5613],
         [41.5961],
         [41.5762],
         [41.4420]]], grad_fn=<ThAddBackward>)
[ 39.91455078  38.86906052  38.64086151  38.77545929  38.9845314   39.272686
  39.70342636  39.89074707  39.65357971  39.27043152  38.95517731
  38.69457245  38.40578461  38.07125854  37.83566284  37.8217392
  37.9414444   38.07153702  38.08289337  37.86384583  37.50052643
  37.16999435  37.00589752  36.95028305  37.08292389  37.49255371
  37.96864319  38.30692291  38.50954819  38.67477417  38.80240631
  38.90388107  38.88051605  38.79219055  38.65494537  38.47653961
  38.41436005  38.44854736  38.42617416  38.32841492  38.36346436
  38.89869308  39.89847565  41.00023651  41.82995605  42.25331497
  42.32186508  41.87617111  41.15716553  40.58853149  40.12861252
  39.52704239  38.75827789  37.83479691  36.88520813  36.13420868
  35.58730698  35.22732925  35.03790283  34.98231888  34.99421692
  35.09501648  35.36621094  35.76611328  36.24708557  36.58872986
  36.73471451  36.67753983  36.50886917  36.36253357  36.33476639
  36.33070755  36.11466599  35.77949524  35.74513245  36.14533615
  36.64063263  36.99459076  37.16023636  37.16513443  37.08095551
  37.02689743  37.1739006   37.62009811  38.38012314  39.22294235
  40.00761414  40.67794418  40.97391129  40.78770828  40.34057617
  39.84284592  39.40838242  39.04853439  38.76025772  38.60019684
  38.6624794   38.44313812  38.1878891   38.13959122  38.19826126
  38.09434509  37.8345871   37.54446793  37.2969017   37.12723923
  37.07220459  37.25881195  37.75374603  38.34663773  38.27717209
  37.46918488  36.57808304  35.94139862  35.5446701   35.20965576
  34.90019989  34.62727356  34.33492279  33.99650955  33.64657593
  33.30186462  33.02550507  32.78268433  32.5135498   32.27806091
  32.17621231  32.23548126  32.44449234  32.66548538  32.9143486   33.120327
  33.18562698  33.12454605  32.94945908  32.7018013   32.41695023
  32.12997818  31.93580818  31.82629776  31.8034668   31.88892174
  32.16667557  32.6201973   33.11996078  33.56127167  33.97173309
  34.49481583  35.07107544  35.58974075  36.02528381  36.18235779
  36.11817169  35.96344376  35.72296524  35.51302719  35.37273026
  35.29052353  35.24487686  35.23735046  35.07240677  34.84106445
  34.65903091  34.57315063  34.73893738  35.07604218  35.68543243
  36.49352264  37.04185867  37.26351547  37.37112427  37.45145798
  37.46591187  38.07106018  39.36845779  40.5569191   41.32196045
  41.66722107  41.74539185  41.75121307  41.75435257  41.73221588
  41.64146042  41.56134796  41.59606552  41.57622528  41.44204712]
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.1009],
         [39.2249],
         [39.1263],
         [39.3533],
         [39.6247],
         [39.9560],
         [40.4187],
         [40.6250],
         [40.3943],
         [40.0116],
         [39.6951],
         [39.4323],
         [39.1398],
         [38.8002],
         [38.5609],
         [38.5466],
         [38.6681],
         [38.8003],
         [38.8118],
         [38.5890],
         [38.2196],
         [37.8833],
         [37.7160],
         [37.6589],
         [37.7936],
         [38.2100],
         [38.6941],
         [39.0384],
         [39.2452],
         [39.4138],
         [39.5441],
         [39.6478]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.9115],
         [39.9811],
         [39.9559],
         [39.8499],
         [39.8361],
         [39.9031],
         [39.9009],
         [39.8141],
         [39.8584],
         [40.4128],
         [41.4438],
         [42.5824],
         [43.4446],
         [43.8907],
         [43.9708],
         [43.5201],
         [42.7850],
         [42.1994],
         [41.7227],
         [41.1000],
         [40.3054],
         [39.3502],
         [38.3667],
         [37.5855],
         [37.0135],
         [36.6345],
         [36.4322],
         [36.3691],
         [36.3772],
         [36.4782],
         [36.7554],
         [37.1662]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.6026],
         [37.9163],
         [38.0395],
         [37.9640],
         [37.7799],
         [37.6223],
         [37.5884],
         [37.5804],
         [37.3561],
         [37.0106],
         [36.9736],
         [37.3821],
         [37.8892],
         [38.2534],
         [38.4253],
         [38.4324],
         [38.3476],
         [38.2932],
         [38.4444],
         [38.9026],
         [39.6833],
         [40.5503],
         [41.3596],
         [42.0524],
         [42.3613],
         [42.1753],
         [41.7202],
         [41.2107],
         [40.7642],
         [40.3929],
         [40.0947],
         [39.9279]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.6109],
         [39.1634],
         [38.7423],
         [38.5873],
         [38.5767],
         [38.4245],
         [38.1312],
         [37.8190],
         [37.5576],
         [37.3801],
         [37.3215],
         [37.5100],
         [38.0113],
         [38.6095],
         [38.5305],
         [37.7063],
         [36.8043],
         [36.1643],
         [35.7677],
         [35.4323],
         [35.1217],
         [34.8478],
         [34.5533],
         [34.2123],
         [33.8597],
         [33.5128],
         [33.2355],
         [32.9917],
         [32.7208],
         [32.4843],
         [32.3837],
         [32.4460]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.3798],
         [32.4382],
         [32.5678],
         [32.6922],
         [32.7013],
         [32.6031],
         [32.4040],
         [32.1424],
         [31.8504],
         [31.5613],
         [31.3690],
         [31.2624],
         [31.2428],
         [31.3311],
         [31.6120],
         [32.0662],
         [32.5627],
         [32.9972],
         [33.4004],
         [33.9179],
         [34.4870],
         [34.9964],
         [35.4218],
         [35.5659],
         [35.4923],
         [35.3321],
         [35.0893],
         [34.8809],
         [34.7438],
         [34.6648],
         [34.6219],
         [34.6165]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[34.4647],
         [34.2453],
         [34.0744],
         [33.9979],
         [34.1734],
         [34.5154],
         [35.1293],
         [35.9354],
         [36.4727],
         [36.6815],
         [36.7806],
         [36.8552],
         [36.8658],
         [37.4771],
         [38.7746],
         [39.9568],
         [40.7036],
         [41.0270],
         [41.0894],
         [41.0866],
         [41.0853],
         [41.0604],
         [40.9674],
         [40.8872],
         [40.9237],
         [40.9030],
         [40.7677]]], grad_fn=<ThAddBackward>)
[ 40.100914    39.22488785  39.12633133  39.35331345  39.62473297
  39.95596313  40.41866684  40.62495041  40.39427185  40.01161194
  39.69510651  39.43230438  39.13984299  38.80022049  38.56085587
  38.54661179  38.66808701  38.80026627  38.8117981   38.58901596
  38.2195549   37.883255    37.71600723  37.65890503  37.79358292
  38.21001816  38.69413757  39.03844452  39.24518204  39.4138298
  39.54412079  39.647789    39.91146088  39.98110962  39.9559021
  39.84986877  39.83611298  39.90305328  39.90090179  39.8140564
  39.85838318  40.41275406  41.44382477  42.58235931  43.44461823
  43.89065552  43.97077942  43.52009201  42.78501892  42.19943619
  41.72273254  41.10004044  40.30536652  39.35018539  38.36671448
  37.58545685  37.01354218  36.63448334  36.43222809  36.36911011
  36.37722778  36.47821045  36.7553978   37.16615295  37.60257721
  37.91629791  38.0394783   37.96397018  37.77993774  37.62234497
  37.58843994  37.58036804  37.35607147  37.01062775  36.97359085
  37.38208771  37.88915253  38.25341797  38.42528915  38.43244934
  38.3476181   38.29324341  38.4444046   38.90258026  39.68333817
  40.55034637  41.35955048  42.05236053  42.36125946  42.17532349
  41.72021866  41.21069717  40.76416016  40.39293671  40.0947113
  39.92790222  39.61092758  39.16340256  38.7423439   38.58734894
  38.57666016  38.42447662  38.1312027   37.81896973  37.55763245
  37.38006592  37.32154465  37.51002884  38.01129913  38.60952759
  38.53049469  37.7062912   36.80430984  36.16428375  35.76773071
  35.43234253  35.12171555  34.84775543  34.55331421  34.21226501
  33.85974884  33.51277924  33.23553085  32.99172974  32.72076416
  32.48431778  32.3837204   32.44600677  32.37982559  32.4381752
  32.56777191  32.69218826  32.70133591  32.60313416  32.40402985
  32.14242554  31.85042191  31.56133842  31.36904907  31.26243973
  31.2428093   31.33109283  31.61196899  32.06618881  32.56274033
  32.99716568  33.40043259  33.91789627  34.4870224   34.99635696
  35.42176819  35.56592941  35.49228287  35.33207321  35.08934784
  34.88086319  34.74383163  34.66484451  34.62191391  34.61649704
  34.46466827  34.24530411  34.07444382  33.99793243  34.17340088
  34.51544952  35.1293335   35.93542099  36.47265244  36.68152618
  36.78064728  36.85522461  36.86575699  37.47711182  38.77464676
  39.95677948  40.70362473  41.0269928   41.08942795  41.086586
  41.08533478  41.06037903  40.96735764  40.88718414  40.92374039
  40.9030304   40.76770782]
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[39.4287],
         [38.5748],
         [38.5008],
         [38.7503],
         [39.0302],
         [39.3630],
         [39.8256],
         [40.0244],
         [39.7846],
         [39.3987],
         [39.0844],
         [38.8256],
         [38.5365],
         [38.2002],
         [37.9667],
         [37.9595],
         [38.0856],
         [38.2191],
         [38.2285],
         [38.0020],
         [37.6315],
         [37.2983],
         [37.1373],
         [37.0851],
         [37.2255],
         [37.6463],
         [38.1294],
         [38.4677],
         [38.6679],
         [38.8315],
         [38.9577],
         [39.0582]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.3282],
         [39.4036],
         [39.3821],
         [39.2786],
         [39.2693],
         [39.3391],
         [39.3374],
         [39.2501],
         [39.2972],
         [39.8586],
         [40.8933],
         [42.0266],
         [42.8745],
         [43.3034],
         [43.3683],
         [42.9036],
         [42.1636],
         [41.5826],
         [41.1127],
         [40.4944],
         [39.7049],
         [38.7556],
         [37.7825],
         [37.0155],
         [36.4583],
         [36.0924],
         [35.9012],
         [35.8462],
         [35.8595],
         [35.9642],
         [36.2447],
         [36.6563]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.0998],
         [37.4162],
         [37.5382],
         [37.4605],
         [37.2756],
         [37.1199],
         [37.0894],
         [37.0831],
         [36.8570],
         [36.5119],
         [36.4813],
         [36.8974],
         [37.4054],
         [37.7654],
         [37.9306],
         [37.9320],
         [37.8435],
         [37.7888],
         [37.9424],
         [38.4040],
         [39.1864],
         [40.0491],
         [40.8499],
         [41.5318],
         [41.8261],
         [41.6263],
         [41.1637],
         [40.6531],
         [40.2102],
         [39.8441],
         [39.5516],
         [39.3908]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.3079],
         [38.9643],
         [38.6191],
         [38.5207],
         [38.5475],
         [38.4149],
         [38.1322],
         [37.8271],
         [37.5719],
         [37.3999],
         [37.3470],
         [37.5440],
         [38.0557],
         [38.6595],
         [38.5656],
         [37.7225],
         [36.8124],
         [36.1747],
         [35.7833],
         [35.4509],
         [35.1415],
         [34.8686],
         [34.5736],
         [34.2316],
         [33.8783],
         [33.5312],
         [33.2554],
         [33.0124],
         [32.7411],
         [32.5055],
         [32.4082],
         [32.4752]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.5848],
         [32.7250],
         [32.9139],
         [33.0769],
         [33.1080],
         [33.0217],
         [32.8273],
         [32.5672],
         [32.2743],
         [31.9841],
         [31.7930],
         [31.6878],
         [31.6709],
         [31.7633],
         [32.0529],
         [32.5171],
         [33.0219],
         [33.4610],
         [33.8694],
         [34.3961],
         [34.9739],
         [35.4891],
         [35.9185],
         [36.0578],
         [35.9787],
         [35.8133],
         [35.5655],
         [35.3548],
         [35.2176],
         [35.1387],
         [35.0963],
         [35.0917]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.1017],
         [34.9497],
         [34.8275],
         [34.7855],
         [34.9935],
         [35.3607],
         [36.0024],
         [36.8366],
         [37.3867],
         [37.5953],
         [37.6970],
         [37.7737],
         [37.7850],
         [38.4196],
         [39.7607],
         [40.9725],
         [41.7298],
         [42.0527],
         [42.1121],
         [42.1092],
         [42.1092],
         [42.0841],
         [41.9880],
         [41.9069],
         [41.9469],
         [41.9239],
         [41.7836]]], grad_fn=<ThAddBackward>)
[ 39.42873383  38.57477188  38.5007515   38.75030899  39.03015137
  39.36304092  39.82556915  40.02441788  39.78456497  39.39870071
  39.08443832  38.82555771  38.53652573  38.2002449   37.96667099
  37.95953751  38.08560944  38.21906662  38.22851562  38.00202179
  37.63150024  37.2983284   37.13728333  37.08514404  37.22550201
  37.64632416  38.12943649  38.46773148  38.66792679  38.83152771
  38.95768356  39.05820847  39.32817841  39.40359497  39.38209534
  39.27861023  39.26926804  39.33914948  39.33740997  39.25009918
  39.29716873  39.85858917  40.89334106  42.02658081  42.87453461
  43.30335999  43.36830139  42.90356064  42.16358185  41.5826416
  41.11273193  40.49438858  39.70490265  38.75562668  37.78248978
  37.01553345  36.45829773  36.09237671  35.90117264  35.84615326
  35.85948181  35.96424103  36.24473953  36.65628815  37.09981918
  37.41617203  37.53815079  37.46045303  37.27560806  37.11985397
  37.08935928  37.08311081  36.85699463  36.51186752  36.48129654
  36.89744568  37.40538788  37.76541138  37.93056107  37.93201447
  37.84349442  37.78879929  37.942379    38.40400696  39.18638611
  40.04907608  40.84988785  41.53179169  41.82612228  41.62633514
  41.1637001   40.65308762  40.21015549  39.84412766  39.55156326
  39.39080429  39.30789566  38.96434784  38.61906052  38.52070999
  38.54745865  38.4148941   38.13217163  37.82712555  37.57186127
  37.39985657  37.34703445  37.54404831  38.05568695  38.65953827
  38.56559753  37.72253036  36.81243134  36.17473602  35.78330994
  35.45085526  35.14145279  34.86862946  34.57364655  34.23160934
  33.87832642  33.53116608  33.25538635  33.01242828  32.74108887
  32.50548172  32.40824127  32.47520065  32.58480835  32.72503662
  32.91387939  33.07693863  33.10801697  33.02173996  32.82732391
  32.56716156  32.27432251  31.98410988  31.79297447  31.68784714
  31.67087936  31.76326752  32.0528717   32.51711273  33.02192688
  33.46098328  33.86936569  34.39611053  34.97389221  35.48913956
  35.91851044  36.05776215  35.97871017  35.81333542  35.56552505
  35.35480881  35.21755219  35.13874054  35.09627914  35.09165192
  35.1016655   34.94968796  34.82751465  34.78551483  34.99354172
  35.36066818  36.00235748  36.83655548  37.38671112  37.59525681
  37.69701004  37.77373505  37.78499603  38.41957092  39.76066208
  40.97254181  41.72975159  42.05271149  42.11211777  42.1092453
  42.10923004  42.08408356  41.98802948  41.90688705  41.94685364
  41.92385864  41.78359985]
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.5707],
         [39.7743],
         [39.7637],
         [40.0638],
         [40.3752],
         [40.7341],
         [41.2210],
         [41.4248],
         [41.1729],
         [40.7755],
         [40.4545],
         [40.1896],
         [39.8913],
         [39.5433],
         [39.3046],
         [39.3014],
         [39.4333],
         [39.5705],
         [39.5767],
         [39.3375],
         [38.9525],
         [38.6097],
         [38.4472],
         [38.3948],
         [38.5441],
         [38.9833],
         [39.4823],
         [39.8278],
         [40.0323],
         [40.2008],
         [40.3309],
         [40.4351]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.5392],
         [40.4949],
         [40.3838],
         [40.2188],
         [40.1762],
         [40.2258],
         [40.2070],
         [40.1051],
         [40.1523],
         [40.7357],
         [41.8043],
         [42.9672],
         [43.8232],
         [44.2452],
         [44.2972],
         [43.8026],
         [43.0386],
         [42.4499],
         [41.9739],
         [41.3379],
         [40.5270],
         [39.5507],
         [38.5569],
         [37.7804],
         [37.2194],
         [36.8531],
         [36.6648],
         [36.6134],
         [36.6297],
         [36.7402],
         [37.0329],
         [37.4576]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.7645],
         [37.9827],
         [38.0256],
         [37.8904],
         [37.6668],
         [37.4904],
         [37.4526],
         [37.4400],
         [37.1964],
         [36.8394],
         [36.8202],
         [37.2597],
         [37.7790],
         [38.1390],
         [38.2950],
         [38.2873],
         [38.1915],
         [38.1374],
         [38.3018],
         [38.7826],
         [39.5884],
         [40.4667],
         [41.2750],
         [41.9569],
         [42.2340],
         [42.0068],
         [41.5234],
         [41.0017],
         [40.5562],
         [40.1901],
         [39.8991],
         [39.7437]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.2979],
         [38.7553],
         [38.2766],
         [38.1031],
         [38.0811],
         [37.9079],
         [37.5969],
         [37.2772],
         [37.0176],
         [36.8473],
         [36.8001],
         [37.0095],
         [37.5354],
         [38.1414],
         [38.0099],
         [37.1297],
         [36.2131],
         [35.5920],
         [35.2204],
         [34.9009],
         [34.5998],
         [34.3346],
         [34.0434],
         [33.7045],
         [33.3552],
         [33.0134],
         [32.7461],
         [32.5096],
         [32.2419],
         [32.0123],
         [31.9254],
         [32.0034]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.8548],
         [31.8568],
         [31.9473],
         [32.0410],
         [32.0217],
         [31.9023],
         [31.6869],
         [31.4172],
         [31.1223],
         [30.8363],
         [30.6550],
         [30.5592],
         [30.5508],
         [30.6500],
         [30.9470],
         [31.4124],
         [31.9092],
         [32.3324],
         [32.7255],
         [33.2415],
         [33.8044],
         [34.2996],
         [34.7077],
         [34.8190],
         [34.7214],
         [34.5466],
         [34.2960],
         [34.0903],
         [33.9611],
         [33.8894],
         [33.8525],
         [33.8520]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.6073],
         [33.3252],
         [33.1187],
         [33.0258],
         [33.2073],
         [33.5504],
         [34.1760],
         [34.9803],
         [35.4833],
         [35.6501],
         [35.7261],
         [35.7863],
         [35.7868],
         [36.4399],
         [37.7728],
         [38.9340],
         [39.6252],
         [39.8886],
         [39.9084],
         [39.8860],
         [39.8768],
         [39.8461],
         [39.7461],
         [39.6667],
         [39.7118],
         [39.6866],
         [39.5449]]], grad_fn=<ThAddBackward>)
[ 40.57065964  39.77433395  39.7637291   40.06376648  40.37517548
  40.73414993  41.22099686  41.4248085   41.17285538  40.7754631
  40.45454025  40.1896019   39.8913002   39.54333115  39.30459595
  39.3013916   39.43326187  39.57051086  39.57671356  39.33749008
  38.95245743  38.6096611   38.44720459  38.39479446  38.54413223
  38.98330688  39.48228836  39.82782745  40.0323143   40.20081711
  40.33089447  40.43512344  40.53916931  40.49492645  40.38380432
  40.21881866  40.17617798  40.22581863  40.20695496  40.10509491
  40.15229797  40.73574066  41.80434418  42.96719742  43.82317352
  44.24519348  44.29718781  43.80257416  43.03862     42.44992828
  41.97393799  41.33786011  40.52700043  39.55065155  38.55688477
  37.78042603  37.21941376  36.85308838  36.66478729  36.61340332
  36.62971115  36.7401886   37.03294754  37.45756912  37.764534
  37.98269653  38.02558517  37.8903656   37.66675949  37.49042511
  37.45263672  37.43995667  37.19644165  36.83939362  36.8201828
  37.25971985  37.77902603  38.13902283  38.2950325   38.28734589
  38.19150543  38.13742447  38.30181122  38.78258896  39.58841705
  40.46671295  41.27499008  41.95685577  42.23403168  42.00682449
  41.52340698  41.00165558  40.55618286  40.19010162  39.89911652
  39.74374008  39.29792404  38.75526428  38.27656555  38.10314941
  38.08111191  37.90790939  37.5968895   37.27721405  37.0175705
  36.84727478  36.80013275  37.00947952  37.53542328  38.14141083
  38.00988007  37.12971878  36.21305466  35.5920105   35.22038269
  34.90087891  34.59981537  34.33456421  34.04338455  33.70448685
  33.35524368  33.01343155  32.74610519  32.50955963  32.24185181
  32.01231766  31.92541504  32.00344467  31.85479927  31.85682487
  31.94726944  32.04102325  32.02173615  31.9023037   31.68691826
  31.41721153  31.12227821  30.83625984  30.65498543  30.55920792
  30.55078697  30.64997673  30.94699287  31.41239548  31.90922356
  32.33235168  32.72549438  33.24145508  33.80437469  34.29964066
  34.70771408  34.81900024  34.72143555  34.54655457  34.29595947
  34.09025955  33.96112061  33.88940811  33.85251999  33.85202026
  33.60730362  33.32518005  33.11867905  33.0257988   33.20730209
  33.55038071  34.17602921  34.98025131  35.48328781  35.65008545
  35.72610092  35.78627014  35.78676224  36.4398613   37.77278519
  38.93401337  39.62519073  39.88855743  39.90840912  39.88599777
  39.87683487  39.84607697  39.74609756  39.66665268  39.71175003
  39.68662643  39.5449028 ]
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[38.0576],
         [37.1752],
         [37.1275],
         [37.3961],
         [37.6673],
         [37.9904],
         [38.4446],
         [38.6075],
         [38.3245],
         [37.9221],
         [37.6138],
         [37.3660],
         [37.0843],
         [36.7542],
         [36.5386],
         [36.5572],
         [36.6996],
         [36.8367],
         [36.8358],
         [36.5907],
         [36.2119],
         [35.8876],
         [35.7486],
         [35.7129],
         [35.8755],
         [36.3173],
         [36.8002],
         [37.1181],
         [37.2969],
         [37.4459],
         [37.5604],
         [37.6525]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[37.8268],
         [37.8333],
         [37.7608],
         [37.6241],
         [37.6069],
         [37.6719],
         [37.6593],
         [37.5613],
         [37.6176],
         [38.2137],
         [39.2721],
         [40.3902],
         [41.1825],
         [41.5438],
         [41.5500],
         [41.0257],
         [40.2652],
         [39.7053],
         [39.2614],
         [38.6526],
         [37.8750],
         [36.9385],
         [35.9978],
         [35.2817],
         [34.7766],
         [34.4568],
         [34.3045],
         [34.2773],
         [34.3076],
         [34.4259],
         [34.7212],
         [35.1381]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[35.7965],
         [36.2174],
         [36.3982],
         [36.3513],
         [36.1875],
         [36.0551],
         [36.0498],
         [36.0564],
         [35.8197],
         [35.4721],
         [35.4723],
         [35.9294],
         [36.4467],
         [36.7932],
         [36.9332],
         [36.9133],
         [36.8108],
         [36.7574],
         [36.9267],
         [37.4131],
         [38.2172],
         [39.0767],
         [39.8584],
         [40.5106],
         [40.7528],
         [40.4967],
         [40.0018],
         [39.4837],
         [39.0513],
         [38.7003],
         [38.4243],
         [38.2836]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.5185],
         [38.3689],
         [38.1696],
         [38.1856],
         [38.2860],
         [38.1845],
         [37.9156],
         [37.6221],
         [37.3800],
         [37.2221],
         [37.1854],
         [37.4075],
         [37.9487],
         [38.5666],
         [38.4193],
         [37.5135],
         [36.5833],
         [35.9608],
         [35.5911],
         [35.2711],
         [34.9679],
         [34.7009],
         [34.4060],
         [34.0628],
         [33.7094],
         [33.3641],
         [33.0956],
         [32.8573],
         [32.5864],
         [32.3554],
         [32.2709],
         [32.3538]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.7124],
         [33.0141],
         [33.3203],
         [33.5564],
         [33.6262],
         [33.5599],
         [33.3721],
         [33.1139],
         [32.8198],
         [32.5290],
         [32.3433],
         [32.2441],
         [32.2351],
         [32.3379],
         [32.6451],
         [33.1303],
         [33.6506],
         [34.0946],
         [34.5096],
         [35.0513],
         [35.6433],
         [36.1660],
         [36.5974],
         [36.7191],
         [36.6239],
         [36.4454],
         [36.1864],
         [35.9725],
         [35.8371],
         [35.7607],
         [35.7207],
         [35.7189]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.6683],
         [35.4962],
         [35.3654],
         [35.3215],
         [35.5420],
         [35.9232],
         [36.5912],
         [37.4485],
         [37.9970],
         [38.1882],
         [38.2828],
         [38.3554],
         [38.3628],
         [39.0322],
         [40.4294],
         [41.6650],
         [42.4182],
         [42.7197],
         [42.7600],
         [42.7500],
         [42.7483],
         [42.7208],
         [42.6188],
         [42.5368],
         [42.5836],
         [42.5566],
         [42.4086]]], grad_fn=<ThAddBackward>)
[ 38.05760574  37.17518234  37.12746429  37.39609146  37.66733551
  37.99035645  38.44460297  38.60746765  38.32449341  37.92208862
  37.61376953  37.3659935   37.08426285  36.75417709  36.53860092
  36.55721283  36.69962311  36.83666611  36.83580399  36.59072876
  36.2118988   35.8875885   35.74858093  35.71285629  35.87554932
  36.31725693  36.80024338  37.11810303  37.29685593  37.44591522
  37.56037903  37.65253448  37.82679749  37.83334732  37.76077652
  37.62406921  37.6068573   37.67192078  37.65927887  37.56129837
  37.6176033   38.21366882  39.2721405   40.39022827  41.18247223
  41.5437851   41.55003357  41.02574158  40.26515961  39.70530701
  39.26140594  38.65263367  37.87495422  36.93849945  35.99781036
  35.2816925   34.77663422  34.45681763  34.30446625  34.2772789
  34.30760956  34.42588806  34.72121048  35.13808441  35.79645157
  36.21735382  36.39818954  36.35131454  36.18747711  36.05512619
  36.04982376  36.05639267  35.81967545  35.47212982  35.47233963
  35.92939377  36.4467392   36.79319763  36.93323517  36.91334152
  36.81084442  36.75736237  36.9266777   37.41307068  38.21717453
  39.07673645  39.85839462  40.51062012  40.75275803  40.49669266
  40.00184631  39.48374557  39.05127716  38.70031357  38.42432785
  38.28355408  38.51846695  38.36889267  38.16964722  38.18563843
  38.28601456  38.18452835  37.91561127  37.62213898  37.37995148
  37.22213364  37.18538666  37.40752792  37.9486618   38.5666008
  38.41927719  37.51345825  36.58325195  35.96083832  35.59111404
  35.27113342  34.96794891  34.70094681  34.40603256  34.0627594
  33.7093811   33.36408615  33.09561539  32.85733795  32.58639526
  32.35541534  32.27085876  32.35379791  32.71238708  33.01409149
  33.32025909  33.55639648  33.62620926  33.55986023  33.3720932
  33.1138916   32.81975555  32.52898026  32.34332657  32.24407578
  32.23506546  32.33787155  32.64506912  33.1302681   33.65064621
  34.09463882  34.50963593  35.05132294  35.64331436  36.16601562
  36.597435    36.71906662  36.62385941  36.44542313  36.18641281
  35.97253418  35.83711243  35.76066589  35.72071075  35.71887207
  35.6683197   35.4961586   35.36539459  35.32150269  35.54201889
  35.92323685  36.59121704  37.44849396  37.99700928  38.18821716
  38.28281021  38.3553772   38.36279297  39.03216553  40.42938995
  41.66497803  42.41816711  42.7196579   42.76004791  42.75000381
  42.74832535  42.72075272  42.6187973   42.53680038  42.58361435
  42.55661011  42.4086113 ]
SimpleRnn: epoch num: 20
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[41.0697],
         [40.2501],
         [40.2626],
         [40.5809],
         [40.8961],
         [41.2614],
         [41.7576],
         [41.9473],
         [41.6662],
         [41.2510],
         [40.9252],
         [40.6586],
         [40.3552],
         [40.0002],
         [39.7642],
         [39.7740],
         [39.9169],
         [40.0590],
         [40.0591],
         [39.8030],
         [39.4033],
         [39.0561],
         [38.9008],
         [38.8546],
         [39.0192],
         [39.4812],
         [39.9933],
         [40.3371],
         [40.5369],
         [40.7035],
         [40.8319],
         [40.9355]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.9778],
         [40.9103],
         [40.7834],
         [40.6055],
         [40.5635],
         [40.6150],
         [40.5921],
         [40.4838],
         [40.5383],
         [41.1572],
         [42.2666],
         [43.4505],
         [44.3021],
         [44.7056],
         [44.7336],
         [44.1992],
         [43.4083],
         [42.8156],
         [42.3387],
         [41.6885],
         [40.8609],
         [39.8632],
         [38.8576],
         [38.0830],
         [37.5294],
         [37.1725],
         [36.9953],
         [36.9534],
         [36.9764],
         [37.0952],
         [37.4022],
         [37.8399]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[38.1009],
         [38.3020],
         [38.3272],
         [38.1744],
         [37.9383],
         [37.7586],
         [37.7248],
         [37.7130],
         [37.4563],
         [37.0902],
         [37.0850],
         [37.5522],
         [38.0834],
         [38.4421],
         [38.5891],
         [38.5716],
         [38.4678],
         [38.4141],
         [38.5895],
         [39.0925],
         [39.9246],
         [40.8156],
         [41.6292],
         [42.3103],
         [42.5679],
         [42.3108],
         [41.8049],
         [41.2713],
         [40.8232],
         [40.4574],
         [40.1685],
         [40.0193]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.5229],
         [38.9566],
         [38.4691],
         [38.3025],
         [38.2855],
         [38.1053],
         [37.7854],
         [37.4612],
         [37.2016],
         [37.0346],
         [36.9939],
         [37.2179],
         [37.7640],
         [38.3806],
         [38.2159],
         [37.2944],
         [36.3610],
         [35.7450],
         [35.3828],
         [35.0674],
         [34.7672],
         [34.5030],
         [34.2094],
         [33.8672],
         [33.5153],
         [33.1720],
         [32.9069],
         [32.6710],
         [32.4013],
         [32.1727],
         [32.0923],
         [32.1797]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.0013],
         [31.9969],
         [32.0864],
         [32.1772],
         [32.1509],
         [32.0245],
         [31.8016],
         [31.5267],
         [31.2278],
         [30.9404],
         [30.7626],
         [30.6707],
         [30.6673],
         [30.7728],
         [31.0814],
         [31.5578],
         [32.0608],
         [32.4837],
         [32.8776],
         [33.4004],
         [33.9680],
         [34.4633],
         [34.8689],
         [34.9660],
         [34.8570],
         [34.6738],
         [34.4170],
         [34.2106],
         [34.0839],
         [34.0146],
         [33.9798],
         [33.9815]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.6998],
         [33.4040],
         [33.1935],
         [33.1012],
         [33.2936],
         [33.6446],
         [34.2883],
         [35.1027],
         [35.5967],
         [35.7495],
         [35.8186],
         [35.8746],
         [35.8717],
         [36.5609],
         [37.9254],
         [39.0857],
         [39.7640],
         [40.0064],
         [40.0100],
         [39.9808],
         [39.9693],
         [39.9361],
         [39.8320],
         [39.7523],
         [39.8019],
         [39.7740],
         [39.6277]]], grad_fn=<ThAddBackward>)
[ 41.06971359  40.25007248  40.26263428  40.58085251  40.89612961
  41.26141739  41.75760269  41.9472847   41.66624451  41.25097656
  40.92520905  40.65858078  40.35523224  40.00024033  39.7641983
  39.77396011  39.91693497  40.05895233  40.05907822  39.80298233
  39.40327835  39.05606461  38.90076828  38.85459518  39.01922989
  39.48117828  39.99334335  40.3371315   40.53689957  40.70349503
  40.83187103  40.93545151  40.97781372  40.91033554  40.78339386
  40.60553741  40.56354904  40.61499023  40.59210968  40.48377228
  40.53830719  41.15724182  42.26657486  43.45053864  44.30212402
  44.70560455  44.73364258  44.19922256  43.40827179  42.81564331
  42.33869171  41.68849182  40.86090088  39.86317062  38.85757065
  38.083004    37.52944946  37.17254639  36.99528122  36.95341492
  36.97644043  37.09521103  37.40222168  37.83989334  38.10086441
  38.30199814  38.32723618  38.1743927   37.93828201  37.75859451
  37.72484589  37.7129631   37.45627975  37.09015274  37.08499527
  37.55217361  38.08341599  38.44210434  38.58913803  38.57163239
  38.4678421   38.41413879  38.58949661  39.09247971  39.92455673
  40.8156395   41.62915421  42.31033707  42.56791306  42.31077194
  41.80493546  41.27128983  40.82320023  40.45737076  40.16849899
  40.01926804  39.52293396  38.95660782  38.46913147  38.30252075
  38.28547668  38.10528564  37.78544617  37.46120453  37.20160294
  37.03458405  36.99390793  37.21792603  37.76396561  38.38064575
  38.21588135  37.29442215  36.36104584  35.74502563  35.38277054
  35.06744766  34.76721191  34.50303268  34.20943069  33.86724091
  33.51531601  33.17202759  32.90685272  32.67099762  32.40134048
  32.1727066   32.09231567  32.17973328  32.00128937  31.9969368
  32.08640289  32.17715073  32.15085983  32.02447891  31.80160522
  31.5267067   31.22783089  30.94041443  30.76256752  30.67066574
  30.66734123  30.77279472  31.08143044  31.55784607  32.06082916
  32.48374557  32.87763596  33.40042114  33.96803665  34.46329117
  34.86893082  34.96603775  34.85698318  34.67382431  34.41703415
  34.21063614  34.08388138  34.01455307  33.97984695  33.98146439
  33.69978333  33.40395737  33.19354248  33.10118866  33.29359818
  33.64458466  34.28829193  35.10271454  35.59671402  35.74948502
  35.81863403  35.8746109   35.87172699  36.56089783  37.92543793
  39.08573151  39.76403809  40.00635529  40.00996399  39.98078537
  39.96926117  39.93610382  39.83203888  39.75228882  39.80187988
  39.7740097   39.62767792]
SimpleRnn: epoch num: 21
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[38.0687],
         [37.1815],
         [37.1560],
         [37.4395],
         [37.7134],
         [38.0396],
         [38.4983],
         [38.6502],
         [38.3483],
         [37.9364],
         [37.6277],
         [37.3811],
         [37.0981],
         [36.7657],
         [36.5541],
         [36.5824],
         [36.7320],
         [36.8713],
         [36.8659],
         [36.6104],
         [36.2239],
         [35.8993],
         [35.7669],
         [35.7362],
         [35.9092],
         [36.3641],
         [36.8524],
         [37.1661],
         [37.3396],
         [37.4859],
         [37.5981],
         [37.6889]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[37.8309],
         [37.8250],
         [37.7437],
         [37.6003],
         [37.5848],
         [37.6514],
         [37.6363],
         [37.5344],
         [37.5959],
         [38.2134],
         [39.2927],
         [40.4180],
         [41.2011],
         [41.5450],
         [41.5333],
         [40.9836],
         [40.2090],
         [39.6509],
         [39.2102],
         [38.5958],
         [37.8118],
         [36.8671],
         [35.9253],
         [35.2166],
         [34.7215],
         [34.4114],
         [34.2686],
         [34.2489],
         [34.2841],
         [34.4077],
         [34.7113],
         [35.1347]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[35.7693],
         [36.1793],
         [36.3468],
         [36.2871],
         [36.1147],
         [35.9803],
         [35.9778],
         [35.9846],
         [35.7394],
         [35.3873],
         [35.3981],
         [35.8722],
         [36.3950],
         [36.7383],
         [36.8701],
         [36.8429],
         [36.7352],
         [36.6823],
         [36.8587],
         [37.3573],
         [38.1746],
         [39.0385],
         [39.8187],
         [40.4652],
         [40.6907],
         [40.4136],
         [39.9051],
         [39.3818],
         [38.9505],
         [38.6023],
         [38.3299],
         [38.1950]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.4061],
         [38.2438],
         [38.0389],
         [38.0588],
         [38.1609],
         [38.0534],
         [37.7783],
         [37.4824],
         [37.2414],
         [37.0868],
         [37.0551],
         [37.2878],
         [37.8423],
         [38.4640],
         [38.2913],
         [37.3593],
         [36.4216],
         [35.8069],
         [35.4468],
         [35.1323],
         [34.8321],
         [34.5681],
         [34.2738],
         [33.9306],
         [33.5778],
         [33.2339],
         [32.9692],
         [32.7334],
         [32.4630],
         [32.2345],
         [32.1559],
         [32.2458]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.5908],
         [32.8890],
         [33.1925],
         [33.4238],
         [33.4863],
         [33.4135],
         [33.2198],
         [32.9581],
         [32.6620],
         [32.3715],
         [32.1895],
         [32.0940],
         [32.0892],
         [32.1966],
         [32.5137],
         [33.0050],
         [33.5264],
         [33.9677],
         [34.3801],
         [34.9246],
         [35.5157],
         [36.0338],
         [36.4602],
         [36.5690],
         [36.4643],
         [36.2795],
         [36.0164],
         [35.8028],
         [35.6698],
         [35.5957],
         [35.5578],
         [35.5578]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.4883],
         [35.3091],
         [35.1764],
         [35.1334],
         [35.3644],
         [35.7495],
         [36.4334],
         [37.2938],
         [37.8273],
         [38.0065],
         [38.0931],
         [38.1602],
         [38.1635],
         [38.8783],
         [40.2956],
         [41.5168],
         [42.2479],
         [42.5264],
         [42.5501],
         [42.5314],
         [42.5255],
         [42.4945],
         [42.3887],
         [42.3062],
         [42.3559],
         [42.3268],
         [42.1758]]], grad_fn=<ThAddBackward>)
[ 38.06866074  37.18152618  37.15597153  37.43946457  37.71341324
  38.03956604  38.49827576  38.6501503   38.34832764  37.9364357
  37.62767792  37.38110733  37.0981369   36.76573181  36.55413437
  36.58241272  36.7319603   36.87131119  36.8658905   36.61037827
  36.2239418   35.89928055  35.76693344  35.73623276  35.90924454
  36.36405182  36.85237122  37.1660881   37.33955002  37.48586655
  37.59807205  37.68885803  37.8308754   37.82495117  37.74372101
  37.60027313  37.58477783  37.65142059  37.63625717  37.53439331
  37.59589005  38.21336365  39.29265976  40.41799164  41.20108032
  41.54502106  41.53326416  40.98363495  40.20896912  39.65093994
  39.21019745  38.59580231  37.81177139  36.86705017  35.92526245
  35.21661377  34.72145081  34.41140747  34.26862335  34.248909
  34.28409958  34.40766907  34.71133804  35.1346817   35.7693367
  36.17925262  36.34676361  36.28710175  36.11466217  35.98031235
  35.9777832   35.98464966  35.73942947  35.38725281  35.39806747
  35.87216187  36.39502335  36.73825073  36.87012863  36.84292984
  36.73517227  36.68228149  36.85866165  37.35733795  38.17459106
  39.03847504  39.81867218  40.46519852  40.69071198  40.41362     39.90514374
  39.38175964  38.9504509   38.60225677  38.3299408   38.19498825
  38.40605545  38.24377823  38.03892517  38.05877304  38.160923
  38.05339432  37.77828217  37.48243332  37.24137497  37.08678818
  37.0551033   37.28775406  37.84233856  38.46399307  38.29133224
  37.35926437  36.42155838  35.80686188  35.44683456  35.1323204
  34.83211136  34.56814957  34.27379608  33.93059158  33.57779312
  33.23394394  32.96920013  32.73337936  32.4630394   32.23453903
  32.15586853  32.24584579  32.59078979  32.88902664  33.19248581
  33.42380142  33.4862709   33.41345596  33.21978378  32.95812607
  32.66200638  32.37147141  32.18946457  32.09395218  32.08919907
  32.19656372  32.51371002  33.00495148  33.52638626  33.96767044
  34.38008118  34.92458344  35.51572037  36.0337944   36.4601593
  36.56903839  36.4642601   36.27946854  36.01644135  35.80276871
  35.66979599  35.59569168  35.55780411  35.55781174  35.48826218
  35.30906677  35.17638779  35.1333847   35.36444473  35.74954605
  36.43342209  37.29382706  37.82726288  38.00651169  38.0931282
  38.16020203  38.16351318  38.87829208  40.29556656  41.51675034
  42.24791336  42.52637863  42.55010605  42.53144073  42.52549362
  42.49449539  42.3887291   42.30618668  42.35593033  42.32678604
  42.17575073]
SimpleRnn: epoch num: 22
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.7998],
         [39.9865],
         [40.0217],
         [40.3555],
         [40.6737],
         [41.0400],
         [41.5389],
         [41.7193],
         [41.4240],
         [41.0023],
         [40.6771],
         [40.4124],
         [40.1095],
         [39.7544],
         [39.5229],
         [39.5408],
         [39.6895],
         [39.8332],
         [39.8301],
         [39.5670],
         [39.1629],
         [38.8169],
         [38.6677],
         [38.6261],
         [38.7994],
         [39.2701],
         [39.7842],
         [40.1235],
         [40.3178],
         [40.4809],
         [40.6064],
         [40.7078]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.7378],
         [40.6681],
         [40.5384],
         [40.3590],
         [40.3206],
         [40.3746],
         [40.3509],
         [40.2405],
         [40.2993],
         [40.9347],
         [42.0574],
         [43.2421],
         [44.0829],
         [44.4703],
         [44.4824],
         [43.9284],
         [43.1276],
         [42.5375],
         [42.0646],
         [41.4127],
         [40.5835],
         [39.5836],
         [38.5816],
         [37.8168],
         [37.2744],
         [36.9281],
         [36.7605],
         [36.7261],
         [36.7541],
         [36.8774],
         [37.1905],
         [37.6321]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.8858],
         [38.0852],
         [38.1048],
         [37.9466],
         [37.7073],
         [37.5286],
         [37.4985],
         [37.4881],
         [37.2264],
         [36.8582],
         [36.8616],
         [37.3413],
         [37.8759],
         [38.2312],
         [38.3714],
         [38.3479],
         [38.2399],
         [38.1864],
         [38.3666],
         [38.8777],
         [39.7176],
         [40.6094],
         [41.4191],
         [42.0932],
         [42.3362],
         [42.0623],
         [41.5461],
         [41.0091],
         [40.5629],
         [40.2003],
         [39.9154],
         [39.7716]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.2715],
         [38.7060],
         [38.2228],
         [38.0663],
         [38.0559],
         [37.8752],
         [37.5539],
         [37.2304],
         [36.9736],
         [36.8105],
         [36.7745],
         [37.0062],
         [37.5608],
         [38.1799],
         [37.9967],
         [37.0565],
         [36.1192],
         [35.5107],
         [35.1572],
         [34.8473],
         [34.5506],
         [34.2898],
         [33.9977],
         [33.6567],
         [33.3063],
         [32.9652],
         [32.7038],
         [32.4707],
         [32.2024],
         [31.9764],
         [31.9010],
         [31.9939]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.8162],
         [31.8166],
         [31.9096],
         [32.0016],
         [31.9734],
         [31.8449],
         [31.6197],
         [31.3439],
         [31.0449],
         [30.7587],
         [30.5844],
         [30.4959],
         [30.4961],
         [30.6051],
         [30.9191],
         [31.3994],
         [31.9031],
         [32.3231],
         [32.7145],
         [33.2377],
         [33.8041],
         [34.2956],
         [34.6965],
         [34.7838],
         [34.6676],
         [34.4799],
         [34.2205],
         [34.0146],
         [33.8900],
         [33.8227],
         [33.7897],
         [33.7928]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.5062],
         [33.2109],
         [33.0031],
         [32.9146],
         [33.1146],
         [33.4701],
         [34.1212],
         [34.9383],
         [35.4251],
         [35.5678],
         [35.6318],
         [35.6844],
         [35.6790],
         [36.3818],
         [37.7592],
         [38.9156],
         [39.5808],
         [39.8083],
         [39.8011],
         [39.7670],
         [39.7534],
         [39.7185],
         [39.6122],
         [39.5325],
         [39.5845],
         [39.5553],
         [39.4069]]], grad_fn=<ThAddBackward>)
[ 40.79984665  39.98646927  40.02167511  40.35548019  40.67367172
  41.03999329  41.53888321  41.71925735  41.42402267  41.00225449
  40.67705154  40.41237259  40.10945511  39.75444412  39.52293015
  39.54082108  39.68946838  39.83322525  39.83010483  39.56700516
  39.16293716  38.81690216  38.66770554  38.62612915  38.79944229
  39.2701149   39.78423691  40.1235199   40.31783295  40.4809494
  40.60638809  40.70782852  40.73782349  40.6680603   40.538414
  40.35900879  40.32055283  40.3745575   40.3508606   40.24053955
  40.29934692  40.93471146  42.05741119  43.2420845   44.08293915
  44.47029114  44.48241425  43.92835999  43.12757111  42.53747559
  42.06458282  41.41265106  40.58351135  39.58359909  38.58158875
  37.81678009  37.27443695  36.92808151  36.76052094  36.72611237
  36.75405121  36.87741089  37.19049072  37.63212204  37.88581085
  38.08523941  38.10484314  37.9466095   37.70731354  37.52855682
  37.49853134  37.48807526  37.22637177  36.85816193  36.86164474
  37.34130478  37.87588882  38.23123932  38.3713913   38.34790802
  38.23991394  38.18644714  38.36659241  38.87765503  39.71757889
  40.60943222  41.41912079  42.09321213  42.33622742  42.06230164
  41.54614258  41.00912094  40.56285858  40.20027542  39.91540527
  39.77159119  39.27153397  38.70603943  38.22275543  38.06628418
  38.05589676  37.87522125  37.55391312  37.23041534  36.97360611
  36.81045532  36.77454376  37.00623322  37.56083679  38.17985916
  37.99665833  37.05646133  36.11921692  35.5107193   35.15721512
  34.84734344  34.55059814  34.28983688  33.99774933  33.65670395
  33.30630493  32.96518707  32.70376968  32.47066116  32.20236206
  31.9764061   31.90099525  31.99392319  31.81621552  31.81662369
  31.90960312  32.00156403  31.97340965  31.84487915  31.6196785   31.343853
  31.04485512  30.75873184  30.58443642  30.49590874  30.49610138
  30.60505676  30.91906738  31.39941978  31.90307236  32.32307434
  32.7145462   33.2376709   33.80409241  34.29564285  34.69654846
  34.78380966  34.66760254  34.4798851   34.22047424  34.01462555
  33.89003754  33.82271576  33.78973389  33.79281616  33.50621414
  33.21094894  33.00309372  32.91462326  33.11464691  33.47006607
  34.12120056  34.93832016  35.42505646  35.56779861  35.63175964
  35.68442154  35.67902756  36.3817749   37.75918198  38.91556931
  39.5807724   39.80826187  39.80112076  39.76700974  39.75337982
  39.71847916  39.61220169  39.53250122  39.58452988  39.55525589
  39.40687943]
SimpleRnn: epoch num: 23
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[37.8245],
         [36.9469],
         [36.9402],
         [37.2346],
         [37.5114],
         [37.8395],
         [38.2998],
         [38.4447],
         [38.1326],
         [37.7163],
         [37.4085],
         [37.1638],
         [36.8816],
         [36.5495],
         [36.3416],
         [36.3762],
         [36.5299],
         [36.6704],
         [36.6625],
         [36.4017],
         [36.0122],
         [35.6888],
         [35.5612],
         [35.5341],
         [35.7130],
         [36.1742],
         [36.6639],
         [36.9737],
         [37.1428],
         [37.2863],
         [37.3962],
         [37.4854]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[37.6195],
         [37.6107],
         [37.5265],
         [37.3811],
         [37.3677],
         [37.4359],
         [37.4197],
         [37.3161],
         [37.3808],
         [38.0089],
         [39.0974],
         [40.2234],
         [40.9973],
         [41.3282],
         [41.3042],
         [40.7397],
         [39.9582],
         [39.4029],
         [38.9659],
         [38.3509],
         [37.5664],
         [36.6208],
         [35.6826],
         [34.9822],
         [34.4963],
         [34.1947],
         [34.0596],
         [34.0456],
         [34.0845],
         [34.2114],
         [34.5195],
         [34.9455]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[35.5736],
         [35.9787],
         [36.1386],
         [36.0717],
         [35.8946],
         [35.7594],
         [35.7588],
         [35.7661],
         [35.5165],
         [35.1625],
         [35.1801],
         [35.6638],
         [36.1889],
         [36.5291],
         [36.6551],
         [36.6229],
         [36.5117],
         [36.4590],
         [36.6391],
         [37.1439],
         [37.9669],
         [38.8307],
         [39.6071],
         [40.2472],
         [40.4605],
         [40.1699],
         [39.6533],
         [39.1275],
         [38.6981],
         [38.3530],
         [38.0843],
         [37.9540]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.1593],
         [37.9927],
         [37.7862],
         [37.8096],
         [37.9136],
         [37.8032],
         [37.5251],
         [37.2287],
         [36.9892],
         [36.8375],
         [36.8095],
         [37.0482],
         [37.6094],
         [38.2325],
         [38.0446],
         [37.0971],
         [36.1570],
         [35.5487],
         [35.1966],
         [34.8874],
         [34.5908],
         [34.3303],
         [34.0378],
         [33.6961],
         [33.3451],
         [33.0036],
         [32.7425],
         [32.5094],
         [32.2407],
         [32.0149],
         [31.9406],
         [32.0354]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.3791],
         [32.6776],
         [32.9801],
         [33.2087],
         [33.2666],
         [33.1898],
         [32.9926],
         [32.7292],
         [32.4325],
         [32.1429],
         [31.9639],
         [31.8715],
         [31.8699],
         [31.9804],
         [32.3020],
         [32.7963],
         [33.3178],
         [33.7561],
         [34.1656],
         [34.7094],
         [35.2986],
         [35.8125],
         [36.2338],
         [36.3335],
         [36.2220],
         [36.0328],
         [35.7673],
         [35.5541],
         [35.4231],
         [35.3509],
         [35.3147],
         [35.3161]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.2427],
         [35.0613],
         [34.9286],
         [34.8872],
         [35.1234],
         [35.5113],
         [36.2005],
         [37.0621],
         [37.5882],
         [37.7575],
         [37.8383],
         [37.9015],
         [37.9019],
         [38.6273],
         [40.0551],
         [41.2716],
         [41.9898],
         [42.2536],
         [42.2663],
         [42.2419],
         [42.2331],
         [42.2001],
         [42.0921],
         [42.0094],
         [42.0611],
         [42.0308],
         [41.8781]]], grad_fn=<ThAddBackward>)
[ 37.82447052  36.94693756  36.94015121  37.23457336  37.5113945
  37.83947754  38.29978943  38.44473267  38.13256073  37.71627045
  37.4084549   37.16381836  36.88157654  36.54953003  36.34164429
  36.37617111  36.52993774  36.6704483   36.66246033  36.40171814
  36.01222229  35.68876266  35.56124878  35.53412628  35.7130127
  36.17417908  36.66389084  36.97366333  37.14276123  37.28631592
  37.39621353  37.48535156  37.61953354  37.61068344  37.5265274
  37.38108444  37.36771393  37.43585968  37.41973495  37.31613922
  37.38075638  38.00886536  39.09740829  40.22340393  40.9973259
  41.32824326  41.30415726  40.73973465  39.95817947  39.40285873
  38.96593094  38.35087585  37.5663681   36.62075424  35.68258286
  34.98220444  34.49626541  34.19472885  34.05960083  34.04563522
  34.08450699  34.2114296   34.51948166  34.94547653  35.5735817
  35.97874832  36.13855362  36.07168961  35.89455795  35.75944901
  35.75882339  35.76609802  35.51652145  35.16254807  35.18006897
  35.66376495  36.18890381  36.52907181  36.65513229  36.62294388
  36.51173401  36.45900345  36.63910675  37.14393616  37.9669075
  38.83071518  39.60707474  40.24715805  40.46051407  40.16986084
  39.65326309  39.1275177   38.69811249  38.35298538  38.08428574
  37.95397568  38.15929413  37.99271393  37.78622818  37.80964661
  37.91363144  37.80316544  37.52510071  37.22869873  36.98924255
  36.83746719  36.80950165  37.04821777  37.60943604  38.2325325
  38.0446167   37.09708786  36.15698242  35.54871368  35.19660568
  34.88739777  34.59081268  34.33028412  34.03777313  33.69608307
  33.34508896  33.00358582  32.74245453  32.50941086  32.24071121
  32.01485825  31.94062233  32.0353508   32.37908554  32.67760086
  32.98008347  33.20868683  33.26662064  33.18976974  32.9926033
  32.72918701  32.43247604  32.14288712  31.96393585  31.87147522
  31.86989403  31.9803524   32.30195618  32.79627609  33.31784058
  33.756073    34.1656456   34.70944977  35.29857254  35.81249237
  36.23380661  36.33353806  36.22195816  36.03279495  35.76731491
  35.55406952  35.42305374  35.35087967  35.31467819  35.31612396
  35.24266815  35.06134796  34.92862701  34.8871727   35.12340546
  35.51133728  36.20052338  37.06214523  37.58815002  37.75746155
  37.8382988   37.90148544  37.90189362  38.6272583   40.05512238
  41.27159119  41.98984146  42.25363159  42.26625061  42.24186707
  42.23312378  42.20005035  42.09207535  42.00939178  42.06112671
  42.03083801  41.87810135]
SimpleRnn: epoch num: 24
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.4817],
         [39.6746],
         [39.7250],
         [40.0676],
         [40.3877],
         [40.7549],
         [41.2542],
         [41.4276],
         [41.1223],
         [40.6960],
         [40.3713],
         [40.1085],
         [39.8067],
         [39.4526],
         [39.2250],
         [39.2490],
         [39.4019],
         [39.5471],
         [39.5419],
         [39.2743],
         [38.8676],
         [38.5229],
         [38.3784],
         [38.3406],
         [38.5195],
         [38.9960],
         [39.5113],
         [39.8468],
         [40.0366],
         [40.1965],
         [40.3192],
         [40.4186]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.4436],
         [40.3708],
         [40.2386],
         [40.0577],
         [40.0215],
         [40.0773],
         [40.0531],
         [39.9415],
         [40.0032],
         [40.6480],
         [41.7787],
         [42.9633],
         [43.7947],
         [44.1686],
         [44.1679],
         [43.5993],
         [42.7915],
         [42.2037],
         [41.7348],
         [41.0834],
         [40.2550],
         [39.2557],
         [38.2585],
         [37.5029],
         [36.9709],
         [36.6341],
         [36.4750],
         [36.4472],
         [36.4794],
         [36.6064],
         [36.9237],
         [37.3678]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.6198],
         [37.8168],
         [37.8315],
         [37.6687],
         [37.4268],
         [37.2486],
         [37.2214],
         [37.2122],
         [36.9473],
         [36.5780],
         [36.5879],
         [37.0767],
         [37.6136],
         [37.9661],
         [38.1006],
         [38.0720],
         [37.9605],
         [37.9069],
         [38.0903],
         [38.6068],
         [39.4516],
         [40.3428],
         [41.1481],
         [41.8151],
         [42.0459],
         [41.7585],
         [41.2341],
         [40.6946],
         [40.2502],
         [39.8909],
         [39.6100],
         [39.4710]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.9733],
         [38.4081],
         [37.9284],
         [37.7799],
         [37.7751],
         [37.5946],
         [37.2727],
         [36.9502],
         [36.6961],
         [36.5365],
         [36.5047],
         [36.7425],
         [37.3034],
         [37.9237],
         [37.7264],
         [36.7715],
         [35.8334],
         [35.2308],
         [34.8852],
         [34.5808],
         [34.2878],
         [34.0305],
         [33.7405],
         [33.4011],
         [33.0527],
         [32.7141],
         [32.4563],
         [32.2261],
         [31.9597],
         [31.7364],
         [31.6654],
         [31.7629]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.5895],
         [31.5932],
         [31.6889],
         [31.7816],
         [31.7518],
         [31.6215],
         [31.3945],
         [31.1181],
         [30.8194],
         [30.5349],
         [30.3639],
         [30.2785],
         [30.2819],
         [30.3939],
         [30.7120],
         [31.1952],
         [31.6987],
         [32.1157],
         [32.5043],
         [33.0265],
         [33.5909],
         [34.0784],
         [34.4744],
         [34.5530],
         [34.4304],
         [34.2387],
         [33.9771],
         [33.7719],
         [33.6493],
         [33.5840],
         [33.5526],
         [33.5571]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.2696],
         [32.9741],
         [32.7683],
         [32.6830],
         [32.8893],
         [33.2482],
         [33.9049],
         [34.7234],
         [35.2032],
         [35.3367],
         [35.3954],
         [35.4447],
         [35.4368],
         [36.1500],
         [37.5369],
         [38.6888],
         [39.3415],
         [39.5551],
         [39.5377],
         [39.4986],
         [39.4828],
         [39.4462],
         [39.3381],
         [39.2585],
         [39.3125],
         [39.2822],
         [39.1323]]], grad_fn=<ThAddBackward>)
[ 40.48172379  39.67463684  39.72496414  40.06758118  40.38771439
  40.754879    41.25418472  41.42755508  41.12232971  40.69596863
  40.37129593  40.10850906  39.80670547  39.45262146  39.22499084
  39.24898911  39.40192795  39.54709625  39.54191971  39.27431107
  38.86764145  38.52292252  38.3784256   38.34060287  38.51951981
  38.99602127  39.51134109  39.84679031  40.03659439  40.19649887
  40.31920242  40.41859818  40.44357681  40.37079239  40.23862839
  40.05772781  40.02150726  40.07728958  40.05309677  39.94147873
  40.0031662   40.64797974  41.77871323  42.96330643  43.79468918
  44.16864395  44.16794205  43.59932327  42.79150772  42.20372009
  41.73484039  41.08336639  40.25497055  39.25568771  38.25849533
  37.50294113  36.97085953  36.63405991  36.47504807  36.44715881
  36.47939301  36.60640335  36.92370987  37.36777115  37.6198349
  37.81675339  37.83151245  37.66868973  37.42675781  37.24860382
  37.2213974   37.21223068  36.94731903  36.57797623  36.58792496
  37.07667542  37.61360168  37.96606445  38.10055923  38.07204437
  37.9605217   37.90692139  38.09030151  38.60678864  39.45159149
  40.34281921  41.14806747  41.81510162  42.04586411  41.75849915
  41.23413086  40.69463348  40.25024414  39.89093781  39.60997391
  39.47098923  38.97328186  38.4081459   37.92835999  37.77990341
  37.77509308  37.59463501  37.27269745  36.95022964  36.69609833
  36.53649521  36.50471497  36.74245453  37.30340958  37.92374802
  37.72637558  36.7715416   35.83339691  35.23084641  34.88521576
  34.58078003  34.28778839  34.03052902  33.74045563  33.40110016
  33.0526886   32.71406555  32.45632935  32.22613525  31.95966911
  31.73644257  31.66537666  31.76288986  31.58951759  31.59324646
  31.68888283  31.78158379  31.75180054  31.62147141  31.39448166
  31.11813164  30.81943893  30.53485298  30.3638916   30.27854919
  30.28190994  30.39385033  30.7120266   31.19516373  31.69874573
  32.11566544  32.50425339  33.02654648  33.59091568  34.07840347
  34.47441483  34.55298996  34.4303894   34.23868179  33.97714615
  33.77190781  33.64934158  33.5839653   33.55262756  33.5570755
  33.26963425  32.97413254  32.76829147  32.68302536  32.88926315
  33.24822998  33.90489578  34.72340775  35.20318985  35.33666229
  35.39538574  35.44466019  35.43680191  36.14995575  37.53693008
  38.68878555  39.3415451   39.55511856  39.53772354  39.49860764
  39.48275757  39.44621658  39.33810425  39.258461    39.31251144
  39.2822113   39.132267  ]
SimpleRnn: epoch num: 25
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[37.5340],
         [36.6686],
         [36.6766],
         [36.9802],
         [37.2589],
         [37.5877],
         [38.0484],
         [38.1867],
         [37.8651],
         [37.4449],
         [37.1380],
         [36.8954],
         [36.6143],
         [36.2832],
         [36.0790],
         [36.1192],
         [36.2769],
         [36.4185],
         [36.4083],
         [36.1432],
         [35.7512],
         [35.4291],
         [35.3062],
         [35.2826],
         [35.4667],
         [35.9332],
         [36.4238],
         [36.7297],
         [36.8944],
         [37.0350],
         [37.1424],
         [37.2298]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[37.3591],
         [37.3460],
         [37.2585],
         [37.1107],
         [37.0990],
         [37.1684],
         [37.1514],
         [37.0462],
         [37.1135],
         [37.7508],
         [38.8469],
         [39.9724],
         [40.7368],
         [41.0547],
         [41.0184],
         [40.4403],
         [39.6527],
         [39.1003],
         [38.6676],
         [38.0531],
         [37.2693],
         [36.3242],
         [35.3906],
         [34.6991],
         [34.2229],
         [33.9302],
         [33.8029],
         [33.7947],
         [33.8372],
         [33.9673],
         [34.2791],
         [34.7072]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[35.3308],
         [35.7297],
         [35.8811],
         [35.8067],
         [35.6247],
         [35.4887],
         [35.4898],
         [35.4975],
         [35.2441],
         [34.8886],
         [34.9125],
         [35.4051],
         [35.9322],
         [36.2692],
         [36.3894],
         [36.3521],
         [36.2374],
         [36.1847],
         [36.3682],
         [36.8785],
         [37.7062],
         [38.5692],
         [39.3408],
         [39.9737],
         [40.1748],
         [39.8708],
         [39.3464],
         [38.8186],
         [38.3915],
         [38.0499],
         [37.7853],
         [37.6598]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[37.8619],
         [37.6901],
         [37.4815],
         [37.5081],
         [37.6137],
         [37.5005],
         [37.2196],
         [36.9228],
         [36.6851],
         [36.5364],
         [36.5123],
         [36.7569],
         [37.3244],
         [37.9485],
         [37.7457],
         [36.7834],
         [35.8443],
         [35.2422],
         [34.8981],
         [34.5943],
         [34.3015],
         [34.0445],
         [33.7540],
         [33.4141],
         [33.0653],
         [32.7265],
         [32.4693],
         [32.2394],
         [31.9727],
         [31.7498],
         [31.6802],
         [31.7797]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.1237],
         [32.4215],
         [32.7224],
         [32.9477],
         [33.0008],
         [32.9196],
         [32.7189],
         [32.4538],
         [32.1567],
         [31.8684],
         [31.6928],
         [31.6037],
         [31.6055],
         [31.7192],
         [32.0451],
         [32.5422],
         [33.0635],
         [33.4982],
         [33.9043],
         [34.4467],
         [35.0331],
         [35.5422],
         [35.9578],
         [36.0479],
         [35.9291],
         [35.7355],
         [35.4676],
         [35.2550],
         [35.1262],
         [35.0562],
         [35.0219],
         [35.0249]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.2352],
         [35.2128],
         [35.1909],
         [35.2217],
         [35.5097],
         [35.9332],
         [36.6536],
         [37.5397],
         [38.0777],
         [38.2505],
         [38.3338],
         [38.3984],
         [38.3990],
         [39.1364],
         [40.5907],
         [41.8241],
         [42.5508],
         [42.8145],
         [42.8241],
         [42.7981],
         [42.7887],
         [42.7546],
         [42.6444],
         [42.5607],
         [42.6141],
         [42.5826],
         [42.4269]]], grad_fn=<ThAddBackward>)
[ 37.53398895  36.6685524   36.67657852  36.98023987  37.25886536
  37.58769989  38.04841614  38.1866684   37.86514664  37.44485474
  37.13795471  36.89538193  36.61431503  36.28317261  36.07897186
  36.11924744  36.27690506  36.41851807  36.40834045  36.14318085
  35.75123978  35.42913818  35.30618286  35.28256989  35.46667862
  35.93319321  36.42376709  36.7296524   36.89439011  37.03496933
  37.14242172  37.2297821   37.3590889   37.34602737  37.25845718
  37.11074448  37.09899139  37.16838455  37.15136337  37.04623413
  37.11352921  37.75077057  38.84689331  39.97242355  40.73680878
  41.05468369  41.01844406  40.44033432  39.65266037  39.10026932
  38.66759872  38.05305481  37.26926041  36.32417297  35.39062881
  34.69913864  34.22288132  33.93019485  33.80285263  33.79467392
  33.83721542  33.96727371  34.27910995  34.70717621  35.33078003
  35.72969055  35.88111496  35.80666351  35.62468719  35.48866272
  35.48976898  35.49750519  35.24407959  34.88863754  34.91249466
  35.40510559  35.93224335  36.26919556  36.38935471  36.35206985
  36.2374115   36.18470764  36.36818695  36.87848282  37.70624161
  38.56916428  39.34084702  39.97372437  40.1747551   39.87080765
  39.34638977  38.81863022  38.39150238  38.0499115   37.78525925
  37.65979385  37.86185837  37.69009018  37.48154831  37.50809097
  37.61374283  37.50048065  37.21962357  36.9228096   36.68513489
  36.53640366  36.51229477  36.75694275  37.32440186  37.94850922
  37.74568558  36.78338623  35.84434891  35.24218369  34.89807129
  34.59430313  34.30147171  34.044487    33.75404358  33.41414261
  33.0653038   32.72650146  32.46930695  32.23940659  31.97273254
  31.74982452  31.68017006  31.77967834  32.12368011  32.4214592
  32.72237015  32.94773865  33.00075912  32.91963577  32.71889877
  32.45380783  32.15673447  31.86841011  31.69282722  31.60371017
  31.60551453  31.71915817  32.04511261  32.54224396  33.06350708
  33.49816895  33.90427399  34.44670868  35.0331459   35.54224777
  35.9578476   36.0479126   35.929142    35.73548508  35.46761322
  35.25502014  35.1262207   35.05622101  35.02188492  35.02487183
  35.23523712  35.21284485  35.19092178  35.22172546  35.50973129
  35.93323135  36.65359497  37.5397377   38.07766724  38.25045776
  38.33381271  38.39836121  38.39903259  39.13638306  40.59069443
  41.82412338  42.55078125  42.81454086  42.8241272   42.79811478
  42.78873062  42.7546463   42.64435577  42.56065369  42.61408234
  42.58255005  42.42694473]
SimpleRnn: epoch num: 26
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[41.3010],
         [40.6417],
         [40.8078],
         [41.2331],
         [41.6092],
         [42.0178],
         [42.5508],
         [42.7415],
         [42.4373],
         [42.0056],
         [41.6751],
         [41.4060],
         [41.0959],
         [40.7317],
         [40.4973],
         [40.5209],
         [40.6768],
         [40.8254],
         [40.8199],
         [40.5450],
         [40.1274],
         [39.7728],
         [39.6234],
         [39.5830],
         [39.7664],
         [40.2554],
         [40.7847],
         [41.1302],
         [41.3270],
         [41.4929],
         [41.6201],
         [41.7233]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[42.0182],
         [42.0945],
         [42.0635],
         [41.9447],
         [41.9504],
         [42.0346],
         [42.0271],
         [41.9228],
         [41.9935],
         [42.6646],
         [43.8392],
         [45.0739],
         [45.9474],
         [46.3489],
         [46.3600],
         [45.7791],
         [44.9462],
         [44.3343],
         [43.8422],
         [43.1608],
         [42.2966],
         [41.2531],
         [40.2102],
         [39.4151],
         [38.8513],
         [38.4911],
         [38.3173],
         [38.2814],
         [38.3103],
         [38.4393],
         [38.7668],
         [39.2270]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[39.7182],
         [40.0554],
         [40.1654],
         [40.0583],
         [39.8451],
         [39.6818],
         [39.6652],
         [39.6625],
         [39.3915],
         [39.0087],
         [39.0159],
         [39.5209],
         [40.0815],
         [40.4563],
         [40.6052],
         [40.5830],
         [40.4710],
         [40.4167],
         [40.6072],
         [41.1457],
         [42.0298],
         [42.9684],
         [43.8230],
         [44.5357],
         [44.7942],
         [44.5101],
         [43.9714],
         [43.4084],
         [42.9390],
         [42.5558],
         [42.2543],
         [42.1010]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[41.7883],
         [41.3103],
         [40.8815],
         [40.7669],
         [40.7856],
         [40.6132],
         [40.2857],
         [39.9499],
         [39.6803],
         [39.5073],
         [39.4681],
         [39.7124],
         [40.2989],
         [40.9551],
         [40.7630],
         [39.7746],
         [38.7853],
         [38.1359],
         [37.7518],
         [37.4156],
         [37.0946],
         [36.8129],
         [36.4982],
         [36.1328],
         [35.7572],
         [35.3915],
         [35.1100],
         [34.8581],
         [34.5698],
         [34.3269],
         [34.2434],
         [34.3383]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[34.3072],
         [34.3957],
         [34.5561],
         [34.6943],
         [34.6915],
         [34.5726],
         [34.3432],
         [34.0547],
         [33.7371],
         [33.4307],
         [33.2413],
         [33.1425],
         [33.1391],
         [33.2524],
         [33.5859],
         [34.0979],
         [34.6385],
         [35.0927],
         [35.5185],
         [36.0840],
         [36.6956],
         [37.2292],
         [37.6672],
         [37.7702],
         [37.6563],
         [37.4608],
         [37.1863],
         [36.9658],
         [36.8302],
         [36.7549],
         [36.7171],
         [36.7183]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[36.2673],
         [35.8712],
         [35.5922],
         [35.4594],
         [35.6467],
         [36.0066],
         [36.6900],
         [37.5522],
         [38.0709],
         [38.2290],
         [38.3039],
         [38.3631],
         [38.3602],
         [39.1094],
         [40.5691],
         [41.8005],
         [42.5190],
         [42.7748],
         [42.7791],
         [42.7507],
         [42.7403],
         [42.7052],
         [42.5936],
         [42.5099],
         [42.5645],
         [42.5321],
         [42.3754]]], grad_fn=<ThAddBackward>)
[ 41.30102158  40.64174271  40.80779266  41.23310089  41.60918045
  42.01783752  42.5508194   42.741539    42.43727112  42.00564194
  41.67510223  41.40597534  41.09589005  40.73167801  40.49728775
  40.52090836  40.67683411  40.82543945  40.81990814  40.54500961
  40.12739944  39.77283478  39.6234169   39.5830307   39.76642609
  40.25540924  40.78470612  41.1301651   41.32700348  41.49285126
  41.62011337  41.72333527  42.01818848  42.09447479  42.06353378
  41.94468689  41.95043182  42.0346489   42.02705383  41.92276764
  41.99349976  42.6646347   43.8391571   45.07393265  45.94744873
  46.34892273  46.3600235   45.77910614  44.9462204   44.33429718
  43.84220123  43.1608429   42.29657364  41.25305939  40.2102356
  39.41507721  38.85126495  38.49105453  38.31725693  38.28142548
  38.31033707  38.43933105  38.76678085  39.22702789  39.7182045
  40.05541611  40.16543961  40.05833054  39.84514999  39.68183517
  39.66524887  39.66253281  39.39146042  39.0086937   39.01585388
  39.52085495  40.08147812  40.45625687  40.60519409  40.58297348
  40.47096634  40.41669464  40.60715866  41.14565659  42.02977753
  42.96842194  43.82304764  44.53571701  44.79420853  44.51013565
  43.97144318  43.4084053   42.93896866  42.55584335  42.25426865
  42.10104752  41.78831482  41.31028366  40.88151169  40.766922
  40.78557205  40.61316299  40.28573227  39.94992828  39.6803093
  39.50730133  39.46807861  39.71235657  40.29887772  40.95505524
  40.76298904  39.77455139  38.78534698  38.13587952  37.75180817
  37.41563416  37.09463501  36.81291199  36.49816132  36.13276291
  35.75724411  35.39149094  35.1099968   34.85813904  34.56977844
  34.32691956  34.24342728  34.33831787  34.30719376  34.39569473
  34.55609131  34.69433975  34.69151688  34.5725975   34.34318924
  34.05472946  33.73708725  33.43069458  33.24132156  33.14247131
  33.1390686   33.25240707  33.5859108   34.09791946  34.6384697
  35.09265518  35.51851654  36.08401871  36.69558716  37.2292366
  37.66717529  37.77019119  37.65626144  37.46084213  37.186306
  36.96577835  36.8301506   36.75491714  36.71710205  36.71831894
  36.26725006  35.87116623  35.59219742  35.45944595  35.6466713
  36.00656509  36.68997955  37.55221939  38.07092667  38.22898483
  38.30388641  38.36309433  38.36021805  39.10942078  40.56908417
  41.80049133  42.51897049  42.77476501  42.77908325  42.75067902
  42.74025726  42.70520401  42.59362411  42.50986481  42.5645256
  42.53211594  42.37535477]
SimpleRnn: epoch num: 27
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.5389],
         [39.5102],
         [39.4203],
         [39.6745],
         [39.9293],
         [40.2549],
         [40.7277],
         [40.8708],
         [40.5376],
         [40.0983],
         [39.7725],
         [39.5118],
         [39.2111],
         [38.8584],
         [38.6380],
         [38.6730],
         [38.8329],
         [38.9796],
         [38.9699],
         [38.6940],
         [38.2836],
         [37.9425],
         [37.8075],
         [37.7764],
         [37.9654],
         [38.4514],
         [38.9671],
         [39.2941],
         [39.4751],
         [39.6290],
         [39.7468],
         [39.8426]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.5111],
         [39.2288],
         [38.9552],
         [38.6876],
         [38.6048],
         [38.6309],
         [38.5848],
         [38.4587],
         [38.5196],
         [39.1739],
         [40.3020],
         [41.4610],
         [42.2483],
         [42.5777],
         [42.5427],
         [41.9461],
         [41.1365],
         [40.5680],
         [40.1199],
         [39.4815],
         [38.6702],
         [37.6909],
         [36.7258],
         [36.0096],
         [35.5149],
         [35.2093],
         [35.0751],
         [35.0641],
         [35.1062],
         [35.2398],
         [35.5621],
         [36.0038]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.2577],
         [36.4386],
         [36.4365],
         [36.2626],
         [36.0177],
         [35.8453],
         [35.8271],
         [35.8212],
         [35.5507],
         [35.1851],
         [35.2148],
         [35.7212],
         [36.2532],
         [36.5892],
         [36.7040],
         [36.6622],
         [36.5436],
         [36.4921],
         [36.6824],
         [37.2055],
         [38.0486],
         [38.9203],
         [39.6973],
         [40.3317],
         [40.5231],
         [40.2037],
         [39.6679],
         [39.1338],
         [38.7048],
         [38.3619],
         [38.0971],
         [37.9741]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[37.8104],
         [37.4211],
         [37.0699],
         [37.0156],
         [37.0684],
         [36.9150],
         [36.6092],
         [36.3009],
         [36.0606],
         [35.9134],
         [35.8934],
         [36.1462],
         [36.7208],
         [37.3411],
         [37.1061],
         [36.1232],
         [35.1914],
         [34.6081],
         [34.2805],
         [33.9862],
         [33.6999],
         [33.4497],
         [33.1633],
         [32.8277],
         [32.4840],
         [32.1515],
         [31.9025],
         [31.6785],
         [31.4158],
         [31.1991],
         [31.1381],
         [31.2455]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.5780],
         [31.8602],
         [32.1479],
         [32.3602],
         [32.4001],
         [32.3099],
         [32.1028],
         [31.8364],
         [31.5403],
         [31.2564],
         [31.0886],
         [31.0058],
         [31.0138],
         [31.1323],
         [31.4652],
         [31.9641],
         [32.4809],
         [32.9051],
         [33.3030],
         [33.8414],
         [34.4199],
         [34.9170],
         [35.3197],
         [35.3899],
         [35.2602],
         [35.0604],
         [34.7913],
         [34.5824],
         [34.4592],
         [34.3935],
         [34.3625],
         [34.3680]]], grad_fn=<ThAddBackward>)
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[34.5590],
         [34.5193],
         [34.4881],
         [34.5151],
         [34.8086],
         [35.2300],
         [35.9554],
         [36.8332],
         [37.3470],
         [37.4954],
         [37.5676],
         [37.6247],
         [37.6205],
         [38.3846],
         [39.8484],
         [41.0584],
         [41.7455],
         [41.9745],
         [41.9633],
         [41.9290],
         [41.9162],
         [41.8790],
         [41.7651],
         [41.6828],
         [41.7410],
         [41.7060],
         [41.5479]]], grad_fn=<ThAddBackward>)
[ 40.53887558  39.51016998  39.4202652   39.67451477  39.92925262
  40.25494766  40.7277298   40.8707962   40.53764343  40.0983429
  39.77249146  39.51180267  39.21112442  38.85839081  38.63803101
  38.67299652  38.83289337  38.97963333  38.96987534  38.69402695
  38.28361511  37.94252777  37.80747986  37.77641296  37.96541595
  38.45137787  38.96713638  39.29405594  39.47505188  39.62902832
  39.74681473  39.84258652  39.51111221  39.22879028  38.9551506
  38.68755341  38.6048317   38.6309166   38.58480072  38.45866013
  38.51958084  39.17394638  40.30203247  41.4609642   42.24825668
  42.57773209  42.54269791  41.94609833  41.13650131  40.5680275
  40.11986923  39.4814682   38.6701622   37.69090652  36.72583389
  36.00963211  35.51490784  35.20930862  35.0751152   35.06408691
  35.10616302  35.2397995   35.56210327  36.00379181  36.25767136
  36.43861008  36.43648148  36.2626152   36.01770401  35.84531784
  35.8270874   35.82123184  35.55070114  35.18510818  35.21483231
  35.72118759  36.25318527  36.58919144  36.70399094  36.6622467
  36.54363632  36.49205017  36.6823616   37.20553589  38.04857254
  38.92029572  39.69733429  40.33174896  40.52312088  40.20365524
  39.66785812  39.13378143  38.70479202  38.36188507  38.09710312
  37.97408295  37.8103714   37.42106247  37.06985474  37.01560974
  37.06841278  36.91503525  36.60918808  36.30086899  36.06056213
  35.9134407   35.89341354  36.1461792   36.72084427  37.34111786
  37.10606766  36.12317276  35.19137192  34.60810471  34.28047562
  33.9862442   33.69994736  33.44973373  33.16328049  32.82774734
  32.48396683  32.15145111  31.90250778  31.67848015  31.41584396
  31.19905663  31.13810539  31.24546814  31.57796669  31.86021614
  32.14787674  32.36024475  32.40013123  32.30993652  32.10283661
  31.83642006  31.540308    31.25635719  31.0885582   31.00584221
  31.01375771  31.13226891  31.46515846  31.96410751  32.48091888
  32.90511322  33.30304337  33.84144974  34.41989899  34.91703415
  35.31971741  35.38987732  35.26022339  35.06040955  34.79129791
  34.58243561  34.45919037  34.39353943  34.36253357  34.3680191
  34.55904388  34.51932907  34.48810959  34.51514816  34.80858231
  35.22996902  35.95535278  36.83317184  37.34704208  37.49544144
  37.56759644  37.62474442  37.62052536  38.38459396  39.84843826
  41.05837631  41.74552155  41.97447205  41.96331787  41.9289856
  41.91622162  41.87901306  41.76505661  41.68278885  41.74104691
  41.70602417  41.54789352]
SimpleRnn: epoch num: 28
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:52:54
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[-0.1107],
         [ 0.2125],
         [ 0.3964],
         [ 0.3902],
         [ 0.4383],
         [ 0.4182],
         [ 0.4413],
         [ 0.4540],
         [ 0.4277],
         [ 0.4199],
         [ 0.4128],
         [ 0.4112],
         [ 0.4114],
         [ 0.4038],
         [ 0.3925],
         [ 0.4004],
         [ 0.4070],
         [ 0.4099],
         [ 0.4187],
         [ 0.4091],
         [ 0.3952],
         [ 0.3896],
         [ 0.3829],
         [ 0.3970],
         [ 0.3764],
         [ 0.4097],
         [ 0.4122],
         [ 0.4242],
         [ 0.4160],
         [ 0.4212],
         [ 0.4198],
         [ 0.4194]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:87: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
[1,     1] loss: 7.516
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[0.6832],
         [0.7754],
         [0.7979],
         [0.7915],
         [0.7799],
         [0.8002],
         [0.7893],
         [0.7931],
         [0.7755],
         [0.7982],
         [0.8497],
         [0.8843],
         [0.9134],
         [0.9033],
         [0.8967],
         [0.8782],
         [0.8222],
         [0.8234],
         [0.8144],
         [0.8099],
         [0.7771],
         [0.7619],
         [0.7156],
         [0.7153],
         [0.7015],
         [0.7051],
         [0.7034],
         [0.7112],
         [0.7122],
         [0.7151],
         [0.7232],
         [0.7445]]], grad_fn=<ThAddBackward>)
[1,     2] loss: 7.327
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[1.0006],
         [1.0954],
         [1.1178],
         [1.1054],
         [1.0924],
         [1.0809],
         [1.0856],
         [1.0917],
         [1.0845],
         [1.0534],
         [1.0559],
         [1.0893],
         [1.1278],
         [1.1309],
         [1.1361],
         [1.1187],
         [1.1160],
         [1.1034],
         [1.1180],
         [1.1395],
         [1.1838],
         [1.2252],
         [1.2466],
         [1.2702],
         [1.2717],
         [1.2398],
         [1.2044],
         [1.1808],
         [1.1650],
         [1.1605],
         [1.1513],
         [1.1524]]], grad_fn=<ThAddBackward>)
[1,     3] loss: 7.030
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[1.4092],
         [1.4807],
         [1.4851],
         [1.4947],
         [1.5143],
         [1.5062],
         [1.4829],
         [1.4661],
         [1.4551],
         [1.4522],
         [1.4542],
         [1.4705],
         [1.5103],
         [1.5514],
         [1.5465],
         [1.4338],
         [1.3893],
         [1.3611],
         [1.3715],
         [1.3608],
         [1.3544],
         [1.3424],
         [1.3345],
         [1.3133],
         [1.3001],
         [1.2829],
         [1.2739],
         [1.2693],
         [1.2564],
         [1.2455],
         [1.2458],
         [1.2581]]], grad_fn=<ThAddBackward>)
[1,     4] loss: 5.675
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[1.5016],
         [1.5938],
         [1.6358],
         [1.6534],
         [1.6483],
         [1.6336],
         [1.6195],
         [1.5934],
         [1.5808],
         [1.5592],
         [1.5562],
         [1.5604],
         [1.5613],
         [1.5770],
         [1.5980],
         [1.6454],
         [1.6731],
         [1.6980],
         [1.7081],
         [1.7437],
         [1.7848],
         [1.8052],
         [1.8233],
         [1.8241],
         [1.7782],
         [1.7810],
         [1.7481],
         [1.7419],
         [1.7403],
         [1.7416],
         [1.7412],
         [1.7447]]], grad_fn=<ThAddBackward>)
[1,     5] loss: 5.178
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[1.9705],
         [2.0194],
         [2.0408],
         [2.0533],
         [2.0814],
         [2.1373],
         [2.1768],
         [2.2759],
         [2.2843],
         [2.2662],
         [2.2552],
         [2.2652],
         [2.2530],
         [2.3106],
         [2.4959],
         [2.5672],
         [2.5750],
         [2.5607],
         [2.5318],
         [2.5222],
         [2.5223],
         [2.5191],
         [2.5080],
         [2.4982],
         [2.5117],
         [2.5153],
         [2.4870]]], grad_fn=<ThAddBackward>)
[1,     6] loss: 7.023
[-0.11074618  0.21247628  0.39640957  0.39019805  0.43832242  0.41815707
  0.44132534  0.45404205  0.42768994  0.41987479  0.41275904  0.4111838
  0.41143981  0.40381297  0.39252341  0.40039623  0.40701184  0.40989745
  0.41865823  0.40905067  0.3952013   0.38959143  0.38286343  0.39697817
  0.37638113  0.40970346  0.41219822  0.42416993  0.41598997  0.42120603
  0.41979733  0.41943207  0.68321496  0.77537555  0.79793835  0.79150909
  0.77986437  0.80023772  0.78929812  0.79313278  0.77550477  0.79822415
  0.84972775  0.88433164  0.91338378  0.90325785  0.89666539  0.87823516
  0.82218975  0.82344395  0.81438345  0.80992883  0.77706867  0.76192081
  0.71555787  0.71530855  0.70149583  0.70511359  0.70335329  0.71124512
  0.71215415  0.71512985  0.72318733  0.74446249  1.00056791  1.09539247
  1.11780047  1.1053822   1.09236324  1.0809145   1.08563495  1.09174752
  1.08449554  1.05335248  1.05593944  1.08933663  1.12782323  1.1309489
  1.13608551  1.11872363  1.11602783  1.10338223  1.11804652  1.13950658
  1.18375647  1.22520423  1.24662185  1.27018964  1.27166212  1.23983157
  1.20440829  1.18080389  1.16500735  1.16048706  1.151335    1.15242839
  1.40924692  1.48071086  1.48507988  1.49474132  1.51432967  1.50624073
  1.48288536  1.46607697  1.45511496  1.45221913  1.45422292  1.47047269
  1.51033664  1.55140817  1.54651129  1.43382132  1.38934171  1.36114657
  1.37146902  1.36084628  1.3543911   1.34238183  1.33454287  1.31332779
  1.30005205  1.28287232  1.27385008  1.26929915  1.25643659  1.24545538
  1.24581659  1.25810838  1.50157464  1.59383702  1.63579667  1.65339005
  1.6483165   1.63355374  1.61946261  1.59344661  1.58077252  1.55924141
  1.55624068  1.56038082  1.56132627  1.57699859  1.59795845  1.64538229
  1.6730864   1.69800937  1.70814073  1.74368346  1.78484881  1.8052057
  1.82329977  1.82413566  1.77821064  1.78103948  1.74814475  1.74189758
  1.74029386  1.74157155  1.74119294  1.74466765  1.97047508  2.01938152
  2.04078031  2.05325103  2.08141041  2.13727474  2.17676187  2.27587485
  2.28432846  2.26622391  2.25519133  2.26518536  2.253021    2.31055689
  2.49585247  2.56715727  2.57497025  2.56067133  2.53179312  2.5222168
  2.52230453  2.5191009   2.50796318  2.49818707  2.51165771  2.5153203
  2.48703885]
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[2.5577],
         [2.5856],
         [2.7190],
         [2.7860],
         [2.8216],
         [2.8484],
         [2.9063],
         [2.9021],
         [2.8320],
         [2.7871],
         [2.7711],
         [2.7561],
         [2.7322],
         [2.7016],
         [2.6904],
         [2.7151],
         [2.7401],
         [2.7508],
         [2.7439],
         [2.7026],
         [2.6566],
         [2.6348],
         [2.6373],
         [2.6494],
         [2.6680],
         [2.7380],
         [2.7816],
         [2.7943],
         [2.7925],
         [2.8031],
         [2.8078],
         [2.8111]]], grad_fn=<ThAddBackward>)
[2,     1] loss: 6.630
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[3.0882],
         [3.1773],
         [3.2092],
         [3.2073],
         [3.2213],
         [3.2473],
         [3.2373],
         [3.2242],
         [3.2349],
         [3.3309],
         [3.4803],
         [3.5930],
         [3.6436],
         [3.6336],
         [3.6025],
         [3.5088],
         [3.3973],
         [3.3667],
         [3.3348],
         [3.2693],
         [3.1797],
         [3.0894],
         [2.9910],
         [2.9514],
         [2.9234],
         [2.9148],
         [2.9188],
         [2.9334],
         [2.9429],
         [2.9617],
         [3.0046],
         [3.0584]]], grad_fn=<ThAddBackward>)
[2,     2] loss: 6.425
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[3.3706],
         [3.4981],
         [3.5426],
         [3.5317],
         [3.5127],
         [3.5011],
         [3.5155],
         [3.5205],
         [3.4765],
         [3.4201],
         [3.4515],
         [3.5466],
         [3.6135],
         [3.6279],
         [3.6317],
         [3.6072],
         [3.5903],
         [3.5813],
         [3.6211],
         [3.6969],
         [3.8137],
         [3.9166],
         [3.9850],
         [4.0472],
         [4.0366],
         [3.9510],
         [3.8712],
         [3.8137],
         [3.7699],
         [3.7445],
         [3.7208],
         [3.7222]]], grad_fn=<ThAddBackward>)
[2,     3] loss: 6.114
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[4.0134],
         [4.0752],
         [4.0869],
         [4.1353],
         [4.1745],
         [4.1500],
         [4.1011],
         [4.0669],
         [4.0422],
         [4.0311],
         [4.0394],
         [4.0889],
         [4.1853],
         [4.2746],
         [4.2046],
         [3.9967],
         [3.8777],
         [3.8350],
         [3.8158],
         [3.7894],
         [3.7602],
         [3.7364],
         [3.7046],
         [3.6589],
         [3.6188],
         [3.5804],
         [3.5554],
         [3.5359],
         [3.5021],
         [3.4776],
         [3.4831],
         [3.5108]]], grad_fn=<ThAddBackward>)
[2,     4] loss: 4.869
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[3.8172],
         [3.9657],
         [4.0610],
         [4.1162],
         [4.1220],
         [4.1024],
         [4.0695],
         [4.0253],
         [3.9837],
         [3.9440],
         [3.9282],
         [3.9279],
         [3.9343],
         [3.9613],
         [4.0193],
         [4.1073],
         [4.1803],
         [4.2339],
         [4.2806],
         [4.3595],
         [4.4469],
         [4.5047],
         [4.5503],
         [4.5427],
         [4.4931],
         [4.4629],
         [4.4164],
         [4.3856],
         [4.3780],
         [4.3739],
         [4.3730],
         [4.3784]]], grad_fn=<ThAddBackward>)
[2,     5] loss: 4.397
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[4.6857],
         [4.7858],
         [4.8415],
         [4.8852],
         [4.9528],
         [5.0528],
         [5.1682],
         [5.3408],
         [5.3994],
         [5.3833],
         [5.3827],
         [5.3904],
         [5.3770],
         [5.5139],
         [5.8210],
         [5.9998],
         [6.0583],
         [6.0651],
         [6.0305],
         [6.0104],
         [6.0060],
         [5.9950],
         [5.9716],
         [5.9560],
         [5.9749],
         [5.9734],
         [5.9326]]], grad_fn=<ThAddBackward>)
[2,     6] loss: 5.869
[ 2.55769968  2.58558035  2.71904802  2.78599954  2.82159328  2.8483901
  2.90627193  2.90210867  2.8320024   2.78711224  2.77109456  2.75613284
  2.73215294  2.70158815  2.6904285   2.71508145  2.74010086  2.75079393
  2.74385071  2.70262194  2.65661788  2.63484383  2.63733602  2.64943051
  2.66801167  2.73798251  2.78155637  2.7942853   2.79250431  2.80307317
  2.80784512  2.81107187  3.08820033  3.1773088   3.20915413  3.20726657
  3.22130561  3.24732494  3.23730254  3.22422886  3.23487067  3.33089185
  3.48031592  3.59300303  3.64363122  3.63361502  3.6025002   3.5088098
  3.39731812  3.36667633  3.33482409  3.26932788  3.17965627  3.08935666
  2.99095082  2.95137334  2.92340779  2.91479564  2.91881537  2.9333508
  2.94286513  2.96174049  3.00463152  3.05839038  3.37062383  3.4981215
  3.54256415  3.53168273  3.51274514  3.50109863  3.51550555  3.52050233
  3.4765048   3.42008948  3.45151973  3.54655051  3.61349654  3.62787271
  3.63173389  3.60716248  3.59033442  3.58133721  3.62114024  3.69685221
  3.8136766   3.91662812  3.98502946  4.04722261  4.03662634  3.95098853
  3.87119746  3.81366587  3.76986265  3.74449801  3.72083259  3.72215319
  4.0133853   4.07524252  4.08689022  4.13528204  4.17445755  4.14995766
  4.10109329  4.06688976  4.04217625  4.03113222  4.03935719  4.08886433
  4.18531942  4.27460623  4.20460796  3.99671459  3.87773037  3.83501768
  3.81575584  3.78937411  3.76020932  3.73638344  3.70461941  3.65889788
  3.61882401  3.58041859  3.55540705  3.53588104  3.50209188  3.47760463
  3.48312759  3.51079512  3.8172133   3.96568918  4.06102228  4.11622667
  4.12196827  4.10240984  4.06952572  4.02528334  3.98368263  3.94398022
  3.92816567  3.92786908  3.93434191  3.96129322  4.01925373  4.10734129
  4.18029499  4.2338624   4.28056002  4.35954332  4.44689322  4.50470257
  4.55031681  4.54266357  4.49313498  4.46288633  4.41641855  4.38557005
  4.37800646  4.37394905  4.37300444  4.37835169  4.68569899  4.78584671
  4.84151173  4.88522673  4.95281887  5.05280447  5.16821098  5.3407917
  5.3994441   5.38329029  5.38273621  5.39042282  5.37695789  5.51394558
  5.82102394  5.99980116  6.0582633   6.06507492  6.03047514  6.01037693
  6.00600433  5.99500036  5.97157526  5.95602608  5.97486591  5.97340393
  5.93260193]
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[6.0308],
         [6.0237],
         [6.2287],
         [6.3668],
         [6.4538],
         [6.5365],
         [6.6487],
         [6.6850],
         [6.5745],
         [6.4756],
         [6.4385],
         [6.3951],
         [6.3412],
         [6.2780],
         [6.2373],
         [6.2670],
         [6.3122],
         [6.3380],
         [6.3388],
         [6.2751],
         [6.1821],
         [6.1265],
         [6.1108],
         [6.1255],
         [6.1557],
         [6.2725],
         [6.3738],
         [6.4193],
         [6.4331],
         [6.4590],
         [6.4765],
         [6.4830]]], grad_fn=<ThAddBackward>)
[3,     1] loss: 5.387
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[6.9364],
         [7.1397],
         [7.2302],
         [7.2680],
         [7.2942],
         [7.3571],
         [7.3553],
         [7.3306],
         [7.3398],
         [7.4929],
         [7.7654],
         [8.0124],
         [8.1685],
         [8.2037],
         [8.1750],
         [8.0404],
         [7.8140],
         [7.7165],
         [7.6494],
         [7.5099],
         [7.3276],
         [7.1419],
         [6.9288],
         [6.8036],
         [6.7245],
         [6.6716],
         [6.6602],
         [6.6754],
         [6.6901],
         [6.7193],
         [6.7934],
         [6.9009]]], grad_fn=<ThAddBackward>)
[3,     2] loss: 5.042
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[7.4500],
         [7.7477],
         [7.8963],
         [7.9354],
         [7.9259],
         [7.9116],
         [7.9297],
         [7.9492],
         [7.8924],
         [7.7800],
         [7.7906],
         [7.9458],
         [8.0872],
         [8.1446],
         [8.1752],
         [8.1553],
         [8.1229],
         [8.1015],
         [8.1503],
         [8.2791],
         [8.4893],
         [8.7086],
         [8.8745],
         [9.0240],
         [9.0722],
         [8.9602],
         [8.8124],
         [8.6911],
         [8.5856],
         [8.5119],
         [8.4507],
         [8.4255]]], grad_fn=<ThAddBackward>)
[3,     3] loss: 4.616
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[8.9525],
         [9.1312],
         [9.1984],
         [9.3074],
         [9.4098],
         [9.3983],
         [9.3239],
         [9.2644],
         [9.2093],
         [9.1761],
         [9.1752],
         [9.2412],
         [9.4043],
         [9.5863],
         [9.5597],
         [9.2045],
         [8.9776],
         [8.8335],
         [8.7545],
         [8.6725],
         [8.5979],
         [8.5353],
         [8.4693],
         [8.3708],
         [8.2857],
         [8.1980],
         [8.1319],
         [8.0838],
         [8.0092],
         [7.9485],
         [7.9397],
         [7.9742]]], grad_fn=<ThAddBackward>)
[3,     4] loss: 3.449
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[ 8.5379],
         [ 8.8713],
         [ 9.1132],
         [ 9.2870],
         [ 9.3623],
         [ 9.3732],
         [ 9.3447],
         [ 9.2692],
         [ 9.1946],
         [ 9.1046],
         [ 9.0581],
         [ 9.0430],
         [ 9.0360],
         [ 9.0748],
         [ 9.1633],
         [ 9.3247],
         [ 9.4690],
         [ 9.6022],
         [ 9.7108],
         [ 9.8757],
         [10.0566],
         [10.1965],
         [10.3203],
         [10.3651],
         [10.2982],
         [10.2695],
         [10.1692],
         [10.1057],
         [10.0728],
         [10.0528],
         [10.0403],
         [10.0428]]], grad_fn=<ThAddBackward>)
[3,     5] loss: 2.944
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[10.6535],
         [10.9336],
         [11.1183],
         [11.2473],
         [11.3983],
         [11.6226],
         [11.8378],
         [12.2130],
         [12.3846],
         [12.4330],
         [12.4567],
         [12.5041],
         [12.4905],
         [12.6800],
         [13.2591],
         [13.6354],
         [13.8693],
         [13.9653],
         [13.9546],
         [13.9472],
         [13.9508],
         [13.9408],
         [13.9054],
         [13.8675],
         [13.8930],
         [13.9022],
         [13.8225]]], grad_fn=<ThAddBackward>)
[3,     6] loss: 3.642
[  6.03084421   6.02365875   6.22873116   6.36680269   6.45381594
   6.53653765   6.64870644   6.68501997   6.57448149   6.47560167
   6.43847036   6.39514828   6.34118843   6.27803326   6.23731518
   6.26700878   6.31216383   6.33799124   6.33876944   6.27505016
   6.18210363   6.12646246   6.1108346    6.12548161   6.15569496
   6.27246618   6.37379265   6.419312     6.4330554    6.45904303
   6.47646713   6.48302412   6.93636656   7.13968754   7.23016405
   7.26800728   7.29419947   7.35711527   7.35529613   7.33057976
   7.33978939   7.49291611   7.76543903   8.01236248   8.16848469
   8.20366764   8.17499638   8.04041386   7.81398392   7.71654797
   7.64938259   7.50987864   7.32757759   7.14188576   6.9288311
   6.80355597   6.72452831   6.67158794   6.6602335    6.67544889
   6.69005394   6.7192688    6.79335308   6.90086746   7.45004845
   7.74774456   7.89628935   7.93537855   7.9258647    7.91161776
   7.92973089   7.94921541   7.89243698   7.78003836   7.79056025
   7.94575453   8.08721924   8.14461231   8.17517662   8.15525436
   8.12285995   8.10151672   8.15031624   8.27905369   8.48925591
   8.70860291   8.87453651   9.02401352   9.07218742   8.96023369
   8.81236076   8.69108009   8.58563137   8.51188564   8.45074272
   8.42546082   8.95249844   9.13116169   9.19837856   9.30739784
   9.40983295   9.39829159   9.32385921   9.26442337   9.20932579
   9.17611027   9.17520046   9.24116993   9.40434837   9.5862627
   9.55965614   9.20453358   8.97764397   8.83353901   8.75451756
   8.67247295   8.59788227   8.53534889   8.46926498   8.37078762
   8.28569508   8.19795609   8.13192844   8.08378315   8.00922489
   7.94851303   7.939744     7.97415304   8.53785038   8.87129116
   9.11318302   9.28704739   9.36229801   9.37320518   9.34472561
   9.26916122   9.1945982    9.10457802   9.05809116   9.04298496
   9.03595924   9.07482624   9.16332722   9.32466793   9.46903515
   9.60224819   9.71082973   9.87572193  10.05662251  10.1965313
  10.3203373   10.3651495   10.29821682  10.26953602  10.1692028
  10.10566235  10.07284069  10.05281353  10.04027843  10.04283333
  10.65348053  10.93363762  11.11834145  11.24729633  11.3982935
  11.62264538  11.83777142  12.21296024  12.38460445  12.43296051
  12.45668507  12.50409603  12.49050331  12.67996693  13.25905132
  13.63542652  13.86934948  13.96534348  13.95458412  13.94718838
  13.95076656  13.94075203  13.90541363  13.86745644  13.8929987
  13.90223312  13.82253361]
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[14.1550],
         [14.2361],
         [14.5905],
         [14.8860],
         [15.1228],
         [15.3144],
         [15.5755],
         [15.6989],
         [15.5791],
         [15.4385],
         [15.3472],
         [15.2569],
         [15.1460],
         [15.0122],
         [14.9167],
         [14.9318],
         [14.9906],
         [15.0348],
         [15.0398],
         [14.9380],
         [14.7776],
         [14.6537],
         [14.5961],
         [14.5914],
         [14.6242],
         [14.8202],
         [15.0123],
         [15.1326],
         [15.1956],
         [15.2672],
         [15.3186],
         [15.3543]]], grad_fn=<ThAddBackward>)
[4,     1] loss: 2.918
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[16.3392],
         [16.8447],
         [17.1473],
         [17.3242],
         [17.4692],
         [17.6365],
         [17.6930],
         [17.7059],
         [17.7480],
         [18.0273],
         [18.5184],
         [19.0163],
         [19.3931],
         [19.5740],
         [19.6264],
         [19.4578],
         [19.1050],
         [18.9060],
         [18.7222],
         [18.4478],
         [18.0746],
         [17.6763],
         [17.2107],
         [16.8903],
         [16.6426],
         [16.4646],
         [16.3606],
         [16.3227],
         [16.3083],
         [16.3365],
         [16.4500],
         [16.6330]]], grad_fn=<ThAddBackward>)
[4,     2] loss: 2.292
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[17.8148],
         [18.5277],
         [18.9938],
         [19.2467],
         [19.3858],
         [19.4682],
         [19.5704],
         [19.6508],
         [19.5965],
         [19.4337],
         [19.4503],
         [19.6897],
         [19.9467],
         [20.0934],
         [20.2021],
         [20.2078],
         [20.1920],
         [20.1662],
         [20.2590],
         [20.4826],
         [20.8596],
         [21.2857],
         [21.6673],
         [22.0324],
         [22.2223],
         [22.1452],
         [21.9551],
         [21.7487],
         [21.5498],
         [21.3868],
         [21.2344],
         [21.1469]]], grad_fn=<ThAddBackward>)
[4,     3] loss: 1.673
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[22.3123],
         [22.8240],
         [23.1251],
         [23.4341],
         [23.7286],
         [23.8425],
         [23.8173],
         [23.7699],
         [23.7058],
         [23.6583],
         [23.6512],
         [23.7556],
         [24.0333],
         [24.3661],
         [24.3838],
         [23.8821],
         [23.4642],
         [23.1496],
         [22.9306],
         [22.7093],
         [22.4991],
         [22.3107],
         [22.1224],
         [21.8912],
         [21.6735],
         [21.4511],
         [21.2641],
         [21.1074],
         [20.9196],
         [20.7532],
         [20.6683],
         [20.6715]]], grad_fn=<ThAddBackward>)
[4,     4] loss: 0.769
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[21.9190],
         [22.7177],
         [23.3506],
         [23.8739],
         [24.2341],
         [24.4557],
         [24.5678],
         [24.5722],
         [24.5301],
         [24.4342],
         [24.3674],
         [24.3344],
         [24.3179],
         [24.3703],
         [24.5161],
         [24.7900],
         [25.0833],
         [25.3697],
         [25.6372],
         [25.9929],
         [26.3991],
         [26.7613],
         [27.0927],
         [27.2948],
         [27.3089],
         [27.3264],
         [27.2354],
         [27.1508],
         [27.0960],
         [27.0529],
         [27.0199],
         [27.0093]]], grad_fn=<ThAddBackward>)
[4,     5] loss: 0.365
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[28.4426],
         [29.2368],
         [29.8458],
         [30.3626],
         [30.8871],
         [31.4845],
         [32.0931],
         [32.9029],
         [33.4878],
         [33.8631],
         [34.1663],
         [34.4493],
         [34.6202],
         [35.0504],
         [36.0438],
         [36.9106],
         [37.5467],
         [38.0183],
         [38.3047],
         [38.5294],
         [38.7199],
         [38.8492],
         [38.9069],
         [38.9326],
         [39.0183],
         [39.0727],
         [39.0020]]], grad_fn=<ThAddBackward>)
[4,     6] loss: 0.125
[ 14.15503216  14.23606873  14.5904541   14.8860302   15.1228199   15.314394
  15.57552433  15.69886398  15.57913208  15.43846512  15.34718895
  15.25692749  15.14600563  15.01216412  14.91672134  14.93179417
  14.99060917  15.0348053   15.03976345  14.93799019  14.77762794
  14.65367222  14.59614563  14.59141922  14.62416458  14.82020092
  15.01233292  15.13261986  15.19559383  15.26715374  15.31855679
  15.3542881   16.33916664  16.84474182  17.14725304  17.32421303
  17.4692173   17.63645554  17.69299316  17.70588875  17.74804497
  18.0272541   18.51841545  19.01633072  19.39314842  19.57403946
  19.62638664  19.45779228  19.10500526  18.90595436  18.72224998
  18.44784355  18.07455635  17.67630005  17.21066856  16.8903141
  16.64258766  16.46461678  16.36057281  16.32266426  16.30825615
  16.33648682  16.44995499  16.63300514  17.81478119  18.52768135
  18.99379539  19.24666595  19.38579369  19.46819115  19.57042694
  19.65083504  19.59645271  19.43366432  19.45025826  19.68970299
  19.94667625  20.09338188  20.20210075  20.20779037  20.19203377
  20.16616631  20.25898552  20.48256111  20.85960197  21.28569412
  21.66731262  22.0323658   22.2222538   22.14515495  21.95513916
  21.74872017  21.54978371  21.3867836   21.23440742  21.14690781
  22.31227684  22.82399178  23.12510109  23.43413734  23.72862625
  23.84253693  23.81734467  23.76994133  23.70576477  23.65834618
  23.65120697  23.75555229  24.03325081  24.36610794  24.38383293
  23.8821106   23.46418953  23.14958572  22.93057442  22.70931625
  22.49905586  22.31071472  22.12236214  21.89118576  21.67350769
  21.45112801  21.26411438  21.10738945  20.91960716  20.75319862
  20.66831207  20.67154121  21.91897202  22.71766663  23.35055351
  23.87388229  24.23410225  24.4557457   24.56782913  24.57218933
  24.53011322  24.43422699  24.36742783  24.33435059  24.31785965
  24.37029266  24.51608658  24.79001617  25.08333778  25.36967278
  25.63718796  25.99291229  26.3991394   26.76132774  27.09268188
  27.29484177  27.30886269  27.3264122   27.23540497  27.15083694
  27.09598923  27.05294037  27.01994705  27.00928497  28.44264603
  29.23682594  29.84581566  30.36259651  30.88707352  31.48454475
  32.09314346  32.9029274   33.4878273   33.86305237  34.16626358
  34.44927216  34.62024689  35.05042267  36.04381561  36.91057205
  37.54667664  38.01827621  38.30469513  38.52936935  38.71986389
  38.84919357  38.90693283  38.93257141  39.01832199  39.07266617
  39.00195694]
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.2663],
         [40.8763],
         [41.7618],
         [42.5889],
         [43.3170],
         [43.9942],
         [44.7406],
         [45.3014],
         [45.4849],
         [45.5298],
         [45.5652],
         [45.5428],
         [45.4400],
         [45.2609],
         [45.0969],
         [45.0703],
         [45.1199],
         [45.1691],
         [45.1767],
         [45.0337],
         [44.7787],
         [44.5372],
         [44.3679],
         [44.2698],
         [44.2423],
         [44.4664],
         [44.7565],
         [44.9939],
         [45.1753],
         [45.3702],
         [45.5435],
         [45.6894]]], grad_fn=<ThAddBackward>)
[5,     1] loss: 0.168
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[47.5076],
         [48.5837],
         [49.4036],
         [50.0404],
         [50.6071],
         [51.1757],
         [51.5791],
         [51.8692],
         [52.1595],
         [52.7454],
         [53.6789],
         [54.7120],
         [55.6509],
         [56.3586],
         [56.8552],
         [56.9850],
         [56.7589],
         [56.5898],
         [56.3752],
         [55.9589],
         [55.3424],
         [54.6028],
         [53.7054],
         [52.9143],
         [52.1922],
         [51.5615],
         [51.0557],
         [50.6759],
         [50.3769],
         [50.1827],
         [50.1557],
         [50.2889]]], grad_fn=<ThAddBackward>)
[5,     2] loss: 1.019
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[51.7556],
         [52.6470],
         [53.3123],
         [53.7741],
         [54.1049],
         [54.3614],
         [54.6276],
         [54.8634],
         [54.9126],
         [54.7855],
         [54.8306],
         [55.1712],
         [55.6014],
         [55.9362],
         [56.2237],
         [56.3859],
         [56.4895],
         [56.5584],
         [56.7530],
         [57.1378],
         [57.7798],
         [58.5617],
         [59.3514],
         [60.1663],
         [60.7833],
         [61.0265],
         [61.0395],
         [60.9383],
         [60.7592],
         [60.5563],
         [60.3245],
         [60.1444]]], grad_fn=<ThAddBackward>)
[5,     3] loss: 1.711
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[60.9236],
         [60.9944],
         [60.9004],
         [60.9345],
         [61.0487],
         [61.0155],
         [60.8437],
         [60.6441],
         [60.4275],
         [60.2328],
         [60.0972],
         [60.1224],
         [60.4118],
         [60.8487],
         [60.9369],
         [60.3867],
         [59.7921],
         [59.2424],
         [58.7631],
         [58.2685],
         [57.7798],
         [57.3154],
         [56.8524],
         [56.3359],
         [55.8237],
         [55.3011],
         [54.8173],
         [54.3741],
         [53.9031],
         [53.4586],
         [53.1230],
         [52.9234]]], grad_fn=<ThAddBackward>)
[5,     4] loss: 2.754
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[53.2859],
         [53.3622],
         [53.4434],
         [53.5700],
         [53.6301],
         [53.6135],
         [53.5220],
         [53.3368],
         [53.1078],
         [52.8257],
         [52.5820],
         [52.3921],
         [52.2430],
         [52.1964],
         [52.2963],
         [52.5979],
         [52.9772],
         [53.3929],
         [53.8217],
         [54.3868],
         [55.0491],
         [55.6912],
         [56.3166],
         [56.7912],
         [57.0227],
         [57.2104],
         [57.2354],
         [57.2197],
         [57.2079],
         [57.1931],
         [57.1772],
         [57.1807]]], grad_fn=<ThAddBackward>)
[5,     5] loss: 2.153
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[57.2847],
         [57.0090],
         [56.7064],
         [56.4961],
         [56.4502],
         [56.6316],
         [56.9744],
         [57.6675],
         [58.2149],
         [58.5794],
         [58.8901],
         [59.1965],
         [59.3910],
         [59.9222],
         [61.1636],
         [62.3628],
         [63.3436],
         [64.1336],
         [64.6937],
         [65.1501],
         [65.5389],
         [65.8352],
         [66.0249],
         [66.1542],
         [66.3374],
         [66.4742],
         [66.4527]]], grad_fn=<ThAddBackward>)
[5,     6] loss: 2.351
[ 40.26633072  40.87626266  41.76181793  42.58890533  43.31701279
  43.99422836  44.74058151  45.3014183   45.48493958  45.52977371
  45.56523514  45.54278946  45.43996811  45.26085281  45.09689713
  45.07034683  45.11994171  45.16907883  45.17669678  45.03371811
  44.77871323  44.53722763  44.36785507  44.26978683  44.24229431
  44.46643448  44.75653839  44.99386215  45.17531204  45.37017441
  45.54354477  45.68941116  47.50764465  48.58365631  49.40355682
  50.04040909  50.60706329  51.17567825  51.57911682  51.8691864
  52.15948486  52.74543762  53.67885971  54.71197891  55.65087891
  56.35858154  56.8551712   56.98496246  56.75894165  56.58981323
  56.37520218  55.95892334  55.34235382  54.60284424  53.7053833
  52.91427994  52.1922226   51.56151581  51.05570221  50.67594528
  50.37690735  50.18266296  50.15574646  50.28892899  51.75559998
  52.64700699  53.31232834  53.77407455  54.10493088  54.36138153
  54.62761307  54.86343002  54.91257858  54.78546143  54.83063507
  55.17115784  55.60135651  55.93618011  56.22370911  56.38591385
  56.48945618  56.55843353  56.75300598  57.13775253  57.77977753
  58.56172943  59.35144806  60.16633987  60.78331375  61.02653122
  61.03950119  60.9383049   60.75916672  60.55627823  60.32453537
  60.14440918  60.92362595  60.9944191   60.90035629  60.9344902
  61.04866028  61.01547241  60.84370804  60.64406204  60.4275322
  60.2327919   60.09715652  60.12240982  60.4117775   60.84874344
  60.93692398  60.38668442  59.79212189  59.24242783  58.763134
  58.26852036  57.77982712  57.31544876  56.8523674   56.33594894
  55.82365417  55.30112076  54.81728745  54.37414551  53.90314102
  53.45857239  53.12301254  52.92338562  53.28591537  53.36222839
  53.44343567  53.57004166  53.63012695  53.6135025   53.52198029
  53.33683014  53.10783386  52.82565308  52.58196259  52.39212036
  52.24304962  52.19642639  52.29627228  52.59790039  52.97719193
  53.39286041  53.8216629   54.38679504  55.04911804  55.69118881
  56.3166275   56.79121399  57.02274323  57.210392    57.23535156
  57.21969604  57.20788193  57.19313049  57.17716217  57.18066406
  57.284729    57.00896835  56.70642471  56.49607086  56.45020676
  56.63155365  56.97437668  57.66754532  58.2149086   58.57941055
  58.89005661  59.19645691  59.39103317  59.92220688  61.16360474
  62.36275482  63.34355545  64.13356781  64.69374847  65.15013885
  65.53894043  65.83522797  66.0249176   66.15415955  66.33743286
  66.47421265  66.45265198]
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[65.4272],
         [64.2087],
         [63.5577],
         [63.1618],
         [62.8903],
         [62.7656],
         [62.8923],
         [62.9507],
         [62.6856],
         [62.3303],
         [62.0162],
         [61.6932],
         [61.3240],
         [60.9015],
         [60.5242],
         [60.3276],
         [60.2547],
         [60.2156],
         [60.1471],
         [59.9211],
         [59.5640],
         [59.2126],
         [58.9481],
         [58.7732],
         [58.7007],
         [58.9248],
         [59.2570],
         [59.5454],
         [59.7767],
         [60.0223],
         [60.2440],
         [60.4348]]], grad_fn=<ThAddBackward>)
[6,     1] loss: 2.414
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[60.1486],
         [59.6895],
         [59.1949],
         [58.7383],
         [58.3840],
         [58.1935],
         [57.9639],
         [57.7239],
         [57.5832],
         [57.8492],
         [58.5674],
         [59.4737],
         [60.3360],
         [60.9796],
         [61.4016],
         [61.4201],
         [61.0432],
         [60.7126],
         [60.3442],
         [59.7866],
         [59.0135],
         [58.1069],
         [57.0394],
         [56.0927],
         [55.2477],
         [54.5293],
         [53.9655],
         [53.5569],
         [53.2499],
         [53.0653],
         [53.0672],
         [53.2456]]], grad_fn=<ThAddBackward>)
[6,     2] loss: 1.819
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[52.8464],
         [52.4534],
         [52.0849],
         [51.6881],
         [51.2831],
         [50.9234],
         [50.6847],
         [50.5052],
         [50.2059],
         [49.7890],
         [49.6081],
         [49.7794],
         [50.0842],
         [50.3236],
         [50.5270],
         [50.6051],
         [50.6273],
         [50.6210],
         [50.7589],
         [51.1082],
         [51.7400],
         [52.5202],
         [53.3107],
         [54.1151],
         [54.6894],
         [54.8601],
         [54.7773],
         [54.5672],
         [54.2841],
         [53.9920],
         [53.6902],
         [53.4631]]], grad_fn=<ThAddBackward>)
[6,     3] loss: 0.917
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[52.3858],
         [51.2775],
         [50.2840],
         [49.6059],
         [49.1449],
         [48.6563],
         [48.1301],
         [47.6522],
         [47.2223],
         [46.8719],
         [46.6270],
         [46.5825],
         [46.8305],
         [47.2377],
         [47.2908],
         [46.6726],
         [46.0242],
         [45.4342],
         [44.9624],
         [44.4980],
         [44.0673],
         [43.6734],
         [43.2906],
         [42.8593],
         [42.4390],
         [42.0106],
         [41.6285],
         [41.2897],
         [40.9245],
         [40.5887],
         [40.3622],
         [40.2717]]], grad_fn=<ThAddBackward>)
[6,     4] loss: 0.573
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[39.4838],
         [38.9441],
         [38.5830],
         [38.3582],
         [38.1271],
         [37.8799],
         [37.6096],
         [37.2897],
         [36.9689],
         [36.6296],
         [36.3638],
         [36.1737],
         [36.0423],
         [36.0232],
         [36.1499],
         [36.4710],
         [36.8483],
         [37.2427],
         [37.6243],
         [38.1235],
         [38.6933],
         [39.2219],
         [39.7158],
         [40.0410],
         [40.1204],
         [40.1677],
         [40.0607],
         [39.9448],
         [39.8503],
         [39.7766],
         [39.7174],
         [39.6906]]], grad_fn=<ThAddBackward>)
[6,     5] loss: 0.111
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[38.7125],
         [37.8965],
         [37.2474],
         [36.7894],
         [36.5639],
         [36.6100],
         [36.8358],
         [37.4097],
         [37.8055],
         [38.0232],
         [38.1778],
         [38.3355],
         [38.3902],
         [38.7931],
         [39.8728],
         [40.8439],
         [41.5811],
         [42.0879],
         [42.3592],
         [42.5439],
         [42.6829],
         [42.7611],
         [42.7638],
         [42.7375],
         [42.7909],
         [42.8152],
         [42.7041]]], grad_fn=<ThAddBackward>)
[6,     6] loss: 0.026
[ 65.42715454  64.20869446  63.55768204  63.1618309   62.89031219
  62.76558304  62.89229584  62.95065308  62.68556213  62.33034515
  62.01620483  61.69322586  61.3239975   60.90153885  60.52415085
  60.32764435  60.25471497  60.21558762  60.14713287  59.92110443
  59.56403351  59.21263885  58.94813156  58.77322388  58.70065308
  58.92483139  59.25699615  59.54537964  59.77672577  60.02233124  60.24403
  60.43477249  60.14863205  59.68946075  59.19487381  58.73830795
  58.38396835  58.19354248  57.96387863  57.7239151   57.58324051
  57.84923935  58.56739807  59.47369766  60.33600998  60.97961807
  61.40159607  61.42012405  61.04318619  60.71258926  60.34424973
  59.78662491  59.01353836  58.10692596  57.03941727  56.09268188
  55.24773788  54.52931595  53.96548843  53.55694199  53.24990463
  53.06527328  53.06723022  53.24563217  52.84638977  52.45344543
  52.08493805  51.68812561  51.28305054  50.92342377  50.6846962
  50.50517654  50.20593262  49.78895569  49.60808563  49.7794075
  50.08422089  50.32356262  50.52703094  50.60514069  50.62734222
  50.62104034  50.75894547  51.10816956  51.74000168  52.52017975
  53.31071472  54.1150589   54.68941498  54.86012268  54.77732849
  54.56720734  54.28412628  53.99202728  53.69023895  53.46314621
  52.38575363  51.27751923  50.28395844  49.60586929  49.14493179
  48.65633774  48.1301384   47.6521759   47.22225571  46.87188339
  46.62699509  46.58247757  46.83052063  47.23765945  47.29082108
  46.67264938  46.02417374  45.43421555  44.9623642   44.49799728
  44.06729507  43.67335892  43.29058456  42.85930252  42.43897629
  42.01057816  41.62850571  41.28965378  40.92454147  40.58873749
  40.36220169  40.27166748  39.48375702  38.9441185   38.58298111
  38.35816193  38.12707138  37.87992859  37.6096344   37.28974533
  36.96889496  36.62963867  36.36375809  36.17370987  36.04228592
  36.02315521  36.14992905  36.47100067  36.84828186  37.24266052
  37.62431717  38.12345123  38.69326401  39.22191238  39.71577072
  40.04100418  40.12043381  40.16772461  40.06074142  39.94475937
  39.85031128  39.77660751  39.71743011  39.69061279  38.71249771
  37.89649963  37.24742508  36.78939438  36.56387329  36.61003113
  36.83583069  37.40970993  37.80550003  38.02322388  38.17781448
  38.33551407  38.39024734  38.79309464  39.87281036  40.84389496
  41.58112335  42.08794785  42.35919189  42.54387283  42.68289185
  42.76106262  42.76383209  42.73752213  42.79092789  42.81518173
  42.70413589]
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.7069],
         [39.1902],
         [38.4389],
         [38.0061],
         [37.7571],
         [37.6679],
         [37.8221],
         [37.8836],
         [37.6335],
         [37.3171],
         [37.0574],
         [36.8090],
         [36.5352],
         [36.2267],
         [35.9757],
         [35.9023],
         [35.9353],
         [35.9841],
         [35.9867],
         [35.8259],
         [35.5452],
         [35.2832],
         [35.1154],
         [35.0378],
         [35.0523],
         [35.3363],
         [35.6810],
         [35.9514],
         [36.1385],
         [36.3222],
         [36.4693],
         [36.5837]]], grad_fn=<ThAddBackward>)
[7,     1] loss: 0.039
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[35.9929],
         [35.5118],
         [35.0989],
         [34.7402],
         [34.4971],
         [34.4140],
         [34.2838],
         [34.1502],
         [34.0959],
         [34.3908],
         [35.0562],
         [35.8419],
         [36.5338],
         [36.9786],
         [37.1964],
         [37.0523],
         [36.5585],
         [36.1852],
         [35.8343],
         [35.3529],
         [34.7162],
         [34.0065],
         [33.1908],
         [32.5437],
         [32.0123],
         [31.6152],
         [31.3545],
         [31.2196],
         [31.1440],
         [31.1454],
         [31.2845],
         [31.5482]]], grad_fn=<ThAddBackward>)
[7,     2] loss: 0.142
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[31.4768],
         [31.4481],
         [31.3776],
         [31.2231],
         [31.0318],
         [30.8571],
         [30.7761],
         [30.7301],
         [30.5654],
         [30.2897],
         [30.2179],
         [30.4428],
         [30.7660],
         [31.0054],
         [31.1848],
         [31.2294],
         [31.2208],
         [31.1878],
         [31.2888],
         [31.5732],
         [32.0947],
         [32.7185],
         [33.3179],
         [33.8963],
         [34.2441],
         [34.2336],
         [34.0187],
         [33.7238],
         [33.4088],
         [33.1292],
         [32.8731],
         [32.7065]]], grad_fn=<ThAddBackward>)
[7,     3] loss: 0.240
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[32.4021],
         [31.9768],
         [31.5837],
         [31.3836],
         [31.3094],
         [31.1505],
         [30.9151],
         [30.6823],
         [30.4646],
         [30.2988],
         [30.2095],
         [30.2801],
         [30.5829],
         [30.9899],
         [31.0450],
         [30.5067],
         [29.9622],
         [29.4835],
         [29.1365],
         [28.8177],
         [28.5380],
         [28.2907],
         [28.0489],
         [27.7632],
         [27.4864],
         [27.2017],
         [26.9596],
         [26.7529],
         [26.5208],
         [26.3138],
         [26.1986],
         [26.1954]]], grad_fn=<ThAddBackward>)
[7,     4] loss: 0.170
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[26.3019],
         [26.4023],
         [26.5492],
         [26.6973],
         [26.7575],
         [26.7340],
         [26.6398],
         [26.4684],
         [26.2746],
         [26.0521],
         [25.8881],
         [25.7837],
         [25.7263],
         [25.7608],
         [25.9155],
         [26.2275],
         [26.5729],
         [26.9158],
         [27.2315],
         [27.6374],
         [28.0912],
         [28.4997],
         [28.8695],
         [29.0789],
         [29.0757],
         [29.0491],
         [28.8974],
         [28.7579],
         [28.6520],
         [28.5771],
         [28.5249],
         [28.5058]]], grad_fn=<ThAddBackward>)
[7,     5] loss: 0.221
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[28.6368],
         [28.5853],
         [28.5481],
         [28.5428],
         [28.6595],
         [28.9378],
         [29.3256],
         [29.9717],
         [30.4120],
         [30.6591],
         [30.8171],
         [30.9546],
         [30.9945],
         [31.3597],
         [32.3240],
         [33.1905],
         [33.8341],
         [34.2444],
         [34.4248],
         [34.5233],
         [34.5863],
         [34.6057],
         [34.5686],
         [34.5156],
         [34.5419],
         [34.5455],
         [34.4360]]], grad_fn=<ThAddBackward>)
[7,     6] loss: 0.325
[ 40.70689392  39.19024277  38.43888474  38.0060997   37.75706863
  37.66789627  37.82210541  37.88360214  37.63352585  37.31711197
  37.0574379   36.80904388  36.53518295  36.22670364  35.97568893
  35.90229416  35.93533325  35.98410416  35.98669434  35.8258934
  35.54523849  35.2832222   35.11538696  35.03776169  35.05225372
  35.33628464  35.68104172  35.95144653  36.13850021  36.32223511
  36.46930313  36.5836792   35.9928894   35.51182556  35.09885025
  34.74020767  34.49707031  34.41401672  34.2837677   34.15018463
  34.09590912  34.39083099  35.05622101  35.84190369  36.53382874
  36.97860718  37.19638443  37.05232239  36.55853271  36.18516922
  35.83432388  35.35289764  34.71621704  34.00645828  33.19077301
  32.54367828  32.01233673  31.61521149  31.3545475   31.21957397
  31.14398193  31.14539719  31.28450394  31.54820442  31.47677994
  31.44807625  31.37760544  31.22308922  31.03176308  30.85713959
  30.77607536  30.73007965  30.56544495  30.28972244  30.21793556
  30.44277573  30.7660408   31.00542068  31.18475723  31.22943115
  31.22083092  31.18775558  31.28884888  31.57324409  32.09473038
  32.71852493  33.31791306  33.89631271  34.24409103  34.23360825
  34.01873398  33.7237854   33.40884781  33.12920761  32.87313461
  32.70648575  32.40213013  31.97684097  31.58371925  31.38355827
  31.30943489  31.15054893  30.91512871  30.68234062  30.464571    30.2987957
  30.20953941  30.28014183  30.58289146  30.98993874  31.04504395
  30.50671387  29.96220779  29.4835453   29.13648796  28.8176899
  28.53798103  28.29074287  28.0489006   27.76320076  27.48640633
  27.20174408  26.95962906  26.75288582  26.52084732  26.31379128
  26.1986084   26.19539833  26.30187798  26.40231323  26.54922104
  26.69725227  26.75748634  26.73396873  26.63980484  26.46840096
  26.27460861  26.05214882  25.8880806   25.7836895   25.72631454
  25.76080894  25.91549492  26.22753906  26.57292366  26.91578293
  27.23150063  27.6373806   28.09119797  28.49970818  28.86953926
  29.07885933  29.07567024  29.04905891  28.89739227  28.75792885
  28.65202904  28.57711601  28.52494049  28.50577736  28.63682556
  28.58528709  28.54808235  28.54282951  28.65946007  28.93775749
  29.32556915  29.97170067  30.41195107  30.6590538   30.81711197
  30.95457268  30.99445343  31.35969353  32.32401276  33.19052505
  33.83414459  34.24436569  34.42482376  34.52334213  34.58625031
  34.60567856  34.56860733  34.51558304  34.54193878  34.54550552
  34.43599319]
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[33.9995],
         [33.4595],
         [33.4545],
         [33.5657],
         [33.7454],
         [33.9715],
         [34.3393],
         [34.5573],
         [34.4413],
         [34.2187],
         [34.0128],
         [33.8045],
         [33.5683],
         [33.2950],
         [33.0724],
         [33.0134],
         [33.0568],
         [33.1205],
         [33.1384],
         [32.9972],
         [32.7368],
         [32.4856],
         [32.3222],
         [32.2475],
         [32.2698],
         [32.5468],
         [32.8844],
         [33.1572],
         [33.3466],
         [33.5221],
         [33.6572],
         [33.7609]]], grad_fn=<ThAddBackward>)
[8,     1] loss: 0.174
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[34.3315],
         [34.6050],
         [34.7772],
         [34.8362],
         [34.8972],
         [35.0214],
         [35.0502],
         [35.0269],
         [35.0532],
         [35.4260],
         [36.1713],
         [37.0292],
         [37.7738],
         [38.2476],
         [38.4741],
         [38.3138],
         [37.8108],
         [37.4113],
         [37.0386],
         [36.5306],
         [35.8608],
         [35.1135],
         [34.2594],
         [33.5875],
         [33.0387],
         [32.6324],
         [32.3714],
         [32.2287],
         [32.1538],
         [32.1669],
         [32.3228],
         [32.6076]]], grad_fn=<ThAddBackward>)
[8,     2] loss: 0.106
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[33.5058],
         [34.1236],
         [34.5637],
         [34.7698],
         [34.8338],
         [34.8326],
         [34.8771],
         [34.9185],
         [34.7938],
         [34.5143],
         [34.4718],
         [34.7538],
         [35.1214],
         [35.4026],
         [35.6154],
         [35.6797],
         [35.6799],
         [35.6445],
         [35.7638],
         [36.0789],
         [36.6554],
         [37.3521],
         [38.0300],
         [38.6871],
         [39.0940],
         [39.1078],
         [38.8618],
         [38.5250],
         [38.1739],
         [37.8630],
         [37.5776],
         [37.3936]]], grad_fn=<ThAddBackward>)
[8,     3] loss: 0.044
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.0308],
         [38.1590],
         [38.1847],
         [38.3067],
         [38.4847],
         [38.4763],
         [38.3162],
         [38.1255],
         [37.9283],
         [37.7721],
         [37.6929],
         [37.7959],
         [38.1728],
         [38.6601],
         [38.7344],
         [38.0788],
         [37.4245],
         [36.8677],
         [36.4565],
         [36.0690],
         [35.7189],
         [35.4077],
         [35.1027],
         [34.7449],
         [34.3978],
         [34.0437],
         [33.7404],
         [33.4802],
         [33.1884],
         [32.9259],
         [32.7773],
         [32.7648]]], grad_fn=<ThAddBackward>)
[8,     4] loss: 0.022
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[33.2972],
         [33.6870],
         [34.0794],
         [34.4174],
         [34.6117],
         [34.6718],
         [34.6238],
         [34.4638],
         [34.2600],
         [34.0100],
         [33.8199],
         [33.6964],
         [33.6245],
         [33.6614],
         [33.8467],
         [34.2254],
         [34.6496],
         [35.0724],
         [35.4693],
         [35.9809],
         [36.5565],
         [37.0825],
         [37.5588],
         [37.8430],
         [37.8669],
         [37.8588],
         [37.6992],
         [37.5425],
         [37.4235],
         [37.3357],
         [37.2709],
         [37.2442]]], grad_fn=<ThAddBackward>)
[8,     5] loss: 0.023
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[37.4332],
         [37.4114],
         [37.3995],
         [37.4180],
         [37.5792],
         [37.9322],
         [38.4192],
         [39.1996],
         [39.7778],
         [40.0973],
         [40.3233],
         [40.5241],
         [40.6040],
         [41.0739],
         [42.2467],
         [43.3487],
         [44.1562],
         [44.7059],
         [44.9852],
         [45.1547],
         [45.2796],
         [45.3400],
         [45.3216],
         [45.2762],
         [45.3223],
         [45.3374],
         [45.2086]]], grad_fn=<ThAddBackward>)
[8,     6] loss: 0.042
[ 33.99952698  33.45948792  33.45452499  33.56573486  33.74542236
  33.9715004   34.3393364   34.55725098  34.44126511  34.21872711
  34.01276779  33.80451202  33.56826401  33.29501343  33.07239532
  33.0134201   33.05678558  33.12049866  33.13841248  32.99715805
  32.73680878  32.48562622  32.32216263  32.24747086  32.26981354
  32.54675293  32.88435745  33.15722656  33.3466301   33.52210236
  33.65716553  33.76094437  34.33153915  34.60503387  34.77717209
  34.83623505  34.89719772  35.02135849  35.05023956  35.02686691
  35.05323792  35.42604828  36.17127228  37.02915573  37.77377701
  38.24755859  38.47414398  38.31377411  37.81083298  37.41125107
  37.03861618  36.53056335  35.8608284   35.11352539  34.25941849
  33.58750916  33.03873062  32.63237381  32.37140274  32.22871399
  32.15381622  32.16691971  32.32284927  32.6076088   33.50575638
  34.12356186  34.56371689  34.76982117  34.83382034  34.83257675
  34.87705612  34.91849518  34.79379654  34.51432419  34.47180939
  34.75377655  35.12144089  35.40264511  35.61536789  35.67965317
  35.67985535  35.64453125  35.76382065  36.07888031  36.6554451
  37.35205078  38.03001785  38.68714523  39.09399796  39.10778427
  38.86176682  38.52501678  38.17387009  37.8629837   37.5776329
  37.39361191  38.0307579   38.15904617  38.18470383  38.30669403
  38.48469925  38.47626877  38.31619263  38.12551498  37.92829132
  37.77209854  37.69289017  37.79593658  38.17276382  38.66008377
  38.73440933  38.07882309  37.42451096  36.86772537  36.45653915
  36.06900787  35.71886826  35.40765762  35.10274887  34.74493027
  34.39777756  34.0437355   33.7404213   33.48020935  33.18838501
  32.92594528  32.7772789   32.76483917  33.29719543  33.68697357
  34.07939148  34.41736603  34.61171722  34.67182159  34.62378693
  34.46376419  34.26002121  34.01002121  33.81986237  33.69643784
  33.62454605  33.66143036  33.84667206  34.22541809  34.64955521
  35.07235718  35.46929932  35.98093414  36.55648804  37.08245087
  37.55877686  37.84303284  37.86688614  37.85876846  37.69923401
  37.54251099  37.42351532  37.33568192  37.27087402  37.2441864
  37.43317413  37.41144562  37.3995285   37.41796112  37.57921219
  37.9321785   38.41919708  39.19964218  39.77778244  40.09728241
  40.323349    40.52409744  40.60396957  41.07390213  42.24674225
  43.34870529  44.15617752  44.70592499  44.98518372  45.15466309
  45.27962494  45.33995056  45.32156372  45.27620316  45.32226181
  45.33737183  45.20859146]
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[44.2599],
         [43.4016],
         [43.2080],
         [43.2108],
         [43.3078],
         [43.4892],
         [43.8664],
         [44.0973],
         [43.9295],
         [43.6461],
         [43.3962],
         [43.1434],
         [42.8509],
         [42.5104],
         [42.2298],
         [42.1470],
         [42.1875],
         [42.2500],
         [42.2587],
         [42.0797],
         [41.7596],
         [41.4537],
         [41.2541],
         [41.1574],
         [41.1744],
         [41.4952],
         [41.8940],
         [42.2160],
         [42.4460],
         [42.6643],
         [42.8412],
         [42.9780]]], grad_fn=<ThAddBackward>)
[9,     1] loss: 0.069
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[42.8764],
         [42.7503],
         [42.6041],
         [42.4199],
         [42.3076],
         [42.3231],
         [42.2633],
         [42.1649],
         [42.1496],
         [42.5491],
         [43.3892],
         [44.3686],
         [45.2147],
         [45.7629],
         [46.0321],
         [45.8641],
         [45.3011],
         [44.8634],
         [44.4397],
         [43.8630],
         [43.1026],
         [42.2416],
         [41.2560],
         [40.4518],
         [39.7825],
         [39.2726],
         [38.9289],
         [38.7387],
         [38.6328],
         [38.6303],
         [38.7970],
         [39.1135]]], grad_fn=<ThAddBackward>)
[9,     2] loss: 0.087
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[39.0833],
         [39.1530],
         [39.1506],
         [39.0181],
         [38.8235],
         [38.6407],
         [38.5660],
         [38.5258],
         [38.3276],
         [37.9885],
         [37.9071],
         [38.1889],
         [38.5862],
         [38.8758],
         [39.0844],
         [39.1468],
         [39.1352],
         [39.1055],
         [39.2304],
         [39.5865],
         [40.2244],
         [40.9903],
         [41.7261],
         [42.4323],
         [42.8707],
         [42.8702],
         [42.6122],
         [42.2640],
         [41.8924],
         [41.5590],
         [41.2503],
         [41.0471]]], grad_fn=<ThAddBackward>)
[9,     3] loss: 0.025
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.2957],
         [39.6108],
         [39.0092],
         [38.6781],
         [38.5248],
         [38.2757],
         [37.9441],
         [37.6322],
         [37.3511],
         [37.1382],
         [37.0220],
         [37.0966],
         [37.4517],
         [37.9358],
         [37.9925],
         [37.3235],
         [36.6563],
         [36.0994],
         [35.6917],
         [35.3096],
         [34.9663],
         [34.6623],
         [34.3640],
         [34.0119],
         [33.6707],
         [33.3229],
         [33.0271],
         [32.7746],
         [32.4893],
         [32.2339],
         [32.0937],
         [32.0864]]], grad_fn=<ThAddBackward>)
[9,     4] loss: 0.017
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.6099],
         [31.4095],
         [31.3391],
         [31.3326],
         [31.2728],
         [31.1497],
         [30.9756],
         [30.7328],
         [30.4775],
         [30.2010],
         [30.0013],
         [29.8755],
         [29.8056],
         [29.8400],
         [30.0202],
         [30.3786],
         [30.7801],
         [31.1672],
         [31.5291],
         [31.9936],
         [32.5218],
         [32.9934],
         [33.4148],
         [33.6527],
         [33.6460],
         [33.6059],
         [33.4434],
         [33.2786],
         [33.1652],
         [33.0828],
         [33.0250],
         [33.0037]]], grad_fn=<ThAddBackward>)
[9,     5] loss: 0.031
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[32.5984],
         [32.2348],
         [31.9682],
         [31.8066],
         [31.8289],
         [32.0625],
         [32.4526],
         [33.1282],
         [33.5988],
         [33.8211],
         [33.9711],
         [34.1091],
         [34.1473],
         [34.5569],
         [35.6527],
         [36.6417],
         [37.3162],
         [37.7425],
         [37.9218],
         [38.0159],
         [38.0854],
         [38.1057],
         [38.0617],
         [38.0019],
         [38.0339],
         [38.0425],
         [37.9176]]], grad_fn=<ThAddBackward>)
[9,     6] loss: 0.120
[ 44.25986481  43.40156555  43.20804214  43.21084213  43.30784225
  43.48917389  43.86642838  44.0973053   43.92951584  43.6460762
  43.39619446  43.14341354  42.85089493  42.51037979  42.22980881
  42.14698792  42.18753052  42.2500267   42.25868607  42.0797348
  41.75957108  41.4537468   41.25410843  41.15738297  41.17438889
  41.49523163  41.89403152  42.21600723  42.44602203  42.66428757
  42.8412323   42.97796249  42.8764267   42.75032043  42.60414886
  42.41988754  42.30755615  42.32312775  42.26330185  42.1648941
  42.14957047  42.5491066   43.38915634  44.36863327  45.21465302
  45.76285934  46.03206253  45.86408234  45.30107117  44.86339188
  44.43970108  43.862957    43.10260773  42.24158096  41.25598526
  40.45181274  39.78246689  39.27262878  38.92894363  38.73874283
  38.63280106  38.63029099  38.79703903  39.1134758   39.0832634
  39.15301132  39.15056992  39.01811981  38.82351685  38.64073563
  38.56596756  38.52575684  38.32762527  37.98849487  37.90712357
  38.18885803  38.58618546  38.87582397  39.08439636  39.14684677
  39.13515472  39.10549545  39.23036957  39.58650589  40.22440338
  40.990345    41.72607803  42.43228149  42.87067413  42.87024689
  42.6121788   42.26398849  41.89236832  41.559021    41.25027084
  41.04708481  40.29572296  39.61078644  39.00924683  38.67809677
  38.52484894  38.27574921  37.94414902  37.63218689  37.35109711
  37.13817596  37.02199554  37.09658432  37.45165634  37.93580246
  37.99250031  37.32354736  36.65626526  36.09938049  35.69168854
  35.30959702  34.96630096  34.66225433  34.36395264  34.01186371
  33.6707077   33.32291031  33.02709961  32.77457047  32.48933792
  32.23387146  32.09371567  32.08635712  31.6099472   31.4095459
  31.33905602  31.33259392  31.27279472  31.14973259  30.9755764
  30.73284721  30.47747421  30.20100594  30.00131607  29.87548637
  29.80555344  29.84004974  30.02020454  30.37858582  30.78010559
  31.16723061  31.52905273  31.99355888  32.52177429  32.99343109
  33.41477966  33.65272141  33.64601898  33.60591888  33.44338989
  33.27856827  33.16518402  33.08281326  33.02498245  33.00374603
  32.5983963   32.23480225  31.96818352  31.80655098  31.82889366
  32.0625267   32.4525795   33.12823486  33.59877396  33.82110214
  33.97111511  34.10911179  34.14725876  34.55685043  35.65269852
  36.64171982  37.31620789  37.74246216  37.92178726  38.01590347
  38.08544922  38.10574722  38.06165695  38.00192642  38.03392029
  38.04246521  37.91757584]
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[36.8020],
         [35.9203],
         [35.7303],
         [35.7294],
         [35.8085],
         [35.9648],
         [36.3026],
         [36.4917],
         [36.3048],
         [36.0142],
         [35.7792],
         [35.5516],
         [35.2932],
         [34.9944],
         [34.7596],
         [34.7170],
         [34.7810],
         [34.8555],
         [34.8704],
         [34.7057],
         [34.4066],
         [34.1353],
         [33.9734],
         [33.9103],
         [33.9499],
         [34.2613],
         [34.6430],
         [34.9273],
         [35.1170],
         [35.2944],
         [35.4347],
         [35.5390]]], grad_fn=<ThAddBackward>)
[10,     1] loss: 0.084
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[35.6613],
         [35.6635],
         [35.6141],
         [35.5087],
         [35.4582],
         [35.5191],
         [35.4907],
         [35.4177],
         [35.4223],
         [35.8217],
         [36.6273],
         [37.5323],
         [38.2787],
         [38.7234],
         [38.9034],
         [38.6742],
         [38.0974],
         [37.6853],
         [37.3000],
         [36.7747],
         [36.0859],
         [35.3193],
         [34.4487],
         [33.7716],
         [33.2214],
         [32.8191],
         [32.5679],
         [32.4510],
         [32.3997],
         [32.4347],
         [32.6181],
         [32.9334]]], grad_fn=<ThAddBackward>)
[10,     2] loss: 0.090
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[33.4729],
         [33.8541],
         [34.0883],
         [34.1235],
         [34.0527],
         [33.9646],
         [33.9595],
         [33.9696],
         [33.8096],
         [33.5067],
         [33.4588],
         [33.7686],
         [34.1633],
         [34.4399],
         [34.6283],
         [34.6678],
         [34.6424],
         [34.6011],
         [34.7273],
         [35.0718],
         [35.6819],
         [36.4019],
         [37.0771],
         [37.7146],
         [38.0841],
         [38.0219],
         [37.7296],
         [37.3740],
         [37.0121],
         [36.6999],
         [36.4180],
         [36.2430]]], grad_fn=<ThAddBackward>)
[10,     3] loss: 0.071
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[36.4869],
         [36.3797],
         [36.2156],
         [36.2061],
         [36.2860],
         [36.2024],
         [35.9873],
         [35.7606],
         [35.5428],
         [35.3777],
         [35.2978],
         [35.4067],
         [35.7914],
         [36.2822],
         [36.3303],
         [35.6431],
         [34.9862],
         [34.4443],
         [34.0545],
         [33.6898],
         [33.3637],
         [33.0763],
         [32.7932],
         [32.4555],
         [32.1290],
         [31.7961],
         [31.5157],
         [31.2778],
         [31.0060],
         [30.7640],
         [30.6370],
         [30.6455]]], grad_fn=<ThAddBackward>)
[10,     4] loss: 0.015
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.2193],
         [31.6224],
         [32.0171],
         [32.3490],
         [32.5304],
         [32.5731],
         [32.5097],
         [32.3383],
         [32.1290],
         [31.8789],
         [31.6951],
         [31.5812],
         [31.5200],
         [31.5677],
         [31.7635],
         [32.1470],
         [32.5683],
         [32.9786],
         [33.3628],
         [33.8533],
         [34.4098],
         [34.9091],
         [35.3565],
         [35.6123],
         [35.6015],
         [35.5688],
         [35.3901],
         [35.2240],
         [35.1029],
         [35.0167],
         [34.9557],
         [34.9335]]], grad_fn=<ThAddBackward>)
[10,     5] loss: 0.010
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.5323],
         [35.7481],
         [35.9122],
         [36.0615],
         [36.3223],
         [36.7503],
         [37.2946],
         [38.1217],
         [38.7165],
         [39.0330],
         [39.2508],
         [39.4438],
         [39.5135],
         [39.9914],
         [41.1916],
         [42.2971],
         [43.0866],
         [43.6091],
         [43.8477],
         [43.9907],
         [44.0922],
         [44.1341],
         [44.0999],
         [44.0434],
         [44.0849],
         [44.0946],
         [43.9590]]], grad_fn=<ThAddBackward>)
[10,     6] loss: 0.024
[ 36.80195618  35.92031479  35.73031616  35.72944641  35.80852509
  35.9647522   36.30262756  36.49171448  36.30475998  36.01416779
  35.77923965  35.55163193  35.29315186  34.99439621  34.75960541
  34.71697998  34.78099823  34.8555336   34.87042999  34.70570374
  34.40659714  34.13526154  33.97335434  33.91033936  33.94991684
  34.26126862  34.64295959  34.92729187  35.11702728  35.29443741
  35.43473434  35.53902435  35.66130066  35.66352081  35.61411667
  35.50868607  35.45817566  35.51913071  35.49067307  35.41774368
  35.42225647  35.82167053  36.62726593  37.53226471  38.27867126
  38.72336197  38.90342712  38.67424011  38.09735489  37.68530273
  37.29997253  36.77466583  36.08591843  35.31933212  34.44867706
  33.77162933  33.22140884  32.81908798  32.56788635  32.45101547
  32.39973831  32.43471527  32.61812973  32.93339539  33.47290421
  33.85414505  34.08834839  34.12345123  34.05266571  33.96461868
  33.95953369  33.96961212  33.80960464  33.5067215   33.45881653
  33.76861572  34.16333389  34.43992615  34.62831879  34.66778564
  34.64244461  34.60113525  34.72730255  35.07177734  35.68193436
  36.40185928  37.07714462  37.7146225   38.08414078  38.02191925
  37.729599    37.37399292  37.01205063  36.69991302  36.41801453
  36.24299622  36.4868927   36.37969208  36.21564865  36.20607758
  36.28598785  36.20241928  35.98733521  35.7606163   35.54275513
  35.37768173  35.29775238  35.40665054  35.79142761  36.2821846
  36.33028793  35.64311981  34.98617172  34.44432068  34.05453873
  33.68980789  33.36371231  33.0763092   32.79315948  32.45550156
  32.12901688  31.7960968   31.51565742  31.27778244  31.00602722
  30.76396942  30.637043    30.64554977  31.21926308  31.62236214
  32.01713181  32.34896088  32.53043365  32.5730896   32.50969315
  32.33831787  32.12896729  31.8788929   31.69511414  31.58122063
  31.51998329  31.56771278  31.76345444  32.14697647  32.56832123
  32.97864914  33.36275864  33.85331726  34.40981293  34.90912247
  35.35647583  35.6122818   35.60150909  35.56881714  35.39008713
  35.22402954  35.1028862   35.01673508  34.95566559  34.9335022
  35.53230667  35.74811554  35.91221237  36.06150818  36.32230377
  36.75032043  37.29455185  38.12169647  38.7164917   39.03299713
  39.2508316   39.4437561   39.51353455  39.99144745  41.19161606
  42.29706573  43.08662796  43.60913086  43.84770203  43.9907341
  44.09216309  44.13407898  44.09986877  44.04341125  44.08490372
  44.09461975  43.95897293]
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[43.4470],
         [42.8429],
         [42.8616],
         [43.0310],
         [43.2523],
         [43.5289],
         [43.9854],
         [44.2562],
         [44.1083],
         [43.8327],
         [43.5886],
         [43.3395],
         [43.0476],
         [42.7055],
         [42.4261],
         [42.3511],
         [42.4017],
         [42.4728],
         [42.4848],
         [42.3008],
         [41.9716],
         [41.6592],
         [41.4588],
         [41.3647],
         [41.3900],
         [41.7294],
         [42.1464],
         [42.4768],
         [42.7075],
         [42.9274],
         [43.1023],
         [43.2381]]], grad_fn=<ThAddBackward>)
[11,     1] loss: 0.074
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[43.5517],
         [43.6668],
         [43.6990],
         [43.6442],
         [43.6281],
         [43.7174],
         [43.7075],
         [43.6422],
         [43.6545],
         [44.1036],
         [45.0130],
         [46.0515],
         [46.9419],
         [47.5060],
         [47.7784],
         [47.5864],
         [46.9978],
         [46.5366],
         [46.0914],
         [45.4866],
         [44.6927],
         [43.7927],
         [42.7663],
         [41.9308],
         [41.2365],
         [40.7090],
         [40.3558],
         [40.1622],
         [40.0563],
         [40.0582],
         [40.2393],
         [40.5797]]], grad_fn=<ThAddBackward>)
[11,     2] loss: 0.148
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[40.9104],
         [41.1940],
         [41.3532],
         [41.3304],
         [41.2120],
         [41.0811],
         [41.0448],
         [41.0320],
         [40.8438],
         [40.5023],
         [40.4259],
         [40.7385],
         [41.1676],
         [41.4782],
         [41.7060],
         [41.7677],
         [41.7595],
         [41.7261],
         [41.8664],
         [42.2490],
         [42.9370],
         [43.7547],
         [44.5390],
         [45.2955],
         [45.7542],
         [45.7462],
         [45.4753],
         [45.1066],
         [44.7116],
         [44.3550],
         [44.0248],
         [43.8062]]], grad_fn=<ThAddBackward>)
[11,     3] loss: 0.094
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[43.3525],
         [42.8236],
         [42.3277],
         [42.0758],
         [41.9811],
         [41.7670],
         [41.4493],
         [41.1388],
         [40.8521],
         [40.6316],
         [40.5104],
         [40.5980],
         [40.9882],
         [41.5127],
         [41.5684],
         [40.8593],
         [40.1481],
         [39.5439],
         [39.0965],
         [38.6772],
         [38.2996],
         [37.9648],
         [37.6363],
         [37.2524],
         [36.8792],
         [36.4992],
         [36.1743],
         [35.8951],
         [35.5827],
         [35.3024],
         [35.1445],
         [35.1336]]], grad_fn=<ThAddBackward>)
[11,     4] loss: 0.107
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[34.8615],
         [34.7862],
         [34.8153],
         [34.8862],
         [34.8752],
         [34.7818],
         [34.6192],
         [34.3734],
         [34.1069],
         [33.8119],
         [33.5942],
         [33.4553],
         [33.3766],
         [33.4144],
         [33.6092],
         [34.0032],
         [34.4404],
         [34.8704],
         [35.2711],
         [35.7906],
         [36.3746],
         [36.8989],
         [37.3723],
         [37.6406],
         [37.6420],
         [37.6132],
         [37.4328],
         [37.2621],
         [37.1357],
         [37.0448],
         [36.9798],
         [36.9549]]], grad_fn=<ThAddBackward>)
[11,     5] loss: 0.022
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[36.3565],
         [35.8804],
         [35.5203],
         [35.2918],
         [35.2783],
         [35.5080],
         [35.9093],
         [36.6429],
         [37.1329],
         [37.3836],
         [37.5481],
         [37.7025],
         [37.7435],
         [38.2011],
         [39.4023],
         [40.4613],
         [41.2134],
         [41.6857],
         [41.8907],
         [42.0073],
         [42.0888],
         [42.1162],
         [42.0716],
         [42.0091],
         [42.0473],
         [42.0540],
         [41.9166]]], grad_fn=<ThAddBackward>)
[11,     6] loss: 0.024
[ 43.44696426  42.84292984  42.8616066   43.03098679  43.252285
  43.52885056  43.98540497  44.25620651  44.10832596  43.83268356
  43.5885849   43.33947754  43.04763031  42.70547104  42.42613983
  42.35110092  42.4017334   42.47275162  42.48476791  42.30080032
  41.97164917  41.6591835   41.45883942  41.36473465  41.38998032
  41.72941208  42.14635849  42.4768219   42.70746231  42.92744446
  43.10232544  43.23811722  43.55166245  43.66676331  43.6989975
  43.64417267  43.62814331  43.7174263   43.70752716  43.6422348
  43.65454865  44.10355377  45.01295853  46.05145264  46.94190598
  47.50598907  47.77835846  47.58644867  46.99783325  46.53662109
  46.091362    45.48656845  44.69267654  43.79265976  42.7663269
  41.93080902  41.2364769   40.70902252  40.35576248  40.16216278
  40.05633926  40.05821609  40.23931122  40.57966614  40.91036224
  41.19396973  41.35323334  41.33043671  41.21204376  41.08109283
  41.04476929  41.03204727  40.84384918  40.50234604  40.42589569
  40.7385025   41.16756439  41.47822571  41.70599747  41.76766968
  41.75951385  41.72608948  41.86638641  42.24899292  42.93703079
  43.754673    44.53903198  45.29547882  45.75423431  45.74615479
  45.47528076  45.10663223  44.711586    44.35500336  44.02483368
  43.8061676   43.35247421  42.82364655  42.32765198  42.07584381
  41.98110962  41.76699829  41.4492569   41.13875198  40.85206604
  40.6315918   40.51043701  40.59802246  40.98817444  41.51266098
  41.56839752  40.85926056  40.14807892  39.54388809  39.09648895
  38.67717743  38.29955673  37.96478653  37.63627625  37.25244904
  36.87922668  36.49923706  36.17428589  35.89505386  35.58269501
  35.30235672  35.14452362  35.13360214  34.86154556  34.78624344
  34.81530762  34.88615417  34.87517548  34.78184509  34.61919403
  34.37338257  34.10690689  33.81190491  33.5941925   33.4552536
  33.37657547  33.41436386  33.60919571  34.00318909  34.44036102
  34.87043381  35.27108765  35.79064178  36.37463379  36.89893341
  37.37233734  37.64060974  37.64196777  37.61317444  37.43283081
  37.26208496  37.13572311  37.04481506  36.97975922  36.95494843
  36.3564949   35.88037491  35.52028275  35.29177475  35.27827835
  35.50801468  35.90927505  36.64294815  37.1329155   37.38359451
  37.54813385  37.70254898  37.74349594  38.20106125  39.4022522
  40.46129227  41.21337509  41.6857338   41.89074326  42.00728226
  42.08878326  42.11616898  42.07159042  42.00913239  42.04732513
  42.05399323  41.91662979]
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.5848],
         [39.5502],
         [39.2551],
         [39.1891],
         [39.2314],
         [39.3724],
         [39.7217],
         [39.9054],
         [39.6965],
         [39.3871],
         [39.1297],
         [38.8805],
         [38.5981],
         [38.2732],
         [38.0160],
         [37.9614],
         [38.0256],
         [38.1030],
         [38.1174],
         [37.9380],
         [37.6211],
         [37.3283],
         [37.1501],
         [37.0758],
         [37.1151],
         [37.4567],
         [37.8613],
         [38.1701],
         [38.3763],
         [38.5722],
         [38.7245],
         [38.8402]]], grad_fn=<ThAddBackward>)
[12,     1] loss: 0.009
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[38.8429],
         [38.7727],
         [38.6665],
         [38.5154],
         [38.4327],
         [38.4739],
         [38.4299],
         [38.3429],
         [38.3411],
         [38.7690],
         [39.6335],
         [40.5997],
         [41.4014],
         [41.8771],
         [42.0750],
         [41.8351],
         [41.2346],
         [40.7917],
         [40.3767],
         [39.8139],
         [39.0781],
         [38.2530],
         [37.3181],
         [36.5830],
         [35.9845],
         [35.5439],
         [35.2650],
         [35.1302],
         [35.0681],
         [35.0998],
         [35.2954],
         [35.6332]]], grad_fn=<ThAddBackward>)
[12,     2] loss: 0.031
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.0992],
         [36.4436],
         [36.6425],
         [36.6467],
         [36.5519],
         [36.4433],
         [36.4255],
         [36.4266],
         [36.2519],
         [35.9302],
         [35.8744],
         [36.1953],
         [36.6150],
         [36.9050],
         [37.1085],
         [37.1490],
         [37.1264],
         [37.0847],
         [37.2175],
         [37.5846],
         [38.2409],
         [39.0078],
         [39.7275],
         [40.4115],
         [40.7998],
         [40.7383],
         [40.4390],
         [40.0653],
         [39.6827],
         [39.3492],
         [39.0475],
         [38.8578]]], grad_fn=<ThAddBackward>)
[12,     3] loss: 0.018
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.0054],
         [38.8281],
         [38.6072],
         [38.5597],
         [38.6147],
         [38.5084],
         [38.2705],
         [38.0226],
         [37.7860],
         [37.6061],
         [37.5168],
         [37.6269],
         [38.0270],
         [38.5469],
         [38.5902],
         [37.8794],
         [37.1856],
         [36.6103],
         [36.1953],
         [35.8082],
         [35.4615],
         [35.1558],
         [34.8544],
         [34.4970],
         [34.1505],
         [33.7974],
         [33.4991],
         [33.2449],
         [32.9562],
         [32.6989],
         [32.5620],
         [32.5674]]], grad_fn=<ThAddBackward>)
[12,     4] loss: 0.021
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.7427],
         [32.9227],
         [33.1443],
         [33.3514],
         [33.4372],
         [33.4122],
         [33.2975],
         [33.0870],
         [32.8478],
         [32.5748],
         [32.3757],
         [32.2521],
         [32.1856],
         [32.2326],
         [32.4336],
         [32.8294],
         [33.2632],
         [33.6857],
         [34.0763],
         [34.5847],
         [35.1550],
         [35.6634],
         [36.1199],
         [36.3706],
         [36.3574],
         [36.3185],
         [36.1320],
         [35.9595],
         [35.8342],
         [35.7455],
         [35.6831],
         [35.6608]]], grad_fn=<ThAddBackward>)
[12,     5] loss: 0.009
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.4865],
         [35.2495],
         [35.0709],
         [34.9730],
         [35.0563],
         [35.3562],
         [35.8088],
         [36.5804],
         [37.0937],
         [37.3576],
         [37.5304],
         [37.6900],
         [37.7336],
         [38.1997],
         [39.4135],
         [40.4792],
         [41.2322],
         [41.7014],
         [41.9013],
         [42.0136],
         [42.0918],
         [42.1164],
         [42.0691],
         [42.0049],
         [42.0430],
         [42.0488],
         [41.9097]]], grad_fn=<ThAddBackward>)
[12,     6] loss: 0.024
[ 40.58480835  39.55023956  39.25510406  39.18909454  39.23135376
  39.37238693  39.72165298  39.90541077  39.69646072  39.38710785
  39.12969208  38.88051987  38.59809113  38.27317429  38.01599121
  37.96143341  38.02561569  38.10301971  38.11740494  37.93800354
  37.62111664  37.3282814   37.15011597  37.07578278  37.11512756
  37.45674515  37.86131287  38.1700592   38.376297    38.57221603
  38.72452927  38.84023666  38.84289169  38.77267075  38.66650772
  38.51540375  38.43271255  38.47393417  38.42985916  38.34286499
  38.34112167  38.76901245  39.63345718  40.59972763  41.40141296
  41.87705994  42.07497025  41.83510971  41.23456573  40.79166794
  40.37669373  39.81391525  39.07809067  38.25295639  37.31808472
  36.58298492  35.98452759  35.54393768  35.26497269  35.13017654
  35.06812286  35.09982681  35.29541779  35.63318634  36.09922028
  36.44364166  36.64251328  36.64672089  36.55191803  36.4432869
  36.42549896  36.42655182  36.25192261  35.9301796   35.87442017
  36.19532013  36.61499023  36.904953    37.10845947  37.14899826
  37.12635803  37.08470917  37.21746445  37.58459854  38.24089432
  39.00783539  39.72753906  40.41147995  40.79982376  40.73828506
  40.43904877  40.06534195  39.68270874  39.34919357  39.04751587
  38.85779572  39.00542831  38.82808304  38.60718536  38.55966568
  38.61465073  38.50841904  38.27048874  38.02256012  37.78603363
  37.60612488  37.5168457   37.62693405  38.02701569  38.54689407
  38.59019089  37.87943268  37.18564606  36.61034393  36.19533157
  35.80817032  35.461483    35.15580368  34.85437012  34.49700928
  34.15047836  33.79738998  33.49908066  33.24489594  32.95624542
  32.69893646  32.56204605  32.56738663  32.74271774  32.92274475
  33.14429092  33.35136795  33.43720245  33.41221619  33.29747772
  33.08703995  32.84775162  32.57475662  32.37572479  32.25206375
  32.18557739  32.23257446  32.43358612  32.82941055  33.26323318
  33.68565369  34.07629395  34.58465576  35.15502548  35.66339111
  36.11989975  36.37059784  36.35738754  36.31850433  36.13203049
  35.95952225  35.8341713   35.74548721  35.68307495  35.6607666
  35.48646164  35.24953842  35.0709343   34.97297287  35.05630112
  35.35616302  35.80880737  36.58044434  37.09367752  37.35757828
  37.53043365  37.6900177   37.73361969  38.19973755  39.41345596
  40.4791832   41.23217392  41.70135117  41.90133286  42.01359177
  42.09181213  42.11640167  42.06908798  42.00487518  42.04302216
  42.04879379  41.9096756 ]
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.9880],
         [40.1708],
         [40.0440],
         [40.1069],
         [40.2456],
         [40.4584],
         [40.8629],
         [41.0846],
         [40.8963],
         [40.5972],
         [40.3445],
         [40.0961],
         [39.8109],
         [39.4804],
         [39.2180],
         [39.1615],
         [39.2261],
         [39.3050],
         [39.3195],
         [39.1363],
         [38.8124],
         [38.5120],
         [38.3283],
         [38.2502],
         [38.2896],
         [38.6375],
         [39.0511],
         [39.3681],
         [39.5813],
         [39.7836],
         [39.9413],
         [40.0615]]], grad_fn=<ThAddBackward>)
[13,     1] loss: 0.009
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.0176],
         [39.9204],
         [39.7925],
         [39.6239],
         [39.5292],
         [39.5630],
         [39.5128],
         [39.4202],
         [39.4164],
         [39.8516],
         [40.7330],
         [41.7205],
         [42.5417],
         [43.0317],
         [43.2377],
         [42.9956],
         [42.3853],
         [41.9287],
         [41.5031],
         [40.9276],
         [40.1750],
         [39.3289],
         [38.3703],
         [37.6117],
         [36.9961],
         [36.5430],
         [36.2550],
         [36.1146],
         [36.0493],
         [36.0804],
         [36.2794],
         [36.6238]]], grad_fn=<ThAddBackward>)
[13,     2] loss: 0.032
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.6764],
         [36.7860],
         [36.8070],
         [36.6833],
         [36.4965],
         [36.3241],
         [36.2639],
         [36.2356],
         [36.0391],
         [35.7022],
         [35.6396],
         [35.9591],
         [36.3773],
         [36.6643],
         [36.8637],
         [36.8999],
         [36.8736],
         [36.8298],
         [36.9629],
         [37.3323],
         [37.9917],
         [38.7596],
         [39.4778],
         [40.1583],
         [40.5396],
         [40.4687],
         [40.1616],
         [39.7828],
         [39.3981],
         [39.0644],
         [38.7640],
         [38.5768]]], grad_fn=<ThAddBackward>)
[13,     3] loss: 0.020
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.3241],
         [37.9224],
         [37.5368],
         [37.3758],
         [37.3498],
         [37.1863],
         [36.9108],
         [36.6400],
         [36.3911],
         [36.2057],
         [36.1149],
         [36.2250],
         [36.6231],
         [37.1359],
         [37.1665],
         [36.4483],
         [35.7564],
         [35.1911],
         [34.7924],
         [34.4222],
         [34.0915],
         [33.8009],
         [33.5132],
         [33.1689],
         [32.8354],
         [32.4954],
         [32.2104],
         [31.9685],
         [31.6913],
         [31.4453],
         [31.3191],
         [31.3331]]], grad_fn=<ThAddBackward>)
[13,     4] loss: 0.011
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.2188],
         [31.2350],
         [31.3310],
         [31.4451],
         [31.4619],
         [31.3887],
         [31.2429],
         [31.0151],
         [30.7690],
         [30.4972],
         [30.3043],
         [30.1881],
         [30.1287],
         [30.1804],
         [30.3821],
         [30.7716],
         [31.1919],
         [31.5954],
         [31.9647],
         [32.4495],
         [32.9924],
         [33.4707],
         [33.8967],
         [34.1190],
         [34.0847],
         [34.0328],
         [33.8406],
         [33.6688],
         [33.5474],
         [33.4639],
         [33.4065],
         [33.3884]]], grad_fn=<ThAddBackward>)
[13,     5] loss: 0.024
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.2931],
         [33.1057],
         [32.9676],
         [32.9004],
         [33.0060],
         [33.3169],
         [33.7703],
         [34.5299],
         [35.0212],
         [35.2625],
         [35.4164],
         [35.5604],
         [35.5921],
         [36.0480],
         [37.2388],
         [38.2651],
         [38.9729],
         [39.3993],
         [39.5646],
         [39.6516],
         [39.7119],
         [39.7240],
         [39.6687],
         [39.6008],
         [39.6372],
         [39.6408],
         [39.5016]]], grad_fn=<ThAddBackward>)
[13,     6] loss: 0.065
[ 40.98802948  40.17077255  40.04397202  40.10686874  40.24562073
  40.45835495  40.86285782  41.08462906  40.89633942  40.59722519
  40.34449005  40.09606934  39.81089783  39.48043823  39.21802902
  39.16146851  39.22611618  39.30496216  39.31950378  39.13628387
  38.81238556  38.51202393  38.32831573  38.25018311  38.28960037
  38.63749695  39.05108643  39.36810303  39.58130264  39.78363419
  39.9413147   40.06152725  40.01763535  39.92035675  39.79245758
  39.62386703  39.52922058  39.56300735  39.51281357  39.42024994
  39.41635513  39.85158539  40.73295593  41.72049332  42.54172897
  43.03165817  43.2377243   42.99562454  42.38525772  41.92869949
  41.5031395   40.92761993  40.17503738  39.32889938  38.37031937
  37.61171341  36.99612045  36.54302216  36.25502396  36.11462784
  36.04932785  36.08044815  36.2794075   36.62382889  36.67641068
  36.78600693  36.80703354  36.68331146  36.49647903  36.32406998
  36.2638588   36.23564148  36.03908539  35.70217133  35.63959122
  35.95913315  36.37729263  36.66427231  36.86373901  36.89993286
  36.87356949  36.82984161  36.96291351  37.33232117  37.99168777
  38.75959778  39.47783661  40.1583252   40.53955841  40.46866608
  40.16157913  39.78284073  39.39806366  39.06443405  38.76397324
  38.5768013   38.32410049  37.9224205   37.53684616  37.37576675
  37.34978104  37.18625259  36.91078186  36.63999557  36.3911171
  36.20571518  36.11489105  36.22496796  36.62313461  37.1358757
  37.16652679  36.44827652  35.75635529  35.191082    34.79237366
  34.42216492  34.09149933  33.80093384  33.51318359  33.16885757
  32.83538437  32.49544144  32.21038055  31.96854401  31.69132233
  31.44530106  31.31910706  31.33309937  31.21875191  31.23504257
  31.33102036  31.44511032  31.46191406  31.38868523  31.24289703
  31.01507378  30.76902008  30.49717712  30.30425262  30.18809509
  30.12865067  30.18043518  30.38208961  30.77160454  31.19185257
  31.59544182  31.96474457  32.44946289  32.99235916  33.47067261
  33.89671326  34.11895752  34.08474731  34.03278732  33.84059525
  33.66875458  33.5474472   33.46387863  33.40651703  33.38840866
  33.29310608  33.10569382  32.96763992  32.90042496  33.00597382
  33.31686783  33.77033997  34.52988052  35.02124786  35.26245499
  35.4164238   35.56041336  35.59205627  36.04804611  37.23875046
  38.2651329   38.97288513  39.39925385  39.56455612  39.65159988
  39.71189117  39.72397995  39.66874313  39.60081863  39.63715744
  39.64079666  39.50156784]
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[38.6553],
         [37.8949],
         [37.8178],
         [37.9223],
         [38.0896],
         [38.3188],
         [38.7292],
         [38.9480],
         [38.7568],
         [38.4607],
         [38.2156],
         [37.9770],
         [37.7031],
         [37.3856],
         [37.1376],
         [37.0940],
         [37.1670],
         [37.2498],
         [37.2653],
         [37.0835],
         [36.7652],
         [36.4745],
         [36.3023],
         [36.2341],
         [36.2813],
         [36.6311],
         [37.0385],
         [37.3441],
         [37.5447],
         [37.7350],
         [37.8815],
         [37.9920]]], grad_fn=<ThAddBackward>)
[14,     1] loss: 0.019
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[38.4074],
         [38.5709],
         [38.6388],
         [38.6093],
         [38.6161],
         [38.7211],
         [38.7189],
         [38.6594],
         [38.6799],
         [39.1344],
         [40.0274],
         [41.0171],
         [41.8322],
         [42.3118],
         [42.5068],
         [42.2537],
         [41.6379],
         [41.1821],
         [40.7601],
         [40.1885],
         [39.4417],
         [38.6030],
         [37.6547],
         [36.9075],
         [36.3060],
         [35.8673],
         [35.5921],
         [35.4623],
         [35.4051],
         [35.4424],
         [35.6460],
         [35.9926]]], grad_fn=<ThAddBackward>)
[14,     2] loss: 0.030
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.8188],
         [37.3718],
         [37.7254],
         [37.8360],
         [37.8146],
         [37.7567],
         [37.7754],
         [37.8014],
         [37.6371],
         [37.3176],
         [37.2694],
         [37.6068],
         [38.0431],
         [38.3464],
         [38.5585],
         [38.6030],
         [38.5809],
         [38.5400],
         [38.6786],
         [39.0601],
         [39.7401],
         [40.5338],
         [41.2805],
         [41.9897],
         [42.3922],
         [42.3309],
         [42.0238],
         [41.6384],
         [41.2434],
         [40.8978],
         [40.5853],
         [40.3883]]], grad_fn=<ThAddBackward>)
[14,     3] loss: 0.017
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.4685],
         [40.2432],
         [39.9845],
         [39.9134],
         [39.9525],
         [39.8302],
         [39.5765],
         [39.3148],
         [39.0669],
         [38.8786],
         [38.7849],
         [38.8983],
         [39.3108],
         [39.8458],
         [39.8845],
         [39.1509],
         [38.4224],
         [37.8255],
         [37.4001],
         [37.0033],
         [36.6477],
         [36.3342],
         [36.0238],
         [35.6561],
         [35.2987],
         [34.9347],
         [34.6273],
         [34.3644],
         [34.0662],
         [33.8009],
         [33.6599],
         [33.6655]]], grad_fn=<ThAddBackward>)
[14,     4] loss: 0.045
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[33.7963],
         [33.9514],
         [34.1567],
         [34.3526],
         [34.4281],
         [34.3935],
         [34.2687],
         [34.0480],
         [33.7985],
         [33.5159],
         [33.3105],
         [33.1825],
         [33.1143],
         [33.1627],
         [33.3709],
         [33.7783],
         [34.2248],
         [34.6587],
         [35.0611],
         [35.5848],
         [36.1712],
         [36.6939],
         [37.1631],
         [37.4183],
         [37.4049],
         [37.3634],
         [37.1714],
         [36.9942],
         [36.8655],
         [36.7743],
         [36.7104],
         [36.6876]]], grad_fn=<ThAddBackward>)
[14,     5] loss: 0.017
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[36.4619],
         [36.1904],
         [35.9869],
         [35.8719],
         [35.9495],
         [36.2498],
         [36.7131],
         [37.5006],
         [38.0226],
         [38.2894],
         [38.4649],
         [38.6260],
         [38.6695],
         [39.1546],
         [40.4011],
         [41.4925],
         [42.2623],
         [42.7395],
         [42.9419],
         [43.0555],
         [43.1344],
         [43.1586],
         [43.1089],
         [43.0432],
         [43.0828],
         [43.0871],
         [42.9443]]], grad_fn=<ThAddBackward>)
[14,     6] loss: 0.021
[ 38.65527725  37.89491653  37.81781006  37.92234802  38.08959579
  38.31877518  38.72918701  38.94803619  38.75680923  38.46069336
  38.21557236  37.97700882  37.70311737  37.38558197  37.13763046
  37.0940094   37.16702652  37.24978638  37.26532745  37.08352661
  36.76515198  36.4745369   36.30233765  36.2341423   36.28126144
  36.63107681  37.03853607  37.34408951  37.54469299  37.7349968
  37.88150406  37.99199295  38.40736389  38.57090378  38.6387825
  38.60931778  38.61607742  38.72105408  38.718853    38.65937424
  38.67990494  39.13441086  40.02736664  41.0171051   41.83224106
  42.31179428  42.50682449  42.253685    41.63788223  41.18214417
  40.76010132  40.18851089  39.44174957  38.60296249  37.65473175
  36.90745163  36.30595016  35.86728668  35.59212112  35.46232986
  35.40507889  35.44237518  35.64604568  35.99257278  36.81883621
  37.37178802  37.72539902  37.83598709  37.81455612  37.75669479
  37.7754097   37.80143738  37.6370697   37.31755829  37.26939011
  37.6067543   38.04310226  38.34640503  38.55852127  38.60295105
  38.58086014  38.53998947  38.67864227  39.06007004  39.74007416
  40.53379059  41.28050995  41.98966599  42.39217377  42.33086014
  42.02382278  41.63842392  41.24340439  40.89782333  40.58527756
  40.3882637   40.46850204  40.24316406  39.984478    39.91339493
  39.9524765   39.83015442  39.57653046  39.31482315  39.06689072
  38.87857819  38.78487015  38.89829636  39.31084061  39.84579086
  39.88446045  39.15085983  38.42240524  37.82546997  37.40011978
  37.00331116  36.64765549  36.3342247   36.02383423  35.65609741
  35.29874802  34.93471146  34.62726974  34.3644104   34.06624603
  33.80090332  33.65991211  33.66547775  33.79630661  33.95138931
  34.15672684  34.35259628  34.42808914  34.39352798  34.26865768
  34.04798508  33.79847336  33.51593781  33.31047058  33.18248749
  33.11425781  33.16272736  33.3709259   33.77830505  34.224823
  34.65871048  35.06106949  35.58479309  36.17120361  36.69393539
  37.16312027  37.41826248  37.40488052  37.36342239  37.17141342
  36.99416351  36.8654747   36.77434921  36.71042252  36.68764114
  36.46186066  36.19044876  35.98685074  35.87192917  35.94948959
  36.24975586  36.71313858  37.50064468  38.02259445  38.28940201
  38.46490097  38.62604141  38.66954041  39.15459824  40.40112305
  41.49251938  42.26227188  42.7395134   42.94187546  43.05545044
  43.13444901  43.15858078  43.10889053  43.04316711  43.08278656
  43.08713913  42.94430542]
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[41.5125],
         [40.4027],
         [40.0696],
         [40.0001],
         [40.0467],
         [40.1970],
         [40.5608],
         [40.7470],
         [40.5270],
         [40.2066],
         [39.9417],
         [39.6861],
         [39.3961],
         [39.0627],
         [38.8020],
         [38.7513],
         [38.8214],
         [38.9036],
         [38.9177],
         [38.7300],
         [38.4022],
         [38.1015],
         [37.9216],
         [37.8477],
         [37.8944],
         [38.2516],
         [38.6701],
         [38.9862],
         [39.1963],
         [39.3954],
         [39.5494],
         [39.6663]]], grad_fn=<ThAddBackward>)
[15,     1] loss: 0.007
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.2119],
         [38.8802],
         [38.5809],
         [38.2938],
         [38.1197],
         [38.0994],
         [38.0113],
         [37.8928],
         [37.8754],
         [38.3070],
         [39.1821],
         [40.1520],
         [40.9436],
         [41.3993],
         [41.5733],
         [41.3018],
         [40.6768],
         [40.2213],
         [39.8032],
         [39.2363],
         [38.4970],
         [37.6679],
         [36.7336],
         [36.0042],
         [35.4212],
         [35.0001],
         [34.7407],
         [34.6236],
         [34.5757],
         [34.6201],
         [34.8290],
         [35.1774]]], grad_fn=<ThAddBackward>)
[15,     2] loss: 0.034
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[35.2755],
         [35.4042],
         [35.4362],
         [35.3187],
         [35.1376],
         [34.9729],
         [34.9214],
         [34.8991],
         [34.7036],
         [34.3707],
         [34.3199],
         [34.6514],
         [35.0716],
         [35.3531],
         [35.5429],
         [35.5694],
         [35.5356],
         [35.4888],
         [35.6229],
         [35.9945],
         [36.6530],
         [37.4108],
         [38.1120],
         [38.7704],
         [39.1233],
         [39.0252],
         [38.7008],
         [38.3151],
         [37.9320],
         [37.6050],
         [37.3141],
         [37.1384]]], grad_fn=<ThAddBackward>)
[15,     3] loss: 0.042
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[36.9316],
         [36.5548],
         [36.1935],
         [36.0566],
         [36.0482],
         [35.8942],
         [35.6263],
         [35.3639],
         [35.1246],
         [34.9493],
         [34.8688],
         [34.9898],
         [35.3966],
         [35.9092],
         [35.9218],
         [35.1866],
         [34.4854],
         [33.9313],
         [33.5520],
         [33.1994],
         [32.8852],
         [32.6101],
         [32.3351],
         [32.0023],
         [31.6804],
         [31.3522],
         [31.0799],
         [30.8495],
         [30.5823],
         [30.3469],
         [30.2323],
         [30.2568]]], grad_fn=<ThAddBackward>)
[15,     4] loss: 0.018
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[30.4997],
         [30.7155],
         [30.9590],
         [31.1747],
         [31.2603],
         [31.2328],
         [31.1158],
         [30.9068],
         [30.6730],
         [30.4099],
         [30.2253],
         [30.1160],
         [30.0630],
         [30.1212],
         [30.3313],
         [30.7290],
         [31.1547],
         [31.5601],
         [31.9306],
         [32.4186],
         [32.9635],
         [33.4413],
         [33.8653],
         [34.0793],
         [34.0367],
         [33.9778],
         [33.7793],
         [33.6046],
         [33.4828],
         [33.3997],
         [33.3434],
         [33.3267]]], grad_fn=<ThAddBackward>)
[15,     5] loss: 0.026
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.5677],
         [33.5648],
         [33.5636],
         [33.5930],
         [33.7740],
         [34.1415],
         [34.6445],
         [35.4466],
         [35.9643],
         [36.2202],
         [36.3851],
         [36.5364],
         [36.5718],
         [37.0509],
         [38.2781],
         [39.3319],
         [40.0574],
         [40.4923],
         [40.6604],
         [40.7494],
         [40.8110],
         [40.8230],
         [40.7656],
         [40.6964],
         [40.7346],
         [40.7368],
         [40.5937]]], grad_fn=<ThAddBackward>)
[15,     6] loss: 0.041
[ 41.51247406  40.40271378  40.06958771  40.00007629  40.04674149
  40.19695282  40.56078339  40.74695587  40.5270462   40.20657349
  39.94165802  39.68608856  39.39613342  39.06272888  38.80199051
  38.75128174  38.82144165  38.90364075  38.91770935  38.7300148
  38.40216827  38.10146332  37.92160416  37.84769058  37.89442444
  38.25163269  38.6701355   38.98622894  39.19626236  39.39540863
  39.54942322  39.66633606  39.21192932  38.88024139  38.58090591
  38.29376221  38.11967087  38.09939957  38.01125336  37.89277649
  37.87537766  38.30701828  39.18211365  40.15197754  40.94364166
  41.39930725  41.57328796  41.30184174  40.676754    40.2212944
  39.8032341   39.2363472   38.49702454  37.6679306   36.7336235
  36.00415802  35.42116547  35.00011444  34.74067307  34.62358093
  34.57574463  34.62010193  34.82900238  35.17744064  35.27554703
  35.40422821  35.43624496  35.31867218  35.13762283  34.97291946
  34.92136383  34.89910507  34.7036171   34.37070847  34.31987381
  34.65139389  35.07155609  35.35306549  35.54291916  35.5694046
  35.53555298  35.4887886   35.62286758  35.99450302  36.65299225
  37.41083145  38.11196518  38.77035522  39.12329483  39.02521896
  38.70075607  38.31505203  37.93199158  37.60502243  37.31410599
  37.1383934   36.93159103  36.55478287  36.19353867  36.05656815
  36.04824448  35.89423752  35.62626266  35.36394882  35.12459183
  34.94934464  34.86875916  34.9897728   35.39655304  35.90919876
  35.92181015  35.18664169  34.48535156  33.93126297  33.55200958
  33.19940948  32.88522339  32.61010742  32.3350563   32.00228119
  31.68036079  31.3521595   31.07985497  30.84950638  30.58226395
  30.34693527  30.23234177  30.25679588  30.49974632  30.71550179
  30.9590435   31.17474174  31.26031113  31.23282242  31.11579323
  30.90681076  30.67303085  30.4099369   30.22527885  30.11601448
  30.06299019  30.12121201  30.33132362  30.72902107  31.15469933
  31.56012917  31.93055344  32.41858673  32.96349716  33.44128418
  33.86531067  34.0792923   34.03670883  33.97779846  33.77931213
  33.60459137  33.48280334  33.39969254  33.34337997  33.32670212
  33.56773758  33.56479263  33.56355667  33.59302902  33.77395248
  34.14150238  34.64447021  35.4466095   35.96428299  36.22021866
  36.38513184  36.53638458  36.57178879  37.05094528  38.27809906
  39.3319397   40.05735779  40.49227905  40.66040421  40.74936676
  40.81100082  40.82300949  40.76557922  40.69638824  40.73459244
  40.73678207  40.59368515]
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.0751],
         [39.4762],
         [39.5380],
         [39.7535],
         [40.0037],
         [40.2986],
         [40.7643],
         [41.0178],
         [40.8393],
         [40.5459],
         [40.2994],
         [40.0556],
         [39.7723],
         [39.4419],
         [39.1839],
         [39.1371],
         [39.2110],
         [39.2960],
         [39.3106],
         [39.1203],
         [38.7887],
         [38.4851],
         [38.3045],
         [38.2304],
         [38.2796],
         [38.6424],
         [39.0660],
         [39.3850],
         [39.5968],
         [39.7977],
         [39.9528],
         [40.0707]]], grad_fn=<ThAddBackward>)
[16,     1] loss: 0.009
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.4147],
         [40.5335],
         [40.5657],
         [40.5080],
         [40.4971],
         [40.5915],
         [40.5798],
         [40.5114],
         [40.5304],
         [41.0022],
         [41.9285],
         [42.9566],
         [43.8041],
         [44.3056],
         [44.5117],
         [44.2498],
         [43.6154],
         [43.1434],
         [42.7038],
         [42.1068],
         [41.3290],
         [40.4523],
         [39.4645],
         [38.6832],
         [38.0531],
         [37.5920],
         [37.3013],
         [37.1617],
         [37.0986],
         [37.1350],
         [37.3456],
         [37.7043]]], grad_fn=<ThAddBackward>)
[16,     2] loss: 0.044
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[38.0962],
         [38.4006],
         [38.5634],
         [38.5339],
         [38.4098],
         [38.2804],
         [38.2522],
         [38.2451],
         [38.0519],
         [37.7106],
         [37.6556],
         [37.9975],
         [38.4376],
         [38.7417],
         [38.9514],
         [38.9915],
         [38.9648],
         [38.9219],
         [39.0642],
         [39.4555],
         [40.1505],
         [40.9572],
         [41.7141],
         [42.4300],
         [42.8294],
         [42.7562],
         [42.4364],
         [42.0402],
         [41.6378],
         [41.2872],
         [40.9716],
         [40.7747]]], grad_fn=<ThAddBackward>)
[16,     3] loss: 0.021
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.4114],
         [39.9332],
         [39.4894],
         [39.2931],
         [39.2431],
         [39.0549],
         [38.7559],
         [38.4650],
         [38.1998],
         [38.0027],
         [37.9062],
         [38.0222],
         [38.4394],
         [38.9739],
         [38.9946],
         [38.2417],
         [37.5081],
         [36.9177],
         [36.5035],
         [36.1178],
         [35.7728],
         [35.4698],
         [35.1679],
         [34.8078],
         [34.4582],
         [34.1023],
         [33.8044],
         [33.5502],
         [33.2593],
         [33.0021],
         [32.8707],
         [32.8857]]], grad_fn=<ThAddBackward>)
[16,     4] loss: 0.028
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.6999],
         [32.6764],
         [32.7468],
         [32.8434],
         [32.8443],
         [32.7568],
         [32.5962],
         [32.3537],
         [32.0933],
         [31.8082],
         [31.6068],
         [31.4851],
         [31.4237],
         [31.4781],
         [31.6907],
         [32.0971],
         [32.5353],
         [32.9547],
         [33.3402],
         [33.8467],
         [34.4121],
         [34.9101],
         [35.3534],
         [35.5806],
         [35.5452],
         [35.4891],
         [35.2887],
         [35.1103],
         [34.9847],
         [34.8979],
         [34.8387],
         [34.8202]]], grad_fn=<ThAddBackward>)
[16,     5] loss: 0.009
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[34.6526],
         [34.4175],
         [34.2450],
         [34.1551],
         [34.2542],
         [34.5668],
         [35.0368],
         [35.8197],
         [36.3227],
         [36.5671],
         [36.7244],
         [36.8704],
         [36.9017],
         [37.3862],
         [38.6251],
         [39.6872],
         [40.4171],
         [40.8532],
         [41.0206],
         [41.1090],
         [41.1702],
         [41.1816],
         [41.1229],
         [41.0529],
         [41.0918],
         [41.0933],
         [40.9486]]], grad_fn=<ThAddBackward>)
[16,     6] loss: 0.034
[ 40.07508469  39.47623444  39.53797531  39.75345993  40.00372696
  40.29864502  40.76429749  41.01775742  40.83927536  40.54592133
  40.29936218  40.05560303  39.77227402  39.44189453  39.18385315
  39.13708496  39.21102142  39.29598999  39.31061935  39.12030411
  38.78867722  38.48511887  38.30449677  38.2304306   38.27962494
  38.64239883  39.06600952  39.38497925  39.59680557  39.7976532
  39.95281219  40.0706749   40.41466141  40.533535    40.56573105
  40.50796127  40.49712753  40.59148407  40.57981873  40.51139069
  40.53037262  41.00223923  41.92852783  42.956604    43.80409622
  44.30561066  44.51168442  44.24983597  43.61541367  43.14344406
  42.7038269   42.10681534  41.3290062   40.4523201   39.46454239
  38.68324661  38.05309296  37.59196854  37.30130386  37.1617012
  37.09855652  37.13503647  37.34555435  37.70426178  38.09624481
  38.40057373  38.56336975  38.53393173  38.40977859  38.28035736
  38.25216675  38.24514389  38.05190277  37.71064758  37.6556282
  37.99747467  38.43762207  38.74168396  38.95140076  38.99152756
  38.9647522   38.92190933  39.06415176  39.45552063  40.15048599
  40.95720291  41.71405029  42.43004608  42.82937622  42.75617218
  42.43635559  42.0402298   41.6378212   41.28718567  40.97160339
  40.77474594  40.41137695  39.93324661  39.48941803  39.29314804
  39.24306107  39.05487061  38.75587463  38.46497726  38.19983292
  38.00273514  37.90615845  38.02215576  38.43939972  38.97387695
  38.99464035  38.24171448  37.50813675  36.91766357  36.50353241
  36.11783218  35.77284241  35.469841    35.1679039   34.80775833
  34.45823669  34.10234833  33.80437851  33.55019379  33.25930405
  33.0020752   32.87069321  32.88573074  32.69994354  32.67638779
  32.74675751  32.84342575  32.84430695  32.7567749   32.59615707
  32.35372925  32.09327698  31.80822563  31.60682106  31.48509598
  31.42373466  31.47814178  31.69072914  32.09711456  32.53532791
  32.95469666  33.34022522  33.84671783  34.41206741  34.91012573
  35.35344696  35.58059311  35.54520416  35.48908234  35.28871155
  35.1103363   34.98466873  34.89794922  34.8387146   34.82015991
  34.65263367  34.4174881   34.24504852  34.15510559  34.25415039
  34.5668335   35.03682327  35.81967545  36.32273102  36.5670929
  36.72438812  36.87039566  36.90171051  37.38617706  38.62507629
  39.68723297  40.41712189  40.85319519  41.02059937  41.10898972
  41.17024231  41.18159485  41.12288284  41.05292892  41.09176636
  41.09332657  40.94861221]
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[39.9879],
         [39.1514],
         [39.0411],
         [39.1342],
         [39.2953],
         [39.5265],
         [39.9465],
         [40.1636],
         [39.9579],
         [39.6480],
         [39.3931],
         [39.1459],
         [38.8620],
         [38.5334],
         [38.2796],
         [38.2382],
         [38.3159],
         [38.4025],
         [38.4169],
         [38.2257],
         [37.8950],
         [37.5951],
         [37.4199],
         [37.3508],
         [37.4047],
         [37.7704],
         [38.1926],
         [38.5066],
         [38.7125],
         [38.9078],
         [39.0576],
         [39.1709]]], grad_fn=<ThAddBackward>)
[17,     1] loss: 0.008
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.5291],
         [39.6563],
         [39.6938],
         [39.6397],
         [39.6329],
         [39.7300],
         [39.7193],
         [39.6515],
         [39.6723],
         [40.1470],
         [41.0723],
         [42.0925],
         [42.9258],
         [43.4107],
         [43.6013],
         [43.3263],
         [42.6858],
         [42.2154],
         [41.7805],
         [41.1891],
         [40.4193],
         [39.5528],
         [38.5789],
         [37.8145],
         [37.2017],
         [36.7570],
         [36.4808],
         [36.3528],
         [36.2984],
         [36.3412],
         [36.5561],
         [36.9161]]], grad_fn=<ThAddBackward>)
[17,     2] loss: 0.033
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.3258],
         [37.6384],
         [37.8045],
         [37.7762],
         [37.6532],
         [37.5259],
         [37.5006],
         [37.4955],
         [37.3027],
         [36.9632],
         [36.9132],
         [37.2599],
         [37.7003],
         [38.0011],
         [38.2058],
         [38.2409],
         [38.2103],
         [38.1657],
         [38.3079],
         [38.6993],
         [39.3924],
         [40.1928],
         [40.9398],
         [41.6436],
         [42.0284],
         [41.9418],
         [41.6135],
         [41.2142],
         [40.8129],
         [40.4660],
         [40.1555],
         [39.9646]]], grad_fn=<ThAddBackward>)
[17,     3] loss: 0.016
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.6286],
         [39.1684],
         [38.7405],
         [38.5589],
         [38.5199],
         [38.3386],
         [38.0451],
         [37.7597],
         [37.5003],
         [37.3089],
         [37.2178],
         [37.3391],
         [37.7599],
         [38.2939],
         [38.3071],
         [37.5478],
         [36.8153],
         [36.2315],
         [35.8261],
         [35.4488],
         [35.1116],
         [34.8161],
         [34.5205],
         [34.1664],
         [33.8230],
         [33.4734],
         [33.1820],
         [32.9340],
         [32.6486],
         [32.3970],
         [32.2715],
         [32.2917]]], grad_fn=<ThAddBackward>)
[17,     4] loss: 0.018
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.1285],
         [32.1191],
         [32.1995],
         [32.3023],
         [32.3062],
         [32.2205],
         [32.0612],
         [31.8206],
         [31.5626],
         [31.2808],
         [31.0837],
         [30.9661],
         [30.9085],
         [30.9660],
         [31.1812],
         [31.5884],
         [32.0248],
         [32.4400],
         [32.8205],
         [33.3221],
         [33.8813],
         [34.3718],
         [34.8070],
         [35.0249],
         [34.9820],
         [34.9208],
         [34.7174],
         [34.5386],
         [34.4139],
         [34.3287],
         [34.2711],
         [34.2541]]], grad_fn=<ThAddBackward>)
[17,     5] loss: 0.012
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[34.1024],
         [33.8778],
         [33.7144],
         [33.6317],
         [33.7376],
         [34.0544],
         [34.5271],
         [35.3091],
         [35.8060],
         [36.0425],
         [36.1936],
         [36.3344],
         [36.3616],
         [36.8471],
         [38.0857],
         [39.1394],
         [39.8565],
         [40.2788],
         [40.4343],
         [40.5142],
         [40.5696],
         [40.5767],
         [40.5150],
         [40.4436],
         [40.4825],
         [40.4831],
         [40.3376]]], grad_fn=<ThAddBackward>)
[17,     6] loss: 0.044
[ 39.98787689  39.15142059  39.04109573  39.13419724  39.29529953
  39.52652359  39.9464798   40.16355133  39.95786285  39.64800262
  39.39310837  39.14585495  38.86197281  38.5333519   38.27961349
  38.23816299  38.31586456  38.40250015  38.41688919  38.22566223
  37.89500046  37.59511185  37.41993332  37.350811    37.40470505
  37.77041245  38.1926384   38.50664902  38.71253967  38.90777588
  39.05762482  39.17087173  39.5291481   39.65625381  39.6937561
  39.63968277  39.63286209  39.72996521  39.71931076  39.65147018
  39.67230988  40.1470108   41.07233429  42.09248734  42.92579651
  43.41069031  43.60134888  43.32629013  42.68577957  42.21537781
  41.78045273  41.1890564   40.41926193  39.55284119  38.57886505
  37.81450653  37.20171738  36.75697327  36.48078156  36.35284424
  36.29835892  36.34118271  36.55606461  36.91609192  37.32575989
  37.63835526  37.80450058  37.77616119  37.65318298  37.52593994
  37.50060272  37.49554443  37.30265427  36.96319962  36.91319275
  37.25985718  37.70031738  38.00111008  38.20578003  38.24090195
  38.21034622  38.16574478  38.30790329  38.69932556  39.39242554
  40.19284058  40.93982697  41.64359665  42.02840805  41.94176483
  41.6134758   41.21416473  40.81290436  40.46598434  40.15553284
  39.96460724  39.62864685  39.16843414  38.74053192  38.55890656
  38.51988983  38.33858871  38.04506302  37.75969696  37.50028992
  37.30894852  37.21784592  37.33906174  37.75993347  38.29388428
  38.30712509  37.54779816  36.81534576  36.23154831  35.82608795
  35.44876862  35.11164474  34.8160553   34.5205307   34.16639709
  33.82299042  33.47336197  33.18204498  32.93400955  32.64860535
  32.39704895  32.27153015  32.29169846  32.12854385  32.11909103
  32.19950104  32.30226517  32.30624008  32.2205162   32.06121826
  31.82056808  31.5625782   31.28081894  31.0836792   30.96609116
  30.90849304  30.96604156  31.18123627  31.58842468  32.02480698
  32.44000626  32.82051468  33.32208633  33.88125992  34.37176895
  34.80702209  35.02490234  34.981987    34.92083359  34.71743011
  34.5385704   34.41392517  34.32873535  34.27112961  34.25405884
  34.10243988  33.87782669  33.7143631   33.6317482   33.73761368
  34.05438614  34.52708435  35.30914688  35.80600357  36.04253769
  36.19356537  36.33443451  36.36157608  36.84710312  38.0856781   39.139431
  39.85649109  40.27877808  40.43426514  40.5141716   40.56956863
  40.57672119  40.51495743  40.44363785  40.4824791   40.48314285
  40.33760452]
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[39.3857],
         [38.5639],
         [38.4701],
         [38.5751],
         [38.7435],
         [38.9792],
         [39.4011],
         [39.6156],
         [39.4055],
         [39.0939],
         [38.8398],
         [38.5945],
         [38.3129],
         [37.9871],
         [37.7375],
         [37.7009],
         [37.7823],
         [37.8708],
         [37.8852],
         [37.6928],
         [37.3620],
         [37.0641],
         [36.8926],
         [36.8269],
         [36.8846],
         [37.2534],
         [37.6757],
         [37.9868],
         [38.1888],
         [38.3805],
         [38.5269],
         [38.6371]]], grad_fn=<ThAddBackward>)
[18,     1] loss: 0.011
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.0016],
         [39.1315],
         [39.1703],
         [39.1169],
         [39.1118],
         [39.2102],
         [39.1996],
         [39.1315],
         [39.1536],
         [39.6317],
         [40.5587],
         [41.5755],
         [42.4001],
         [42.8737],
         [43.0532],
         [42.7674],
         [42.1210],
         [41.6508],
         [41.2184],
         [40.6297],
         [39.8639],
         [39.0027],
         [38.0365],
         [37.2828],
         [36.6813],
         [36.2475],
         [35.9812],
         [35.8614],
         [35.8130],
         [35.8605],
         [36.0790],
         [36.4408]]], grad_fn=<ThAddBackward>)
[18,     2] loss: 0.030
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.8602],
         [37.1750],
         [37.3403],
         [37.3099],
         [37.1856],
         [37.0584],
         [37.0344],
         [37.0302],
         [36.8365],
         [36.4973],
         [36.4510],
         [36.8025],
         [37.2441],
         [37.5431],
         [37.7441],
         [37.7754],
         [37.7417],
         [37.6958],
         [37.8384],
         [38.2312],
         [38.9249],
         [39.7226],
         [40.4640],
         [41.1601],
         [41.5344],
         [41.4370],
         [41.1013],
         [40.6986],
         [40.2974],
         [39.9526],
         [39.6454],
         [39.4587]]], grad_fn=<ThAddBackward>)
[18,     3] loss: 0.015
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.5388],
         [39.3084],
         [39.0488],
         [38.9869],
         [39.0335],
         [38.9089],
         [38.6514],
         [38.3892],
         [38.1444],
         [37.9627],
         [37.8787],
         [38.0080],
         [38.4396],
         [38.9844],
         [38.9975],
         [38.2283],
         [37.4852],
         [36.8928],
         [36.4805],
         [36.0968],
         [35.7536],
         [35.4527],
         [35.1514],
         [34.7911],
         [34.4415],
         [34.0857],
         [33.7892],
         [33.5363],
         [33.2457],
         [32.9897],
         [32.8619],
         [32.8820]]], grad_fn=<ThAddBackward>)
[18,     4] loss: 0.026
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[33.0256],
         [33.1897],
         [33.4006],
         [33.5964],
         [33.6656],
         [33.6230],
         [33.4895],
         [33.2625],
         [33.0093],
         [32.7267],
         [32.5270],
         [32.4065],
         [32.3470],
         [32.4053],
         [32.6265],
         [33.0453],
         [33.4968],
         [33.9279],
         [34.3253],
         [34.8471],
         [35.4285],
         [35.9409],
         [36.3968],
         [36.6287],
         [36.5932],
         [36.5347],
         [36.3291],
         [36.1462],
         [36.0173],
         [35.9283],
         [35.8676],
         [35.8486]]], grad_fn=<ThAddBackward>)
[18,     5] loss: 0.009
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.6283],
         [35.3596],
         [35.1625],
         [35.0564],
         [35.1501],
         [35.4637],
         [35.9444],
         [36.7433],
         [37.2554],
         [37.5027],
         [37.6626],
         [37.8105],
         [37.8419],
         [38.3447],
         [39.6163],
         [40.7042],
         [41.4506],
         [41.8947],
         [42.0646],
         [42.1544],
         [42.2167],
         [42.2279],
         [42.1671],
         [42.0958],
         [42.1362],
         [42.1366],
         [41.9883]]], grad_fn=<ThAddBackward>)
[18,     6] loss: 0.023
[ 39.38568497  38.56390381  38.47008896  38.57513809  38.74349976
  38.97915268  39.40114212  39.61558151  39.40550995  39.09386444
  38.8398056   38.59450912  38.31293869  37.98706436  37.73746109
  37.70090866  37.78226089  37.87075043  37.88515091  37.69278336
  37.36203384  37.06413269  36.89261246  36.82691574  36.884552
  37.25340652  37.67567825  37.98675919  38.18883133  38.38049316
  38.52689362  38.63714981  39.00157928  39.13150787  39.17034912
  39.11693573  39.11182404  39.21020889  39.19957733  39.13147736
  39.15359497  39.63172913  40.55870056  41.57545471  42.40006256
  42.87366104  43.05323029  42.76742554  42.1210022   41.6508255
  41.21844864  40.62966919  39.86387253  39.00268173  38.0365181
  37.28277206  36.68128204  36.24745941  35.9811821   35.86138153
  35.81297302  35.86045837  36.07904816  36.44083786  36.86024857
  37.1749649   37.34025192  37.30994034  37.18557739  37.05844498
  37.03442764  37.03023911  36.83647537  36.49725723  36.4510498
  36.8024559   37.24414825  37.54307938  37.74412918  37.775383
  37.74173355  37.69577408  37.83842087  38.23120499  38.92491531
  39.7225914   40.46396255  41.16007233  41.5344162   41.43698502
  41.10131454  40.69861221  40.29739761  39.95258713  39.64543533
  39.45871735  39.53881836  39.30841827  39.04875565  38.98688126
  39.0334549   38.90893173  38.65143585  38.38919449  38.14443207
  37.96269608  37.8787117   38.00797653  38.4396019   38.98438644
  38.99752426  38.2283287   37.48519516  36.89282227  36.48054123
  36.09677505  35.75359726  35.45267105  35.1514473   34.79110718
  34.44145584  34.0856514   33.78917313  33.53632736  33.24573135
  32.98974228  32.86187744  32.8819809   33.02563477  33.18972778
  33.40062332  33.59644318  33.66555405  33.62299347  33.48951721
  33.26247406  33.00933838  32.72674179  32.52697754  32.40645599
  32.34700775  32.40528488  32.62648392  33.04533386  33.49677658
  33.92793655  34.32528305  34.84706879  35.4284668   35.94085312
  36.39680481  36.62865829  36.5931778   36.53473282  36.32910156
  36.14616013  36.01732635  35.92828369  35.86760712  35.84858704
  35.62833786  35.35956192  35.16254044  35.05636978  35.15009689
  35.46368027  35.94439316  36.74331284  37.25535583  37.50268173
  37.66264343  37.81046677  37.84189224  38.34467316  39.61629486
  40.70419693  41.45058823  41.89471054  42.06455612  42.15441513
  42.2167244   42.22790909  42.16711807  42.09579849  42.13620758
  42.13664627  41.98828506]
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.9492],
         [40.0690],
         [39.9375],
         [40.0193],
         [40.1735],
         [40.4038],
         [40.8285],
         [41.0436],
         [40.8275],
         [40.5076],
         [40.2454],
         [39.9917],
         [39.7006],
         [39.3641],
         [39.1059],
         [39.0653],
         [39.1460],
         [39.2352],
         [39.2489],
         [39.0514],
         [38.7123],
         [38.4058],
         [38.2281],
         [38.1581],
         [38.2162],
         [38.5925],
         [39.0250],
         [39.3452],
         [39.5550],
         [39.7538],
         [39.9062],
         [40.0216]]], grad_fn=<ThAddBackward>)
[19,     1] loss: 0.008
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.9323],
         [39.8028],
         [39.6501],
         [39.4623],
         [39.3638],
         [39.3984],
         [39.3436],
         [39.2446],
         [39.2480],
         [39.7199],
         [40.6473],
         [41.6648],
         [42.4866],
         [42.9543],
         [43.1264],
         [42.8293],
         [42.1732],
         [41.6989],
         [41.2640],
         [40.6710],
         [39.9006],
         [39.0342],
         [38.0644],
         [37.3105],
         [36.7106],
         [36.2796],
         [36.0170],
         [35.9009],
         [35.8555],
         [35.9062],
         [36.1292],
         [36.4951]]], grad_fn=<ThAddBackward>)
[19,     2] loss: 0.030
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.5324],
         [36.6281],
         [36.6303],
         [36.4857],
         [36.2820],
         [36.1026],
         [36.0456],
         [36.0190],
         [35.8090],
         [35.4615],
         [35.4149],
         [35.7673],
         [36.2050],
         [36.4961],
         [36.6880],
         [36.7112],
         [36.6718],
         [36.6233],
         [36.7656],
         [37.1578],
         [37.8476],
         [38.6349],
         [39.3610],
         [40.0389],
         [40.3926],
         [40.2771],
         [39.9311],
         [39.5257],
         [39.1278],
         [38.7894],
         [38.4903],
         [38.3123]]], grad_fn=<ThAddBackward>)
[19,     3] loss: 0.022
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[38.0323],
         [37.6047],
         [37.2068],
         [37.0539],
         [37.0363],
         [36.8674],
         [36.5836],
         [36.3087],
         [36.0606],
         [35.8810],
         [35.8012],
         [35.9336],
         [36.3626],
         [36.8953],
         [36.8906],
         [36.1157],
         [35.3851],
         [34.8161],
         [34.4296],
         [34.0703],
         [33.7497],
         [33.4697],
         [33.1873],
         [32.8454],
         [32.5146],
         [32.1778],
         [31.9004],
         [31.6651],
         [31.3909],
         [31.1511],
         [31.0380],
         [31.0691]]], grad_fn=<ThAddBackward>)
[19,     4] loss: 0.011
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.2727],
         [31.4700],
         [31.7016],
         [31.9077],
         [31.9813],
         [31.9414],
         [31.8111],
         [31.5896],
         [31.3444],
         [31.0720],
         [30.8838],
         [30.7739],
         [30.7232],
         [30.7874],
         [31.0108],
         [31.4256],
         [31.8664],
         [32.2822],
         [32.6622],
         [33.1653],
         [33.7247],
         [34.2128],
         [34.6441],
         [34.8524],
         [34.7998],
         [34.7312],
         [34.5214],
         [34.3399],
         [34.2152],
         [34.1310],
         [34.0749],
         [34.0595]]], grad_fn=<ThAddBackward>)
[19,     5] loss: 0.015
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[34.2520],
         [34.2180],
         [34.1942],
         [34.2097],
         [34.3913],
         [34.7646],
         [35.2858],
         [36.1092],
         [36.6307],
         [36.8798],
         [37.0400],
         [37.1868],
         [37.2167],
         [37.7239],
         [38.9979],
         [40.0774],
         [40.8093],
         [41.2374],
         [41.3933],
         [41.4733],
         [41.5289],
         [41.5352],
         [41.4708],
         [41.3980],
         [41.4386],
         [41.4379],
         [41.2885]]], grad_fn=<ThAddBackward>)
[19,     6] loss: 0.029
[ 40.9491806   40.06904602  39.93746185  40.01932526  40.17354584
  40.40383148  40.82846069  41.04356766  40.82753372  40.5075798
  40.24538422  39.99171066  39.70063782  39.36410522  39.10585785
  39.06534958  39.14599609  39.23518372  39.24889755  39.05142212
  38.71229553  38.40577698  38.22814178  38.15814972  38.21622849
  38.59250641  39.02499771  39.34516907  39.55497742  39.75382996
  39.90623474  40.0215683   39.93232346  39.80282974  39.65010452
  39.46232224  39.36382294  39.39839554  39.34358597  39.24461746
  39.24804306  39.71987152  40.64732361  41.66479874  42.48661423
  42.95427322  43.12636566  42.82928085  42.1731987   41.69887161
  41.26398849  40.67095566  39.90062332  39.03424454  38.06442642
  37.31048965  36.71062088  36.27959061  36.01699066  35.90088272
  35.85554886  35.90624237  36.12915802  36.495121    36.53240204
  36.62810135  36.63031769  36.48572922  36.28199387  36.10264969
  36.04556656  36.01896286  35.80903244  35.46149063  35.41487885
  35.76731491  36.20495987  36.49609375  36.68800354  36.71118164
  36.67184067  36.62332916  36.76562881  37.15780258  37.84759903
  38.63488007  39.36099625  40.03889465  40.39258957  40.27711868
  39.93111038  39.52574539  39.1277504   38.78940582  38.49032593
  38.3123436   38.03233719  37.60473633  37.20682907  37.05386734
  37.03625488  36.86736298  36.58358002  36.30869675  36.06062317  35.88097
  35.80120468  35.93363953  36.3625679   36.89533615  36.89062119
  36.11568451  35.38514709  34.81610107  34.42960739  34.07025909
  33.74972916  33.46968842  33.18733597  32.8454361   32.51455688
  32.17782211  31.90038872  31.66509819  31.39088631  31.15106392
  31.03795815  31.06905365  31.27268791  31.47002792  31.70162773
  31.90773582  31.98126793  31.94144058  31.81114006  31.58958626
  31.3443985   31.07203674  30.88383484  30.77386475  30.72315979
  30.78739357  31.01080132  31.4256134   31.86641121  32.28215027
  32.66218185  33.16528702  33.72473145  34.2128067   34.64412308
  34.85237122  34.79979706  34.73115158  34.52140808  34.33987808
  34.21522903  34.1309967   34.0748558   34.05952454  34.25201416
  34.21796036  34.19422913  34.20970535  34.39130402  34.76461029
  35.28578186  36.10916901  36.63065338  36.87983704  37.03995514
  37.18677902  37.21665192  37.7239151   38.99788666  40.07735443
  40.8092804   41.23738861  41.39326859  41.47328568  41.528862
  41.53520203  41.47082138  41.39796066  41.43856049  41.43790054
  41.28845215]
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[40.6721],
         [40.0304],
         [40.0829],
         [40.2970],
         [40.5454],
         [40.8434],
         [41.3172],
         [41.5631],
         [41.3629],
         [41.0517],
         [40.7950],
         [40.5440],
         [40.2529],
         [39.9144],
         [39.6551],
         [39.6160],
         [39.6990],
         [39.7901],
         [39.8038],
         [39.6029],
         [39.2589],
         [38.9483],
         [38.7690],
         [38.6982],
         [38.7585],
         [39.1408],
         [39.5794],
         [39.9035],
         [40.1160],
         [40.3174],
         [40.4717],
         [40.5886]]], grad_fn=<ThAddBackward>)
[20,     1] loss: 0.012
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.8845],
         [40.9727],
         [40.9784],
         [40.8988],
         [40.8778],
         [40.9671],
         [40.9480],
         [40.8716],
         [40.8927],
         [41.3887],
         [42.3495],
         [43.4043],
         [44.2599],
         [44.7535],
         [44.9421],
         [44.6457],
         [43.9793],
         [43.4925],
         [43.0431],
         [42.4296],
         [41.6336],
         [40.7357],
         [39.7315],
         [38.9457],
         [38.3179],
         [37.8638],
         [37.5841],
         [37.4562],
         [37.4033],
         [37.4509],
         [37.6771],
         [38.0515]]], grad_fn=<ThAddBackward>)
[20,     2] loss: 0.050
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[38.4148],
         [38.7018],
         [38.8440],
         [38.7931],
         [38.6507],
         [38.5105],
         [38.4798],
         [38.4704],
         [38.2652],
         [37.9131],
         [37.8659],
         [38.2293],
         [38.6842],
         [38.9932],
         [39.2000],
         [39.2327],
         [39.1977],
         [39.1517],
         [39.3005],
         [39.7089],
         [40.4280],
         [41.2534],
         [42.0219],
         [42.7427],
         [43.1292],
         [43.0290],
         [42.6832],
         [42.2676],
         [41.8536],
         [41.4968],
         [41.1791],
         [40.9859]]], grad_fn=<ThAddBackward>)
[20,     3] loss: 0.024
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.5847],
         [40.0795],
         [39.6194],
         [39.4230],
         [39.3753],
         [39.1812],
         [38.8742],
         [38.5780],
         [38.3109],
         [38.1156],
         [38.0248],
         [38.1551],
         [38.5940],
         [39.1447],
         [39.1444],
         [38.3529],
         [37.5960],
         [36.9992],
         [36.5871],
         [36.2037],
         [35.8608],
         [35.5608],
         [35.2589],
         [34.8969],
         [34.5458],
         [34.1888],
         [33.8929],
         [33.6404],
         [33.3490],
         [33.0935],
         [32.9689],
         [32.9942]]], grad_fn=<ThAddBackward>)
[20,     4] loss: 0.030
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[32.7917],
         [32.7631],
         [32.8325],
         [32.9276],
         [32.9223],
         [32.8272],
         [32.6578],
         [32.4075],
         [32.1406],
         [31.8516],
         [31.6516],
         [31.5334],
         [31.4776],
         [31.5400],
         [31.7659],
         [32.1871],
         [32.6359],
         [33.0598],
         [33.4485],
         [33.9626],
         [34.5340],
         [35.0334],
         [35.4751],
         [35.6889],
         [35.6383],
         [35.5696],
         [35.3572],
         [35.1728],
         [35.0458],
         [34.9596],
         [34.9020],
         [34.8859]]], grad_fn=<ThAddBackward>)
[20,     5] loss: 0.008
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[34.6906],
         [34.4378],
         [34.2558],
         [34.1626],
         [34.2700],
         [34.5924],
         [35.0807],
         [35.8808],
         [36.3819],
         [36.6140],
         [36.7621],
         [36.9001],
         [36.9235],
         [37.4319],
         [38.7084],
         [39.7834],
         [40.5065],
         [40.9242],
         [41.0708],
         [41.1443],
         [41.1954],
         [41.1985],
         [41.1316],
         [41.0575],
         [41.0984],
         [41.0970],
         [40.9464]]], grad_fn=<ThAddBackward>)
[20,     6] loss: 0.033
[ 40.67212296  40.03035355  40.08293533  40.29698944  40.54542542
  40.84339523  41.31720734  41.56314468  41.36286926  41.05171967
  40.79500198  40.54398727  40.25288391  39.91435242  39.65506363
  39.61603546  39.6989975   39.79011154  39.80377197  39.60287857
  39.25885773  38.94831085  38.76897049  38.69816971  38.75846863
  39.14080429  39.579422    39.90352631  40.11603546  40.31742477
  40.47172165  40.58862305  40.88450623  40.9726944   40.9783783
  40.8988266   40.87783813  40.9670639   40.94800186  40.87159348
  40.89268875  41.38874054  42.34949112  43.4042511   44.25993347
  44.75353241  44.94206238  44.64574432  43.97929764  43.4925499
  43.04308319  42.4295845   41.6335907   40.73570251  39.73149872
  38.9457283   38.31787872  37.86380768  37.58407593  37.45621872
  37.40327835  37.45093536  37.67707825  38.05147934  38.41476059
  38.70183563  38.84395218  38.79306793  38.65071106  38.51052856
  38.4797554   38.4704361   38.26518631  37.91313553  37.8658638
  38.22927475  38.68418121  38.99322891  39.19996262  39.23267365
  39.19773102  39.15169525  39.30051804  39.70887756  40.42802429
  41.25338364  42.02186584  42.74268723  43.12919998  43.02902985
  42.6832428   42.26758194  41.85360718  41.49681091  41.17914581
  40.98585129  40.58465195  40.07951736  39.61940002  39.42304611
  39.37530136  39.18115997  38.87415314  38.57798386  38.31094742
  38.11555862  38.02477264  38.15507889  38.59402847  39.14474869
  39.1444397   38.35287857  37.59598923  36.99923325  36.58708191
  36.20365143  35.8608284   35.56076431  35.25889206  34.89693832
  34.5458107   34.18881226  33.89288712  33.64044952  33.34904099
  33.09347153  32.968853    32.99423599  32.79171753  32.76314545
  32.8325386   32.92755508  32.92227554  32.82724762  32.65784454
  32.40752411  32.14062881  31.85155106  31.65157509  31.53339195
  31.47763062  31.53999519  31.76593971  32.18705368  32.63592148
  33.05980301  33.44848251  33.96261597  34.53401566  35.03338242
  35.4750824   35.68889236  35.63834381  35.56955338  35.35720444
  35.17280579  35.04577637  34.95957184  34.90201187  34.88593674
  34.69062042  34.43784332  34.25583649  34.16264725  34.26996613
  34.5924263   35.08074951  35.88084412  36.38185883  36.6139946
  36.7620697   36.90013504  36.92347336  37.43189621  38.70836258
  39.78335953  40.50646973  40.92422104  41.07078552  41.14427185
  41.19542694  41.19851685  41.13155365  41.05754089  41.09844208
  41.09695816  40.94642639]
SimpleRnn: epoch num: 20
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[39.9152],
         [39.0591],
         [38.9585],
         [39.0633],
         [39.2314],
         [39.4708],
         [39.9005],
         [40.1102],
         [39.8839],
         [39.5591],
         [39.2977],
         [39.0472],
         [38.7597],
         [38.4274],
         [38.1768],
         [38.1462],
         [38.2344],
         [38.3275],
         [38.3410],
         [38.1401],
         [37.7994],
         [37.4959],
         [37.3252],
         [37.2618],
         [37.3280],
         [37.7121],
         [38.1461],
         [38.4612],
         [38.6640],
         [38.8563],
         [39.0023],
         [39.1121]]], grad_fn=<ThAddBackward>)
[21,     1] loss: 0.008
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[39.4403],
         [39.5457],
         [39.5639],
         [39.4936],
         [39.4808],
         [39.5754],
         [39.5592],
         [39.4849],
         [39.5089],
         [40.0063],
         [40.9607],
         [41.9989],
         [42.8305],
         [43.2986],
         [43.4651],
         [43.1529],
         [42.4819],
         [42.0006],
         [41.5607],
         [40.9588],
         [40.1783],
         [39.2999],
         [38.3201],
         [37.5619],
         [36.9612],
         [36.5319],
         [36.2732],
         [36.1616],
         [36.1203],
         [36.1759],
         [36.4062],
         [36.7800]]], grad_fn=<ThAddBackward>)
[21,     2] loss: 0.031
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[37.1764],
         [37.4779],
         [37.6276],
         [37.5811],
         [37.4429],
         [37.3078],
         [37.2821],
         [37.2763],
         [37.0732],
         [36.7255],
         [36.6853],
         [37.0536],
         [37.5068],
         [37.8097],
         [38.0085],
         [38.0342],
         [37.9944],
         [37.9462],
         [38.0942],
         [38.5006],
         [39.2135],
         [40.0262],
         [40.7772],
         [41.4778],
         [41.8429],
         [41.7252],
         [41.3700],
         [40.9526],
         [40.5427],
         [40.1932],
         [39.8844],
         [39.7003]]], grad_fn=<ThAddBackward>)
[21,     3] loss: 0.015
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[39.3486],
         [38.8737],
         [38.4399],
         [38.2661],
         [38.2350],
         [38.0518],
         [37.7538],
         [37.4668],
         [37.2088],
         [37.0222],
         [36.9391],
         [37.0757],
         [37.5174],
         [38.0648],
         [38.0544],
         [37.2573],
         [36.5055],
         [35.9207],
         [35.5225],
         [35.1522],
         [34.8216],
         [34.5328],
         [34.2408],
         [33.8884],
         [33.5470],
         [33.1999],
         [32.9141],
         [32.6710],
         [32.3881],
         [32.1411],
         [32.0248],
         [32.0568]]], grad_fn=<ThAddBackward>)
[21,     4] loss: 0.015
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[31.8923],
         [31.8851],
         [31.9694],
         [32.0732],
         [32.0727],
         [31.9809],
         [31.8143],
         [31.5676],
         [31.3053],
         [31.0220],
         [30.8287],
         [30.7166],
         [30.6661],
         [30.7324],
         [30.9607],
         [31.3811],
         [31.8254],
         [32.2417],
         [32.6217],
         [33.1270],
         [33.6878],
         [34.1749],
         [34.6039],
         [34.8046],
         [34.7441],
         [34.6692],
         [34.4539],
         [34.2700],
         [34.1452],
         [34.0616],
         [34.0066],
         [33.9927]]], grad_fn=<ThAddBackward>)
[21,     5] loss: 0.015
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[33.8269],
         [33.5919],
         [33.4248],
         [33.3432],
         [33.4601],
         [33.7876],
         [34.2778],
         [35.0741],
         [35.5648],
         [35.7853],
         [35.9244],
         [36.0553],
         [36.0731],
         [36.5811],
         [37.8530],
         [38.9091],
         [39.6154],
         [40.0130],
         [40.1430],
         [40.2053],
         [40.2488],
         [40.2468],
         [40.1763],
         [40.1011],
         [40.1421],
         [40.1397],
         [39.9887]]], grad_fn=<ThAddBackward>)
[21,     6] loss: 0.051
[ 39.9151535   39.05908585  38.95849991  39.0632515   39.23139572
  39.47080994  39.90049362  40.1101799   39.88394547  39.55913162
  39.29774857  39.04717255  38.75968552  38.42742538  38.17675781
  38.1461525   38.23440933  38.32753372  38.34098816  38.14011765
  37.79944992  37.49588394  37.3251915   37.26182938  37.32799149
  37.71208572  38.14609146  38.46121597  38.66395187  38.85626984
  39.00228119  39.11208725  39.44025803  39.54574966  39.56391525
  39.49362183  39.480793    39.57542801  39.55916977  39.48493195
  39.50886917  40.00630188  40.96074677  41.9989357   42.83052444
  43.2985611   43.46510696  43.15289307  42.48187637  42.00055695
  41.56066513  40.95880508  40.17834854  39.29989624  38.32005692
  37.56190491  36.96123886  36.53190231  36.27316284  36.16160202
  36.12033081  36.17587662  36.40621567  36.77999496  37.17635345
  37.47787476  37.62759399  37.5810585   37.44293976  37.30780029
  37.28208542  37.27629089  37.07317352  36.72548294  36.68533325
  37.05361176  37.50682449  37.80968475  38.0085144   38.03417206
  37.99436951  37.94615555  38.09420013  38.50055695  39.21352386
  40.02619934  40.77722931  41.47783661  41.84294891  41.72522354
  41.36997604  40.95260239  40.54272461  40.19324112  39.88441086
  39.70029449  39.34856033  38.87366104  38.43992233  38.26612854
  38.23495102  38.05176163  37.75384903  37.46676254  37.20882034
  37.02215576  36.93913651  37.07572556  37.51740265  38.06477356
  38.05444336  37.25728989  36.50546646  35.92071533  35.5225029
  35.15224838  34.82159042  34.53278351  34.24084854  33.88841248
  33.54698563  33.19988251  32.9140892   32.67098618  32.38811493
  32.14112854  32.02479172  32.05678177  31.89232445  31.88509178
  31.96935081  32.07322311  32.07273865  31.98090553  31.81427956
  31.56757164  31.30531311  31.02196884  30.82865524  30.71661758
  30.66609001  30.73236275  30.96069145  31.38105583  31.82536507
  32.24168396  32.62174225  33.12702179  33.68777084  34.17486954
  34.60390472  34.80455399  34.74407578  34.66918564  34.45391464
  34.26995468  34.14515686  34.06159592  34.00658798  33.99266052
  33.82688141  33.5919075   33.4247551   33.34320068  33.46013641
  33.78759384  34.27776718  35.07411575  35.56476593  35.78533554
  35.92441559  36.05533218  36.07314682  36.58110428  37.85297012
  38.9091301   39.61536407  40.01298141  40.1429863   40.20529556
  40.24879074  40.24675751  40.1763382   40.10107422  40.14214325
  40.1397438   39.98868942]
SimpleRnn: epoch num: 21
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[38.9807],
         [38.1515],
         [38.0766],
         [38.1985],
         [38.3767],
         [38.6216],
         [39.0527],
         [39.2582],
         [39.0267],
         [38.7006],
         [38.4415],
         [38.1947],
         [37.9114],
         [37.5841],
         [37.3398],
         [37.3159],
         [37.4087],
         [37.5039],
         [37.5171],
         [37.3152],
         [36.9754],
         [36.6755],
         [36.5104],
         [36.4519],
         [36.5227],
         [36.9097],
         [37.3421],
         [37.6520],
         [37.8486],
         [38.0354],
         [38.1764],
         [38.2818]]], grad_fn=<ThAddBackward>)
[22,     1] loss: 0.016
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[38.6250],
         [38.7371],
         [38.7595],
         [38.6922],
         [38.6830],
         [38.7802],
         [38.7646],
         [38.6906],
         [38.7165],
         [39.2173],
         [40.1713],
         [41.2016],
         [42.0188],
         [42.4700],
         [42.6213],
         [42.2960],
         [41.6194],
         [41.1407],
         [40.7062],
         [40.1101],
         [39.3375],
         [38.4690],
         [37.5026],
         [36.7610],
         [36.1773],
         [35.7636],
         [35.5185],
         [35.4177],
         [35.3841],
         [35.4452],
         [35.6793],
         [36.0539]]], grad_fn=<ThAddBackward>)
[22,     2] loss: 0.029
SimpleRnn: batch num: 2
SimpleRnn: train data for x,y are: 
tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]]) tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]])
y_pred is: 
tensor([[[36.8389],
         [37.3573],
         [37.6631],
         [37.7226],
         [37.6578],
         [37.5731],
         [37.5823],
         [37.6000],
         [37.4093],
         [37.0683],
         [37.0367],
         [37.4161],
         [37.8779],
         [38.1861],
         [38.3873],
         [38.4131],
         [38.3725],
         [38.3243],
         [38.4751],
         [38.8879],
         [39.6105],
         [40.4325],
         [41.1916],
         [41.8987],
         [42.2647],
         [42.1423],
         [41.7810],
         [41.3578],
         [40.9434],
         [40.5903],
         [40.2788],
         [40.0937]]], grad_fn=<ThAddBackward>)
[22,     3] loss: 0.016
SimpleRnn: batch num: 3
SimpleRnn: train data for x,y are: 
tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]]) tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]])
y_pred is: 
tensor([[[40.1210],
         [39.8498],
         [39.5611],
         [39.4892],
         [39.5303],
         [39.3928],
         [39.1211],
         [38.8483],
         [38.5973],
         [38.4139],
         [38.3335],
         [38.4765],
         [38.9314],
         [39.4950],
         [39.4863],
         [38.6744],
         [37.9034],
         [37.3008],
         [36.8872],
         [36.5024],
         [36.1583],
         [35.8574],
         [35.5533],
         [35.1881],
         [34.8338],
         [34.4738],
         [34.1767],
         [33.9231],
         [33.6294],
         [33.3728],
         [33.2501],
         [33.2800]]], grad_fn=<ThAddBackward>)
[22,     4] loss: 0.034
SimpleRnn: batch num: 4
SimpleRnn: train data for x,y are: 
tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]]) tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]])
y_pred is: 
tensor([[[33.3943],
         [33.5440],
         [33.7461],
         [33.9337],
         [33.9909],
         [33.9360],
         [33.7893],
         [33.5504],
         [33.2873],
         [32.9975],
         [32.7969],
         [32.6783],
         [32.6236],
         [32.6897],
         [32.9254],
         [33.3613],
         [33.8260],
         [34.2642],
         [34.6673],
         [35.2002],
         [35.7913],
         [36.3083],
         [36.7657],
         [36.9856],
         [36.9352],
         [36.8638],
         [36.6452],
         [36.4552],
         [36.3243],
         [36.2352],
         [36.1758],
         [36.1591]]], grad_fn=<ThAddBackward>)
[22,     5] loss: 0.011
SimpleRnn: batch num: 5
SimpleRnn: train data for x,y are: 
tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]]) tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]])
y_pred is: 
tensor([[[35.8966],
         [35.6006],
         [35.3877],
         [35.2743],
         [35.3749],
         [35.6983],
         [36.1996],
         [37.0193],
         [37.5316],
         [37.7677],
         [37.9192],
         [38.0598],
         [38.0835],
         [38.6140],
         [39.9176],
         [41.0203],
         [41.7791],
         [42.2074],
         [42.3573],
         [42.4335],
         [42.4862],
         [42.4896],
         [42.4203],
         [42.3448],
         [42.3877],
         [42.3849],
         [42.2300]]], grad_fn=<ThAddBackward>)
[22,     6] loss: 0.021
[ 38.98074722  38.15146255  38.07656479  38.19854355  38.37672424
  38.62156677  39.05273056  39.2582283   39.02667618  38.70063782
  38.44153976  38.19469833  37.9114418   37.58406448  37.33981323
  37.31593704  37.40874481  37.50385666  37.51708603  37.31515121
  36.97538376  36.67554855  36.51041031  36.45192337  36.52273941
  36.90973282  37.3421402   37.65197372  37.84863663  38.03544998
  38.17636108  38.28183746  38.62496567  38.73706055  38.75951767
  38.69216156  38.68301392  38.78016281  38.7645607   38.69063187
  38.71649551  39.21733475  40.17130661  41.2016449   42.01877213
  42.47001266  42.62127686  42.2959671   41.61943436  41.14071274
  40.70624161  40.11011124  39.33752823  38.46896744  37.50255966
  36.76104355  36.17725754  35.76359558  35.51851273  35.41770554
  35.38413239  35.44520187  35.67933655  36.05394363  36.83891678
  37.35725021  37.66307449  37.72256851  37.65778351  37.57307053
  37.58233643  37.60004807  37.40926361  37.0682869   37.03666306
  37.41607666  37.87794876  38.18610764  38.38726425  38.41313171
  38.37248993  38.32426834  38.47509003  38.88786697  39.61053085
  40.43250656  41.1916008   41.89869308  42.26472092  42.14234924
  41.78096008  41.3577919   40.94335556  40.59028244  40.27876663
  40.09369278  40.12096024  39.84976578  39.56109619  39.48917389
  39.5303154   39.39284134  39.12108994  38.84830093  38.59732437
  38.41392899  38.33345413  38.47651291  38.93142319  39.4949913
  39.48630905  38.67435455  37.90336227  37.30083847  36.88719559
  36.50243378  36.1582756   35.85743713  35.55334854  35.1881218
  34.83378601  34.47382736  34.17672729  33.92313004  33.62940216
  33.37276077  33.25014496  33.28001022  33.39431763  33.54403305
  33.74605179  33.9336853   33.99093246  33.93595886  33.78928375
  33.55039215  33.28725815  32.997509    32.79690552  32.67832947
  32.62356186  32.68968582  32.92542648  33.36127853  33.82604599
  34.26424026  34.66733932  35.20015335  35.79125977  36.30832291
  36.76567459  36.98558426  36.93519974  36.86375809  36.64515305
  36.45521164  36.32430649  36.23522568  36.17584229  36.15914536
  35.89662552  35.60060883  35.38767624  35.27429581  35.37485504
  35.698349    36.19963837  37.0193367   37.53160095  37.76765442
  37.91924667  38.05978394  38.08349991  38.61404037  39.91762543
  41.02027512  41.77912521  42.20740509  42.35729218  42.43346405
  42.48616409  42.4896431   42.42033386  42.34481049  42.38766861
  42.38493347  42.23004913]
SimpleRnn: epoch num: 22
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: batch num: 0
SimpleRnn: train data for x,y are: 
tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]]) tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]])
y_pred is: 
tensor([[[41.0979],
         [40.1887],
         [40.0616],
         [40.1521],
         [40.3115],
         [40.5495],
         [40.9846],
         [41.1921],
         [40.9536],
         [40.6167],
         [40.3466],
         [40.0884],
         [39.7923],
         [39.4504],
         [39.1942],
         [39.1646],
         [39.2563],
         [39.3524],
         [39.3650],
         [39.1566],
         [38.8057],
         [38.4941],
         [38.3203],
         [38.2558],
         [38.3269],
         [38.7236],
         [39.1699],
         [39.4924],
         [39.7000],
         [39.8967],
         [40.0459],
         [40.1584]]], grad_fn=<ThAddBackward>)
[23,     1] loss: 0.009
SimpleRnn: batch num: 1
SimpleRnn: train data for x,y are: 
tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]]) tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]])
y_pred is: 
tensor([[[40.0249],
         [39.8698],
         [39.6976],
         [39.4955],
         [39.3941],
         [39.4293],
         [39.3713],
         [39.2676],
         [39.2762],
         [39.7747],
         [40.7363],
         [41.7765],
         [42.5999],
         [43.0525],
         [43.2013],
         [42.8659],
         [42.1775],
         [41.6914],
         [41.2505],
         [40.6448],
         [39.8609],
         [38.9787],
         [37.9992],
         [37.2485],
         [36.6582],
         [36.2406],
         [35.9939],
         [35.8931],
         [35.8602],
         [35.9232],
         [36.1621],
         [36.5429]]], grad_fn=<ThAddBackward>)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 16:59:30
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]])
Arguments: (tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]]),)
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:88: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]])
Arguments: (tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]])
Arguments: (tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]])
Arguments: (tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]])
Arguments: (tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]])
Arguments: (tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]]),)
[-0.01687443  0.37614256  0.47597691  0.57361233  0.51884055  0.54193532
  0.52873498  0.51088184  0.51596993  0.5073728   0.4980619   0.49836063
  0.49193823  0.48785293  0.49599308  0.49548063  0.4985745   0.50484538
  0.49113002  0.48407596  0.48203933  0.47548646  0.48318416  0.47432813
  0.50717539  0.49709556  0.5100649   0.51054418  0.51679397  0.50968575
  0.51220089  0.51629323  0.90179861  0.91249168  0.90398014  0.90122592
  0.91413969  0.89882982  0.9113186   0.89713907  0.92297149  0.96137416
  0.99518508  1.03072047  1.02619147  1.02471304  0.99978292  0.95049608
  0.95918238  0.92337912  0.92442596  0.89559048  0.88213408  0.82862377
  0.83590639  0.80621994  0.81352448  0.81156838  0.81753451  0.81845957
  0.82504451  0.83402622  0.85077757  0.85976255  1.24168921  1.24337912
  1.24188089  1.23316586  1.21376836  1.21899152  1.22344303  1.22294509
  1.1960758   1.18865049  1.20434165  1.2507143   1.26576722  1.27821195
  1.25277054  1.25905907  1.23744714  1.25279748  1.26471841  1.31148088
  1.35358965  1.38455832  1.41260767  1.41330433  1.39794242  1.36159718
  1.323928    1.30848062  1.30477154  1.29146826  1.29367924  1.29296362
  1.60639441  1.60690522  1.60080957  1.62391746  1.6307112   1.61659396
  1.58783627  1.57188582  1.57110226  1.56826997  1.57857347  1.61101484
  1.6560812   1.68738544  1.60405922  1.53931332  1.45637417  1.48837614
  1.47808588  1.4747206   1.45772958  1.45158601  1.43280196  1.4202795
  1.39668012  1.38878262  1.38114524  1.37073898  1.36119628  1.35167885
  1.36077726  1.37810898  1.67534685  1.71920788  1.74485755  1.75261581
  1.74284065  1.72607327  1.70229673  1.68831944  1.66240406  1.65500796
  1.65048265  1.65400815  1.66613173  1.6765058   1.71969581  1.75205421
  1.79213619  1.79984987  1.82712328  1.86115003  1.89795756  1.92297661
  1.93160486  1.90563893  1.8958745   1.85452008  1.85702419  1.83600438
  1.84127915  1.84279537  1.84400332  1.84657836  2.14842248  2.17419171
  2.17739463  2.19338489  2.23712564  2.27425718  2.36032796  2.41529822
  2.44349885  2.39676738  2.39844298  2.39922142  2.39817476  2.50585246
  2.66063118  2.75635457  2.72798944  2.70208931  2.68177032  2.66889763
  2.6739614   2.66864324  2.65290523  2.65128946  2.6659615   2.65826964
  2.63836432]
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]])
Arguments: (tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]])
Arguments: (tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]])
Arguments: (tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]])
Arguments: (tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]])
Arguments: (tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]])
Arguments: (tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]]),)
[ 2.78014851  2.76235819  2.90386915  3.0085876   3.01724195  3.05562735
  3.10398054  3.09208965  3.00483418  2.94075322  2.93684506  2.92986655
  2.90029836  2.86595631  2.86148643  2.89446974  2.92933106  2.93978119
  2.91912794  2.86741805  2.81216884  2.79142189  2.8090477   2.82214785
  2.86044264  2.92387867  2.98640919  2.98623776  2.98092031  2.98672056
  2.99561691  2.99829292  3.37649226  3.42591214  3.4216907   3.40729046
  3.415694    3.43909931  3.43406558  3.40231037  3.42852306  3.55269885
  3.7258141   3.85278273  3.88516545  3.8540349   3.79159999  3.65889478
  3.54182148  3.52594256  3.5118103   3.42224622  3.31912494  3.20855594
  3.11807704  3.0863874   3.07968426  3.07775664  3.09090352  3.10844374
  3.12264752  3.14313745  3.19492006  3.25659204  3.6776824   3.77150607
  3.78340054  3.75148344  3.71599722  3.70558906  3.72490764  3.73155236
  3.66716266  3.60539961  3.6597352   3.77778387  3.84885907  3.85965014
  3.84840631  3.82089472  3.79413843  3.7943697   3.84477806  3.94554567
  4.0713377   4.18021965  4.25375271  4.30620909  4.2743144   4.16238594
  4.06798029  4.0109396   3.97605848  3.95301604  3.93562841  3.94103098
  4.35825968  4.3760047   4.36938667  4.41809177  4.44761848  4.40552998
  4.34725761  4.31125021  4.2880764   4.28045416  4.29596138  4.36263704
  4.48006582  4.57022858  4.43351841  4.18898773  4.06628227  4.05591345
  4.04290295  4.0197053   3.98754191  3.96420217  3.92502594  3.87560368
  3.83209038  3.79303288  3.77132559  3.74993634  3.71225357  3.69005466
  3.7042079   3.74048829  4.15727997  4.29010963  4.3641057   4.40144062
  4.38560629  4.35423803  4.30520201  4.25515842  4.20611715  4.16726494
  4.15765285  4.16228151  4.17835283  4.21320152  4.29159689  4.38483
  4.47140455  4.51711607  4.56944132  4.66057587  4.75174761  4.80975723
  4.85267973  4.81916618  4.76242065  4.71526241  4.66695738  4.63790751
  4.636168    4.63601828  4.63957596  4.64736795  5.06251574  5.13307381
  5.14632893  5.16882181  5.25109768  5.34156847  5.51100588  5.66366482
  5.73302507  5.67789602  5.66940498  5.6629014   5.65216112  5.85457182
  6.185606    6.41342306  6.43864965  6.39949369  6.33677912  6.31164408
  6.30779409  6.29652929  6.26828146  6.25724745  6.28649187  6.27383566
  6.22879791]
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]])
Arguments: (tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]])
Arguments: (tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]])
Arguments: (tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]])
Arguments: (tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]])
Arguments: (tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]])
Arguments: (tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]]),)
[  6.29190922   6.28777599   6.48440933   6.67242861   6.74652529
   6.82735205   6.93247271   6.93014002   6.76531315   6.64525032
   6.6082468    6.57869148   6.52008152   6.4495306    6.42758989
   6.48020887   6.54423761   6.57602978   6.55068302   6.45650244
   6.34524632   6.28775024   6.30119133   6.32187939   6.39444828
   6.52661324   6.64963675   6.68295527   6.68092918   6.69276524
   6.70805168   6.71630335   7.22717285   7.35378027   7.36726046
   7.34661579   7.36499786   7.40404558   7.39926291   7.35154676
   7.38761806   7.61215687   7.94835901   8.22151852   8.32544708
   8.29918766   8.20375919   7.973773     7.7268796    7.63730669
   7.60420322   7.44319963   7.234272     7.01022816   6.81183147
   6.70334959   6.67055082   6.64653826   6.66237736   6.69336653
   6.72137642   6.76507902   6.8634057    6.98342037   7.59112549
   7.79812574   7.83842516   7.78566122   7.71481037   7.68831921
   7.70974588   7.7301836    7.63105345   7.50179195   7.55986309
   7.77348232   7.94255066   7.98900223   7.97005129   7.9256649
   7.8711071    7.86573124   7.93796968   8.12169933   8.37876701
   8.60945988   8.77373314   8.89054775   8.86749649   8.67150688
   8.47097111   8.34425735   8.2592268    8.2005167    8.16157627
   8.15961456   8.72666073   8.79912949   8.76204395   8.83286858
   8.90520382   8.85125637   8.7326231    8.6483202    8.60103798
   8.57950211   8.60188198   8.70929432   8.9167738    9.11707401
   8.94368553   8.50507927   8.21877003   8.14462948   8.10999393
   8.06615162   8.0000782    7.9516573    7.88169384   7.78829813
   7.69609928   7.61833811   7.56673145   7.52508545   7.45706892
   7.40457392   7.41920996   7.48276234   8.0922184    8.35626984
   8.5001812    8.58320618   8.57497406   8.51792526   8.43465614
   8.34041882   8.24553776   8.16719055   8.13477707   8.13855171
   8.16086102   8.21817589   8.34909248   8.52699375   8.69682026
   8.80241776   8.9024601    9.06017685   9.2421608    9.37010765
   9.45729446   9.42575455   9.32414436   9.23477364   9.14957428
   9.07933617   9.06818962   9.06418133   9.06627846   9.07794285
   9.68286991   9.83383465   9.87735653   9.9352293   10.07238865
  10.25126839  10.51956081  10.82927227  10.97567558  10.91860294
  10.90538597  10.906003    10.88948441  11.18462181  11.78201675
  12.21455669  12.32198429  12.2912178   12.21234035  12.16306496
  12.15280533  12.13107014  12.07948208  12.05393314  12.09354115
  12.08168793  12.00874329]
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.2850, 35.3400],
        [35.3400, 40.3000],
        [40.3000, 40.0900],
        [40.0900, 40.4000],
        [40.4000, 40.1000],
        [40.1000, 41.8200],
        [41.8200, 41.9100],
        [41.9100, 39.2500],
        [39.2500, 38.7500],
        [38.7500, 39.1500],
        [39.1500, 38.9400],
        [38.9400, 38.5650],
        [38.5650, 37.9700],
        [37.9700, 37.6300],
        [37.6300, 38.5900],
        [38.5900, 39.1600],
        [39.1600, 39.0300],
        [39.0300, 39.0800],
        [39.0800, 38.0700],
        [38.0700, 37.0100],
        [37.0100, 37.0100],
        [37.0100, 37.1600],
        [37.1600, 37.8700],
        [37.8700, 37.2900],
        [37.2900, 39.5000],
        [39.5000, 40.0000],
        [40.0000, 39.8000],
        [39.8000, 39.3500],
        [39.3500, 39.7890],
        [39.7890, 39.8400],
        [39.8400, 39.7300],
        [39.7300, 39.9700]])
Arguments: (tensor([[[40.3000],
         [40.0900],
         [40.4000],
         [40.1000],
         [41.8200],
         [41.9100],
         [39.2500],
         [38.7500],
         [39.1500],
         [38.9400],
         [38.5650],
         [37.9700],
         [37.6300],
         [38.5900],
         [39.1600],
         [39.0300],
         [39.0800],
         [38.0700],
         [37.0100],
         [37.0100],
         [37.1600],
         [37.8700],
         [37.2900],
         [39.5000],
         [40.0000],
         [39.8000],
         [39.3500],
         [39.7890],
         [39.8400],
         [39.7300],
         [39.9700],
         [39.3700]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.9700, 39.3700],
        [39.3700, 39.3100],
        [39.3100, 38.9800],
        [38.9800, 38.7100],
        [38.7100, 40.0800],
        [40.0800, 39.1300],
        [39.1300, 39.0200],
        [39.0200, 38.6900],
        [38.6900, 40.6500],
        [40.6500, 43.7400],
        [43.7400, 44.7400],
        [44.7400, 45.0800],
        [45.0800, 43.9800],
        [43.9800, 43.5900],
        [43.5900, 42.5600],
        [42.5600, 39.2000],
        [39.2000, 40.6500],
        [40.6500, 40.5100],
        [40.5100, 39.5000],
        [39.5000, 37.5300],
        [37.5300, 37.2700],
        [37.2700, 34.8200],
        [34.8200, 35.5300],
        [35.5300, 35.2100],
        [35.2100, 35.2100],
        [35.2100, 35.3500],
        [35.3500, 35.7600],
        [35.7600, 35.7500],
        [35.7500, 35.9100],
        [35.9100, 36.5900],
        [36.5900, 37.7000],
        [37.7000, 37.8500]])
Arguments: (tensor([[[39.3100],
         [38.9800],
         [38.7100],
         [40.0800],
         [39.1300],
         [39.0200],
         [38.6900],
         [40.6500],
         [43.7400],
         [44.7400],
         [45.0800],
         [43.9800],
         [43.5900],
         [42.5600],
         [39.2000],
         [40.6500],
         [40.5100],
         [39.5000],
         [37.5300],
         [37.2700],
         [34.8200],
         [35.5300],
         [35.2100],
         [35.2100],
         [35.3500],
         [35.7600],
         [35.7500],
         [35.9100],
         [36.5900],
         [37.7000],
         [37.8500],
         [37.8800]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[37.8500, 37.8800],
        [37.8800, 37.8100],
        [37.8100, 36.8600],
        [36.8600, 36.6300],
        [36.6300, 36.3400],
        [36.3400, 36.8700],
        [36.8700, 37.1800],
        [37.1800, 36.5200],
        [36.5200, 34.7300],
        [34.7300, 35.8000],
        [35.8000, 37.9300],
        [37.9300, 39.0000],
        [39.0000, 37.9200],
        [37.9200, 38.4200],
        [38.4200, 37.3900],
        [37.3900, 37.6000],
        [37.6000, 37.0400],
        [37.0400, 38.0600],
        [38.0600, 39.0000],
        [39.0000, 40.8100],
        [40.8100, 42.2100],
        [42.2100, 42.0800],
        [42.0800, 43.1400],
        [43.1400, 42.8700],
        [42.8700, 40.6900],
        [40.6900, 39.7000],
        [39.7000, 39.4900],
        [39.4900, 38.9700],
        [38.9700, 39.1300],
        [39.1300, 38.6400],
        [38.6400, 38.8600],
        [38.8600, 39.2400]])
Arguments: (tensor([[[37.8100],
         [36.8600],
         [36.6300],
         [36.3400],
         [36.8700],
         [37.1800],
         [36.5200],
         [34.7300],
         [35.8000],
         [37.9300],
         [39.0000],
         [37.9200],
         [38.4200],
         [37.3900],
         [37.6000],
         [37.0400],
         [38.0600],
         [39.0000],
         [40.8100],
         [42.2100],
         [42.0800],
         [43.1400],
         [42.8700],
         [40.6900],
         [39.7000],
         [39.4900],
         [38.9700],
         [39.1300],
         [38.6400],
         [38.8600],
         [39.2400],
         [37.6900]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[39.2400, 37.6900],
        [37.6900, 37.0900],
        [37.0900, 37.8900],
        [37.8900, 38.9100],
        [38.9100, 37.8600],
        [37.8600, 36.8500],
        [36.8500, 36.8700],
        [36.8700, 36.6000],
        [36.6000, 36.6700],
        [36.6700, 36.7800],
        [36.7800, 37.2900],
        [37.2900, 38.9000],
        [38.9000, 40.1000],
        [40.1000, 39.6800],
        [39.6800, 33.6100],
        [33.6100, 34.4700],
        [34.4700, 34.3400],
        [34.3400, 34.9700],
        [34.9700, 34.3800],
        [34.3800, 34.0900],
        [34.0900, 33.8500],
        [33.8500, 33.8200],
        [33.8200, 32.8700],
        [32.8700, 32.8200],
        [32.8200, 32.3000],
        [32.3000, 32.1400],
        [32.1400, 32.3300],
        [32.3300, 31.6500],
        [31.6500, 31.3800],
        [31.3800, 31.6900],
        [31.6900, 32.2200],
        [32.2200, 32.6400]])
Arguments: (tensor([[[37.0900],
         [37.8900],
         [38.9100],
         [37.8600],
         [36.8500],
         [36.8700],
         [36.6000],
         [36.6700],
         [36.7800],
         [37.2900],
         [38.9000],
         [40.1000],
         [39.6800],
         [33.6100],
         [34.4700],
         [34.3400],
         [34.9700],
         [34.3800],
         [34.0900],
         [33.8500],
         [33.8200],
         [32.8700],
         [32.8200],
         [32.3000],
         [32.1400],
         [32.3300],
         [31.6500],
         [31.3800],
         [31.6900],
         [32.2200],
         [32.6400],
         [32.7600]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[32.6400, 32.7600],
        [32.7600, 33.0900],
        [33.0900, 33.4500],
        [33.4500, 33.0000],
        [33.0000, 32.4900],
        [32.4900, 32.3900],
        [32.3900, 31.5800],
        [31.5800, 31.6100],
        [31.6100, 30.9600],
        [30.9600, 31.0900],
        [31.0900, 31.4200],
        [31.4200, 31.2000],
        [31.2000, 31.8000],
        [31.8000, 32.0900],
        [32.0900, 33.6500],
        [33.6500, 33.8200],
        [33.8200, 34.3600],
        [34.3600, 34.1500],
        [34.1500, 35.2400],
        [35.2400, 36.3600],
        [36.3600, 36.3100],
        [36.3100, 36.6800],
        [36.6800, 36.8500],
        [36.8500, 34.7500],
        [34.7500, 35.8900],
        [35.8900, 34.6000],
        [34.6000, 34.6800],
        [34.6800, 34.8600],
        [34.8600, 34.8700],
        [34.8700, 34.8200],
        [34.8200, 34.9600],
        [34.9600, 35.0200]])
Arguments: (tensor([[[33.0900],
         [33.4500],
         [33.0000],
         [32.4900],
         [32.3900],
         [31.5800],
         [31.6100],
         [30.9600],
         [31.0900],
         [31.4200],
         [31.2000],
         [31.8000],
         [32.0900],
         [33.6500],
         [33.8200],
         [34.3600],
         [34.1500],
         [35.2400],
         [36.3600],
         [36.3100],
         [36.6800],
         [36.8500],
         [34.7500],
         [35.8900],
         [34.6000],
         [34.6800],
         [34.8600],
         [34.8700],
         [34.8200],
         [34.9600],
         [35.0200],
         [34.0000]]]),)
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 994, in emit
    msg = self.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 840, in format
    return fmt.format(record)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 577, in format
    record.message = record.getMessage()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\logging\__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 403, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 224, in TrainSimpleRnn
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 80, in Train
    logging.debug(xs, ys)
Message: tensor([[35.0200, 34.0000],
        [34.0000, 34.1900],
        [34.1900, 34.4600],
        [34.4600, 34.5400],
        [34.5400, 36.6700],
        [36.6700, 35.7700],
        [35.7700, 39.6800],
        [39.6800, 38.2900],
        [38.2900, 37.4600],
        [37.4600, 37.2800],
        [37.2800, 38.1100],
        [38.1100, 37.2800],
        [37.2800, 37.5700],
        [37.5700, 44.4000],
        [44.4000, 43.6900],
        [43.6900, 42.7300],
        [42.7300, 42.7500],
        [42.7500, 41.6500],
        [41.6500, 41.8300],
        [41.8300, 42.0500],
        [42.0500, 41.9100],
        [41.9100, 41.6900],
        [41.6900, 41.2700],
        [41.2700, 41.9200],
        [41.9200, 42.3100],
        [42.3100, 40.7900],
        [40.7900, 41.3100]])
Arguments: (tensor([[[34.1900],
         [34.4600],
         [34.5400],
         [36.6700],
         [35.7700],
         [39.6800],
         [38.2900],
         [37.4600],
         [37.2800],
         [38.1100],
         [37.2800],
         [37.5700],
         [44.4000],
         [43.6900],
         [42.7300],
         [42.7500],
         [41.6500],
         [41.8300],
         [42.0500],
         [41.9100],
         [41.6900],
         [41.2700],
         [41.9200],
         [42.3100],
         [40.7900],
         [41.3100],
         [40.7400]]]),)
[ 12.11593628  12.08925629  12.37250042  12.67401791  12.82422256
  12.98385239  13.18986511  13.21037865  12.99668884  12.78705406
  12.68916607  12.6123867   12.50409603  12.3773632   12.32349968
  12.3884716   12.48586464  12.54619884  12.52109241  12.37924862
  12.19641113  12.07949829  12.07110977  12.0893898   12.19939995
  12.41845131  12.64081955  12.72327328  12.7508316   12.78514099
  12.81459618  12.83388996  13.6867981   13.97916603  14.05342007
  14.05258083  14.093153    14.15552902  14.1548214   14.09093761
  14.14417267  14.45885563  14.99700546  15.50706768  15.78725338
  15.83063412  15.7244606   15.3805418   14.98043346  14.75370789
  14.62142849  14.34740925  14.00339317  13.59479809  13.22048759
  12.97041988  12.84431744  12.77356911  12.77399731  12.81218529
  12.85571575  12.93052197  13.08901215  13.29398918  14.3199482
  14.78237152  14.94487381  14.93549824  14.85295677  14.8026228
  14.82240295  14.84308052  14.70383835  14.51739216  14.54927349
  14.84548378  15.13074303  15.26926231  15.28659344  15.24155045
  15.16016865  15.13387775  15.23367214  15.50696564  15.9220953
  16.34508324  16.68723869  16.9436512   16.97600555  16.73623085
  16.42681503  16.16902924  15.97920132  15.83742428  15.74081707
  15.70754719  16.63635826  16.86842155  16.89038277  17.01708984
  17.14503288  17.102808    16.94439888  16.79914474  16.6911602
  16.62692451  16.63397789  16.78072548  17.10098839  17.4470005
  17.30212212  16.70554543  16.16756248  15.93346977  15.78935146
  15.66709518  15.5392189   15.43431568  15.30645847  15.14127731
  14.97211838  14.81658936  14.69985771  14.60540009  14.48136234
  14.37538815  14.36403275  14.44257164  15.44568062  15.9761095
  16.32404137  16.55694199  16.63076591  16.59559822  16.48897362
  16.33884621  16.17243767  16.0182972   15.92728329  15.8981657
  15.91063213  15.98631668  16.17923546  16.46848297  16.77004242
  17.00116539  17.21184349  17.49574089  17.82184601  18.08670616
  18.29052925  18.31948662  18.20672798  18.07240677  17.92538261
  17.78722954  17.72870827  17.6978054   17.68478203  17.69452286
  18.71829796  19.10887146  19.29742813  19.46105766  19.70282936
  20.01304436  20.46428871  21.00511742  21.36420631  21.41500664
  21.44615173  21.48603821  21.47664261  21.85891151  22.76395226
  23.5678463   23.94266319  24.06907272  24.04854393  23.99835396
  23.98558044  23.95356178  23.87541008  23.81601906  23.84931755
  23.84674072  23.73948097]
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 17:02:06
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:89: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 20
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 21
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 17:16:36
INFO - ******************MLNX*******************
[[ 35.285  35.34 ]
 [ 35.34   40.3  ]
 [ 40.3    40.09 ]
 [ 40.09   40.4  ]
 [ 40.4    40.1  ]
 [ 40.1    41.82 ]
 [ 41.82   41.91 ]
 [ 41.91   39.25 ]
 [ 39.25   38.75 ]
 [ 38.75   39.15 ]
 [ 39.15   38.94 ]
 [ 38.94   38.565]
 [ 38.565  37.97 ]
 [ 37.97   37.63 ]
 [ 37.63   38.59 ]
 [ 38.59   39.16 ]
 [ 39.16   39.03 ]
 [ 39.03   39.08 ]
 [ 39.08   38.07 ]
 [ 38.07   37.01 ]
 [ 37.01   37.01 ]
 [ 37.01   37.16 ]
 [ 37.16   37.87 ]
 [ 37.87   37.29 ]
 [ 37.29   39.5  ]
 [ 39.5    40.   ]
 [ 40.     39.8  ]
 [ 39.8    39.35 ]
 [ 39.35   39.789]
 [ 39.789  39.84 ]
 [ 39.84   39.73 ]
 [ 39.73   39.97 ]
 [ 39.97   39.37 ]
 [ 39.37   39.31 ]
 [ 39.31   38.98 ]
 [ 38.98   38.71 ]
 [ 38.71   40.08 ]
 [ 40.08   39.13 ]
 [ 39.13   39.02 ]
 [ 39.02   38.69 ]
 [ 38.69   40.65 ]
 [ 40.65   43.74 ]
 [ 43.74   44.74 ]
 [ 44.74   45.08 ]
 [ 45.08   43.98 ]
 [ 43.98   43.59 ]
 [ 43.59   42.56 ]
 [ 42.56   39.2  ]
 [ 39.2    40.65 ]
 [ 40.65   40.51 ]
 [ 40.51   39.5  ]
 [ 39.5    37.53 ]
 [ 37.53   37.27 ]
 [ 37.27   34.82 ]
 [ 34.82   35.53 ]
 [ 35.53   35.21 ]
 [ 35.21   35.21 ]
 [ 35.21   35.35 ]
 [ 35.35   35.76 ]
 [ 35.76   35.75 ]
 [ 35.75   35.91 ]
 [ 35.91   36.59 ]
 [ 36.59   37.7  ]
 [ 37.7    37.85 ]
 [ 37.85   37.88 ]
 [ 37.88   37.81 ]
 [ 37.81   36.86 ]
 [ 36.86   36.63 ]
 [ 36.63   36.34 ]
 [ 36.34   36.87 ]
 [ 36.87   37.18 ]
 [ 37.18   36.52 ]
 [ 36.52   34.73 ]
 [ 34.73   35.8  ]
 [ 35.8    37.93 ]
 [ 37.93   39.   ]
 [ 39.     37.92 ]
 [ 37.92   38.42 ]
 [ 38.42   37.39 ]
 [ 37.39   37.6  ]
 [ 37.6    37.04 ]
 [ 37.04   38.06 ]
 [ 38.06   39.   ]
 [ 39.     40.81 ]
 [ 40.81   42.21 ]
 [ 42.21   42.08 ]
 [ 42.08   43.14 ]
 [ 43.14   42.87 ]
 [ 42.87   40.69 ]
 [ 40.69   39.7  ]
 [ 39.7    39.49 ]
 [ 39.49   38.97 ]
 [ 38.97   39.13 ]
 [ 39.13   38.64 ]
 [ 38.64   38.86 ]
 [ 38.86   39.24 ]
 [ 39.24   37.69 ]
 [ 37.69   37.09 ]
 [ 37.09   37.89 ]
 [ 37.89   38.91 ]
 [ 38.91   37.86 ]
 [ 37.86   36.85 ]
 [ 36.85   36.87 ]
 [ 36.87   36.6  ]
 [ 36.6    36.67 ]
 [ 36.67   36.78 ]
 [ 36.78   37.29 ]
 [ 37.29   38.9  ]
 [ 38.9    40.1  ]
 [ 40.1    39.68 ]
 [ 39.68   33.61 ]
 [ 33.61   34.47 ]
 [ 34.47   34.34 ]
 [ 34.34   34.97 ]
 [ 34.97   34.38 ]
 [ 34.38   34.09 ]
 [ 34.09   33.85 ]
 [ 33.85   33.82 ]
 [ 33.82   32.87 ]
 [ 32.87   32.82 ]
 [ 32.82   32.3  ]
 [ 32.3    32.14 ]
 [ 32.14   32.33 ]
 [ 32.33   31.65 ]
 [ 31.65   31.38 ]
 [ 31.38   31.69 ]
 [ 31.69   32.22 ]
 [ 32.22   32.64 ]
 [ 32.64   32.76 ]
 [ 32.76   33.09 ]
 [ 33.09   33.45 ]
 [ 33.45   33.   ]
 [ 33.     32.49 ]
 [ 32.49   32.39 ]
 [ 32.39   31.58 ]
 [ 31.58   31.61 ]
 [ 31.61   30.96 ]
 [ 30.96   31.09 ]
 [ 31.09   31.42 ]
 [ 31.42   31.2  ]
 [ 31.2    31.8  ]
 [ 31.8    32.09 ]
 [ 32.09   33.65 ]
 [ 33.65   33.82 ]
 [ 33.82   34.36 ]
 [ 34.36   34.15 ]
 [ 34.15   35.24 ]
 [ 35.24   36.36 ]
 [ 36.36   36.31 ]
 [ 36.31   36.68 ]
 [ 36.68   36.85 ]
 [ 36.85   34.75 ]
 [ 34.75   35.89 ]
 [ 35.89   34.6  ]
 [ 34.6    34.68 ]
 [ 34.68   34.86 ]
 [ 34.86   34.87 ]
 [ 34.87   34.82 ]
 [ 34.82   34.96 ]
 [ 34.96   35.02 ]
 [ 35.02   34.   ]
 [ 34.     34.19 ]
 [ 34.19   34.46 ]
 [ 34.46   34.54 ]
 [ 34.54   36.67 ]
 [ 36.67   35.77 ]
 [ 35.77   39.68 ]
 [ 39.68   38.29 ]
 [ 38.29   37.46 ]
 [ 37.46   37.28 ]
 [ 37.28   38.11 ]
 [ 38.11   37.28 ]
 [ 37.28   37.57 ]
 [ 37.57   44.4  ]
 [ 44.4    43.69 ]
 [ 43.69   42.73 ]
 [ 42.73   42.75 ]
 [ 42.75   41.65 ]
 [ 41.65   41.83 ]
 [ 41.83   42.05 ]
 [ 42.05   41.91 ]
 [ 41.91   41.69 ]
 [ 41.69   41.27 ]
 [ 41.27   41.92 ]
 [ 41.92   42.31 ]
 [ 42.31   40.79 ]
 [ 40.79   41.31 ]]
[[ 40.3  ]
 [ 40.09 ]
 [ 40.4  ]
 [ 40.1  ]
 [ 41.82 ]
 [ 41.91 ]
 [ 39.25 ]
 [ 38.75 ]
 [ 39.15 ]
 [ 38.94 ]
 [ 38.565]
 [ 37.97 ]
 [ 37.63 ]
 [ 38.59 ]
 [ 39.16 ]
 [ 39.03 ]
 [ 39.08 ]
 [ 38.07 ]
 [ 37.01 ]
 [ 37.01 ]
 [ 37.16 ]
 [ 37.87 ]
 [ 37.29 ]
 [ 39.5  ]
 [ 40.   ]
 [ 39.8  ]
 [ 39.35 ]
 [ 39.789]
 [ 39.84 ]
 [ 39.73 ]
 [ 39.97 ]
 [ 39.37 ]
 [ 39.31 ]
 [ 38.98 ]
 [ 38.71 ]
 [ 40.08 ]
 [ 39.13 ]
 [ 39.02 ]
 [ 38.69 ]
 [ 40.65 ]
 [ 43.74 ]
 [ 44.74 ]
 [ 45.08 ]
 [ 43.98 ]
 [ 43.59 ]
 [ 42.56 ]
 [ 39.2  ]
 [ 40.65 ]
 [ 40.51 ]
 [ 39.5  ]
 [ 37.53 ]
 [ 37.27 ]
 [ 34.82 ]
 [ 35.53 ]
 [ 35.21 ]
 [ 35.21 ]
 [ 35.35 ]
 [ 35.76 ]
 [ 35.75 ]
 [ 35.91 ]
 [ 36.59 ]
 [ 37.7  ]
 [ 37.85 ]
 [ 37.88 ]
 [ 37.81 ]
 [ 36.86 ]
 [ 36.63 ]
 [ 36.34 ]
 [ 36.87 ]
 [ 37.18 ]
 [ 36.52 ]
 [ 34.73 ]
 [ 35.8  ]
 [ 37.93 ]
 [ 39.   ]
 [ 37.92 ]
 [ 38.42 ]
 [ 37.39 ]
 [ 37.6  ]
 [ 37.04 ]
 [ 38.06 ]
 [ 39.   ]
 [ 40.81 ]
 [ 42.21 ]
 [ 42.08 ]
 [ 43.14 ]
 [ 42.87 ]
 [ 40.69 ]
 [ 39.7  ]
 [ 39.49 ]
 [ 38.97 ]
 [ 39.13 ]
 [ 38.64 ]
 [ 38.86 ]
 [ 39.24 ]
 [ 37.69 ]
 [ 37.09 ]
 [ 37.89 ]
 [ 38.91 ]
 [ 37.86 ]
 [ 36.85 ]
 [ 36.87 ]
 [ 36.6  ]
 [ 36.67 ]
 [ 36.78 ]
 [ 37.29 ]
 [ 38.9  ]
 [ 40.1  ]
 [ 39.68 ]
 [ 33.61 ]
 [ 34.47 ]
 [ 34.34 ]
 [ 34.97 ]
 [ 34.38 ]
 [ 34.09 ]
 [ 33.85 ]
 [ 33.82 ]
 [ 32.87 ]
 [ 32.82 ]
 [ 32.3  ]
 [ 32.14 ]
 [ 32.33 ]
 [ 31.65 ]
 [ 31.38 ]
 [ 31.69 ]
 [ 32.22 ]
 [ 32.64 ]
 [ 32.76 ]
 [ 33.09 ]
 [ 33.45 ]
 [ 33.   ]
 [ 32.49 ]
 [ 32.39 ]
 [ 31.58 ]
 [ 31.61 ]
 [ 30.96 ]
 [ 31.09 ]
 [ 31.42 ]
 [ 31.2  ]
 [ 31.8  ]
 [ 32.09 ]
 [ 33.65 ]
 [ 33.82 ]
 [ 34.36 ]
 [ 34.15 ]
 [ 35.24 ]
 [ 36.36 ]
 [ 36.31 ]
 [ 36.68 ]
 [ 36.85 ]
 [ 34.75 ]
 [ 35.89 ]
 [ 34.6  ]
 [ 34.68 ]
 [ 34.86 ]
 [ 34.87 ]
 [ 34.82 ]
 [ 34.96 ]
 [ 35.02 ]
 [ 34.   ]
 [ 34.19 ]
 [ 34.46 ]
 [ 34.54 ]
 [ 36.67 ]
 [ 35.77 ]
 [ 39.68 ]
 [ 38.29 ]
 [ 37.46 ]
 [ 37.28 ]
 [ 38.11 ]
 [ 37.28 ]
 [ 37.57 ]
 [ 44.4  ]
 [ 43.69 ]
 [ 42.73 ]
 [ 42.75 ]
 [ 41.65 ]
 [ 41.83 ]
 [ 42.05 ]
 [ 41.91 ]
 [ 41.69 ]
 [ 41.27 ]
 [ 41.92 ]
 [ 42.31 ]
 [ 40.79 ]
 [ 41.31 ]
 [ 40.74 ]]
train data size is: 187
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:90: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[4.35033506155014]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 404, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 233, in PredictSimpleRnn
    net = RnnSimpleModel()
TypeError: __init__() missing 3 required positional arguments: 'input_size', 'rnn_hidden_size', and 'output_size'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 17:42:18
INFO - ******************MLNX*******************
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:90: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[13.819672544797262, 3.8159361282984414, 4.472098569075267, 3.911172032356262, 3.1794872482617698, 4.718221147855123, 4.373409549395244, 4.128791968027751, 5.213945209980011, 4.683892011642456, 4.077167828877767, 3.084565897782644, 4.6556345621744795, 4.116649409135182, 4.690513630708058, 3.7588082750638327, 3.1918011903762817, 3.0979443788528442, 4.8002623319625854, 4.53005188703537]
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 404, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 233, in PredictSimpleRnn
    net = RnnSimpleModel()
TypeError: __init__() missing 3 required positional arguments: 'input_size', 'rnn_hidden_size', and 'output_size'
hello all, todays date & time is: 11/13/18 17:48:25
INFO - ******************MLNX*******************
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:90: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[127.18369734287262, 339.66989239056903, 82.09660561879475, 105.8289270401001, 7.972758412361145, 23.85501754283905, 16.30648680528005, 5.457259953022003, 20.40085045496623, 26.42764937877655, 31.133913437525432, 22.049868782361347, 26.461146235466003, 23.772536357243855, 11.92677390575409, 4.1275352239608765, 7.0422472059726715, 13.080608328183493, 13.319932460784912, 13.348655144373575]
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 582, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 485, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 404, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 233, in PredictSimpleRnn
    net = RnnSimpleModel()
TypeError: __init__() missing 3 required positional arguments: 'input_size', 'rnn_hidden_size', and 'output_size'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 18:07:30
INFO - ******************MLNX*******************
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 587, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 490, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 402, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 189, in ConstructTestData
    logging.debug("test_data_length: " + str(test_data_length))
NameError: name 'test_data_length' is not defined
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/13/18 18:08:25
INFO - ******************MLNX*******************
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:90: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[4.521799215248653, 9.039164340921811, 6.667212505425725, 2.4888593213898793, 3.556559839418956, 2.00276821319546, 2.784308965717043, 2.831295288034848, 2.298373660870961, 2.9875405111483166, 1.9577794671058655, 1.5002539243016924, 3.2332689272505895, 2.1096405626407693, 1.7984278798103333, 2.1605185163872584, 1.9513105260474342, 2.242594656135355, 2.0743488696004664, 3.151647667799677]
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - ******************MLNX*******************
hello all, todays date & time is: 11/14/18 09:24:00
SimpleRnn: epoch num: 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:91: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
SimpleRnn: epoch num: 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 20
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 21
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 22
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 23
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 24
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 25
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 26
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 27
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 28
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 29
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 30
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 31
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 32
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 33
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 34
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 35
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 36
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 37
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 38
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 39
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 40
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 41
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 42
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 43
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 44
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 45
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 46
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 47
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 48
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 49
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 50
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 51
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 52
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 53
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 54
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 55
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 56
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 57
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 58
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 59
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 60
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 61
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 62
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 63
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 64
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 65
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 66
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 67
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 68
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 69
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 70
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 71
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 72
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 73
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 74
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 75
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 76
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 77
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 78
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 79
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 80
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 81
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 82
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 83
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 84
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 85
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 86
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 87
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 88
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 89
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 90
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 91
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 92
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 93
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 94
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 95
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 96
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 97
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 98
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
SimpleRnn: epoch num: 99
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[608.6076870645795, 10.300478654248375, 9.755271290029798, 11.861451178789139, 10.50340485572815, 15.034592177186694, 15.911491973059517, 12.173064291477203, 3.4727495866162434, 5.035040478621211, 7.3018951032842905, 3.717710567372186, 3.1831759672079767, 3.041300132870674, 6.516946207199778, 2.5627390518784523, 3.3306794528450285, 3.5965394377708435, 1.930497048156602, 3.1850298834698543, 2.137155532836914, 2.5408418753317425, 2.020282892244203, 1.938608210001673, 1.7501326554587908, 3.5770569443702698, 2.1025877594947815, 2.0986527406743596, 4.413406129394259, 1.7161847651004791, 1.5573803958083903, 2.59444470597165, 2.105953193136624, 1.306138158908912, 1.4169590239013945, 1.1713084317743778, 2.1091175132564137, 3.9129068361861363, 2.206863522529602, 2.1486037533198084, 2.090646560703005, 1.7155241189258439, 4.581972101969378, 1.7714504706008094, 1.6268271399395806, 1.464649916759559, 1.4958873965910502, 1.9330173688275474, 2.4440406981323446, 3.0267485699483325, 2.560033389500209, 3.303558813674109, 3.5260751800877705, 1.2685532197356224, 1.4012379007680076, 1.2670826720339912, 2.7072542192680493, 3.051408188683646, 2.547491022518703, 1.5520096368023328, 3.232963058565344, 2.1007281754698073, 1.5305534867303712, 5.618041887879372, 4.409486101674182, 1.346753189606326, 1.4896513537636824, 2.0035631954669952, 1.3278159946203232, 2.2667889084134782, 2.244916319847107, 1.1318832598626614, 1.105627205222845, 1.4921415842005186, 1.7010095029004983, 1.800272109253066, 2.0744634866714478, 3.134149344904082, 1.4107827310051237, 3.4528588971921375, 1.6619876729590553, 1.66839121814285, 1.914324747664588, 1.9445384506668364, 1.4129695328218597, 3.4113709841455733, 2.611355989107064, 1.3752396606973238, 1.7227251934153693, 2.9730102281485284, 1.8699887533273016, 2.03802760850106, 3.5766613483428955, 1.8467226326465607, 1.8353515852774893, 6.657047435641289, 2.143955838467394, 2.999938351767404, 2.400693801896913, 2.0133349363292967]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 589, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 492, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 411, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params)
TypeError: PredictSimpleRnn() missing 1 required positional argument: 'file_path'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/14/18 20:09:46
INFO - ******************MLNX*******************
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:108: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current loss is: 
INFO - 14.100686073303223
INFO - current loss is: 
INFO - 10.53170394897461
INFO - current loss is: 
INFO - 5.561623573303223
INFO - current loss is: 
INFO - 1.411132574081421
INFO - current loss is: 
INFO - 0.6099821925163269
INFO - current loss is: 
INFO - 3.5380280017852783
INFO - current loss is: 
INFO - 1.683706283569336
INFO - current loss is: 
INFO - 0.9705379605293274
INFO - current loss is: 
INFO - 2.124241828918457
INFO - current loss is: 
INFO - 4.09492301940918
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/14/18 20:10:13
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 587, in <module>
    )
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 490, in HyperParameter_Optimizations
    curr_config = config_net_default
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 409, in RunNetworkArch
    
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 240, in PredictSimpleRnn
    
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 33, in forward
    def forward(self, x):
AttributeError: 'str' object has no attribute 'unsqueeze'
INFO - ******************MLNX*******************
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 06:48:51
INFO - ******************MLNX*******************
INFO - *****************finished executing******************
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 61, in <module>
    from Models import SimpleRNN
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 52
    def Train(input_size, hidden_size, output_size, train_loader,learning_rate=0.001,num_epochs = 100,file_path):
             ^
SyntaxError: non-default argument follows default argument
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 240
    num_epochs = model_params.num_epochs, file_path, cuda = model_params.use_cuda)
                                         ^
SyntaxError: positional argument follows keyword argument
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 61, in <module>
    from Models import SimpleRNN
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 52
    def Train(input_size, hidden_size, output_size, train_loader,learning_rate=0.001,num_epochs = 100,file_path):
             ^
SyntaxError: non-default argument follows default argument
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:09:01
INFO - ******************MLNX*******************
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:106: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current loss is: 
INFO - 12.212804794311523
INFO - current loss is: 
INFO - 10.368217468261719
INFO - current loss is: 
INFO - 5.638168811798096
INFO - current loss is: 
INFO - 1.4456108808517456
INFO - current loss is: 
INFO - 0.6031202077865601
INFO - current loss is: 
INFO - 3.6098384857177734
INFO - current loss is: 
INFO - 1.6613209247589111
INFO - current loss is: 
INFO - 0.715372622013092
INFO - current loss is: 
INFO - 2.207742691040039
INFO - current loss is: 
INFO - 4.433875560760498
INFO - current loss is: 
INFO - 4.092886924743652
INFO - current loss is: 
INFO - 1.1099351644515991
INFO - current loss is: 
INFO - 1.154125690460205
INFO - current loss is: 
INFO - 1.0017380714416504
INFO - current loss is: 
INFO - 0.9782745242118835
INFO - current loss is: 
INFO - 0.4917786121368408
INFO - current loss is: 
INFO - 3.200915575027466
INFO - current loss is: 
INFO - 3.0109946727752686
INFO - current loss is: 
INFO - 3.604013681411743
INFO - current loss is: 
INFO - 1.41066312789917
INFO - current loss is: 
INFO - 1.5672954320907593
INFO - current loss is: 
INFO - 0.3010474145412445
INFO - current loss is: 
INFO - 0.3922026455402374
INFO - current loss is: 
INFO - 0.6803587079048157
INFO - current loss is: 
INFO - 0.3401734232902527
INFO - current loss is: 
INFO - 1.2239253520965576
INFO - current loss is: 
INFO - 1.3295871019363403
INFO - current loss is: 
INFO - 0.38658103346824646
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.112333059310913
INFO - current loss is: 
INFO - 1.6077306270599365
INFO - current loss is: 
INFO - 1.1912140846252441
INFO - current loss is: 
INFO - 1.3079473972320557
INFO - current loss is: 
INFO - 0.5062770247459412
INFO - current loss is: 
INFO - 2.36855411529541
INFO - current loss is: 
INFO - 1.0680112838745117
INFO - current loss is: 
INFO - 1.2405036687850952
INFO - current loss is: 
INFO - 0.837984561920166
INFO - current loss is: 
INFO - 0.5588870048522949
INFO - current loss is: 
INFO - 1.2076438665390015
INFO - current loss is: 
INFO - 2.62155818939209
INFO - current loss is: 
INFO - 2.4074482917785645
INFO - current loss is: 
INFO - 0.9676471948623657
INFO - current loss is: 
INFO - 0.8603506684303284
INFO - current loss is: 
INFO - 0.49176090955734253
INFO - current loss is: 
INFO - 1.1249992847442627
INFO - current loss is: 
INFO - 2.077881336212158
INFO - current loss is: 
INFO - 2.815875291824341
INFO - current loss is: 
INFO - 0.764821469783783
INFO - current loss is: 
INFO - 1.4582024812698364
INFO - current loss is: 
INFO - 0.18865390121936798
INFO - current loss is: 
INFO - 0.37267768383026123
INFO - current loss is: 
INFO - 0.7918210625648499
INFO - current loss is: 
INFO - 0.6258100867271423
INFO - current loss is: 
INFO - 0.8881287574768066
INFO - current loss is: 
INFO - 1.1990807056427002
INFO - current loss is: 
INFO - 1.6468100547790527
INFO - SimpleRnn: epoch num: 
INFO - 2
Traceback (most recent call last):
  File "<string>", line 1, in <module>
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 599, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 497, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 414, in RunNetworkArch
    TrainSimpleRnn(Data['x_train'],Data['y_train'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 226, in TrainSimpleRnn
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\spawn.py", line 105, in spawn_main
    SimpleRNN.Train(input_size, hidden_size, output_size, train_loader,file_path,learning_rate=model_params.learning_rate,num_epochs = model_params.num_epochs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 83, in Train
    exitcode = _main(fd)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\spawn.py", line 114, in _main
    for i, data in enumerate(train_loader):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\utils\data\dataloader.py", line 501, in __iter__
    prepare(preparation_data)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\spawn.py", line 225, in prepare
    return _DataLoaderIter(self)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\utils\data\dataloader.py", line 289, in __init__
    _fixup_main_from_path(data['init_main_from_path'])
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    w.start()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\context.py", line 223, in _Popen
    run_name="__mp_main__")
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\runpy.py", line 261, in run_path
    code, fname = _get_code_from_file(run_name, path_name)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\runpy.py", line 236, in _get_code_from_file
    return _default_context.get_context().Process._Popen(process_obj)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\context.py", line 322, in _Popen
    code = compile(f.read(), fname, 'exec')
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594
    )
    ^
SyntaxError: invalid syntax
    return Popen(process_obj)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\popen_spawn_win32.py", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594
    )
    ^
SyntaxError: invalid syntax
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:12:43
INFO - ******************MLNX*******************
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current loss is: 
INFO - 1521.5540771484375
INFO - current loss is: 
INFO - 1445.0150146484375
INFO - current loss is: 
INFO - 1357.984130859375
INFO - current loss is: 
INFO - 1046.4776611328125
INFO - current loss is: 
INFO - 941.4432983398438
INFO - current loss is: 
INFO - 1273.0106201171875
INFO - current loss is: 
INFO - 1315.6688232421875
INFO - current loss is: 
INFO - 1239.5238037109375
INFO - current loss is: 
INFO - 1127.3868408203125
INFO - current loss is: 
INFO - 1066.8541259765625
INFO - current loss is: 
INFO - 903.8036499023438
INFO - current loss is: 
INFO - 774.9445190429688
INFO - current loss is: 
INFO - 497.8729248046875
INFO - current loss is: 
INFO - 185.7720184326172
INFO - current loss is: 
INFO - 36.089317321777344
INFO - current loss is: 
INFO - 47.98638916015625
INFO - current loss is: 
INFO - 300.1119384765625
INFO - current loss is: 
INFO - 495.5299987792969
INFO - current loss is: 
INFO - 980.5609741210938
INFO - current loss is: 
INFO - 846.9816284179688
INFO - current loss is: 
INFO - 633.7086791992188
INFO - current loss is: 
INFO - 396.6647644042969
INFO - current loss is: 
INFO - 164.18702697753906
INFO - current loss is: 
INFO - 52.927635192871094
INFO - current loss is: 
INFO - 2.361154556274414
INFO - current loss is: 
INFO - 23.26361846923828
INFO - current loss is: 
INFO - 67.82612609863281
INFO - current loss is: 
INFO - 46.94511032104492
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 57.049415588378906
INFO - current loss is: 
INFO - 57.27389907836914
INFO - current loss is: 
INFO - 43.582054138183594
INFO - current loss is: 
INFO - 15.808313369750977
INFO - current loss is: 
INFO - 5.750519275665283
INFO - current loss is: 
INFO - 3.5631330013275146
INFO - current loss is: 
INFO - 9.45668888092041
INFO - current loss is: 
INFO - 21.699298858642578
INFO - current loss is: 
INFO - 16.471858978271484
INFO - current loss is: 
INFO - 10.549323081970215
INFO - current loss is: 
INFO - 3.633336067199707
INFO - current loss is: 
INFO - 4.590222358703613
INFO - current loss is: 
INFO - 7.098458766937256
INFO - current loss is: 
INFO - 7.171196937561035
INFO - current loss is: 
INFO - 7.985321521759033
INFO - current loss is: 
INFO - 2.2821285724639893
INFO - current loss is: 
INFO - 4.960769176483154
INFO - current loss is: 
INFO - 5.952469348907471
INFO - current loss is: 
INFO - 8.875048637390137
INFO - current loss is: 
INFO - 13.077665328979492
INFO - current loss is: 
INFO - 2.266857862472534
INFO - current loss is: 
INFO - 1.7805514335632324
INFO - current loss is: 
INFO - 3.7364039421081543
INFO - current loss is: 
INFO - 5.826662063598633
INFO - current loss is: 
INFO - 4.699289798736572
INFO - current loss is: 
INFO - 4.055622577667236
INFO - current loss is: 
INFO - 2.590698719024658
INFO - current loss is: 
INFO - 24.790489196777344
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 10.217881202697754
INFO - current loss is: 
INFO - 8.110552787780762
INFO - current loss is: 
INFO - 2.878053903579712
INFO - current loss is: 
INFO - 3.3289546966552734
INFO - current loss is: 
INFO - 7.938934803009033
INFO - current loss is: 
INFO - 18.87359046936035
INFO - current loss is: 
INFO - 11.328150749206543
INFO - current loss is: 
INFO - 5.226716041564941
INFO - current loss is: 
INFO - 0.7653734683990479
INFO - current loss is: 
INFO - 7.93710994720459
INFO - current loss is: 
INFO - 25.186262130737305
INFO - current loss is: 
INFO - 26.844751358032227
INFO - current loss is: 
INFO - 35.31229019165039
INFO - current loss is: 
INFO - 20.276865005493164
INFO - current loss is: 
INFO - 3.8349435329437256
INFO - current loss is: 
INFO - 1.9441280364990234
INFO - current loss is: 
INFO - 3.9663825035095215
INFO - current loss is: 
INFO - 19.54464340209961
INFO - current loss is: 
INFO - 12.217948913574219
INFO - current loss is: 
INFO - 5.7113542556762695
INFO - current loss is: 
INFO - 2.7795989513397217
INFO - current loss is: 
INFO - 6.012750148773193
INFO - current loss is: 
INFO - 4.24434232711792
INFO - current loss is: 
INFO - 1.9951916933059692
INFO - current loss is: 
INFO - 1.904304027557373
INFO - current loss is: 
INFO - 7.183610439300537
INFO - current loss is: 
INFO - 4.335331439971924
INFO - current loss is: 
INFO - 3.576859474182129
INFO - SimpleRnn: epoch num: 
INFO - 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.903723955154419
INFO - current loss is: 
INFO - 4.81465482711792
INFO - current loss is: 
INFO - 3.0460364818573
INFO - current loss is: 
INFO - 2.997683048248291
INFO - current loss is: 
INFO - 1.271501064300537
INFO - current loss is: 
INFO - 4.3390116691589355
INFO - current loss is: 
INFO - 1.2429814338684082
INFO - current loss is: 
INFO - 2.260120153427124
INFO - current loss is: 
INFO - 0.6582164168357849
INFO - current loss is: 
INFO - 1.4096734523773193
INFO - current loss is: 
INFO - 0.9237226843833923
INFO - current loss is: 
INFO - 1.2235652208328247
INFO - current loss is: 
INFO - 2.381885528564453
INFO - current loss is: 
INFO - 2.4296765327453613
INFO - current loss is: 
INFO - 5.624073505401611
INFO - current loss is: 
INFO - 7.3340301513671875
INFO - current loss is: 
INFO - 3.6174747943878174
INFO - current loss is: 
INFO - 9.359586715698242
INFO - current loss is: 
INFO - 1.9312938451766968
INFO - current loss is: 
INFO - 9.519500732421875
INFO - current loss is: 
INFO - 3.547539710998535
INFO - current loss is: 
INFO - 2.677330732345581
INFO - current loss is: 
INFO - 2.172137975692749
INFO - current loss is: 
INFO - 4.286295413970947
INFO - current loss is: 
INFO - 4.222301006317139
INFO - current loss is: 
INFO - 4.463473320007324
INFO - current loss is: 
INFO - 1.1337475776672363
INFO - current loss is: 
INFO - 15.11047649383545
INFO - SimpleRnn: epoch num: 
INFO - 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 5.011373996734619
INFO - current loss is: 
INFO - 5.018911361694336
INFO - current loss is: 
INFO - 3.6210057735443115
INFO - current loss is: 
INFO - 2.859217643737793
INFO - current loss is: 
INFO - 2.762974500656128
INFO - current loss is: 
INFO - 5.045139312744141
INFO - current loss is: 
INFO - 1.2496004104614258
INFO - current loss is: 
INFO - 4.27810525894165
INFO - current loss is: 
INFO - 1.5633771419525146
INFO - current loss is: 
INFO - 1.0441672801971436
INFO - current loss is: 
INFO - 2.434293746948242
INFO - current loss is: 
INFO - 8.58633804321289
INFO - current loss is: 
INFO - 4.5758233070373535
INFO - current loss is: 
INFO - 2.7750697135925293
INFO - current loss is: 
INFO - 1.9690083265304565
INFO - current loss is: 
INFO - 2.197692632675171
INFO - current loss is: 
INFO - 3.928536891937256
INFO - current loss is: 
INFO - 5.469641208648682
INFO - current loss is: 
INFO - 2.227748394012451
INFO - current loss is: 
INFO - 6.851982116699219
INFO - current loss is: 
INFO - 2.392056465148926
INFO - current loss is: 
INFO - 1.6688458919525146
INFO - current loss is: 
INFO - 3.6922342777252197
INFO - current loss is: 
INFO - 6.926158428192139
INFO - current loss is: 
INFO - 7.1538896560668945
INFO - current loss is: 
INFO - 7.336683750152588
INFO - current loss is: 
INFO - 0.9798175096511841
INFO - current loss is: 
INFO - 18.1549015045166
INFO - SimpleRnn: epoch num: 
INFO - 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 15.208422660827637
INFO - current loss is: 
INFO - 18.86826515197754
INFO - current loss is: 
INFO - 11.853196144104004
INFO - current loss is: 
INFO - 7.900290489196777
INFO - current loss is: 
INFO - 1.2533272504806519
INFO - current loss is: 
INFO - 9.723494529724121
INFO - current loss is: 
INFO - 13.856348037719727
INFO - current loss is: 
INFO - 17.649974822998047
INFO - current loss is: 
INFO - 17.244476318359375
INFO - current loss is: 
INFO - 12.166760444641113
INFO - current loss is: 
INFO - 2.8808887004852295
INFO - current loss is: 
INFO - 1.3520214557647705
INFO - current loss is: 
INFO - 8.299642562866211
INFO - current loss is: 
INFO - 11.680525779724121
INFO - current loss is: 
INFO - 4.74271297454834
INFO - current loss is: 
INFO - 1.4427049160003662
INFO - current loss is: 
INFO - 1.512038230895996
INFO - current loss is: 
INFO - 12.412660598754883
INFO - current loss is: 
INFO - 8.410669326782227
INFO - current loss is: 
INFO - 5.185784339904785
INFO - current loss is: 
INFO - 2.463026523590088
INFO - current loss is: 
INFO - 3.1332573890686035
INFO - current loss is: 
INFO - 1.7418259382247925
INFO - current loss is: 
INFO - 1.4202717542648315
INFO - current loss is: 
INFO - 1.3450862169265747
INFO - current loss is: 
INFO - 2.667571544647217
INFO - current loss is: 
INFO - 1.5366147756576538
INFO - current loss is: 
INFO - 14.785552024841309
INFO - SimpleRnn: epoch num: 
INFO - 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 4.03441858291626
INFO - current loss is: 
INFO - 4.352109432220459
INFO - current loss is: 
INFO - 4.478418350219727
INFO - current loss is: 
INFO - 3.9211041927337646
INFO - current loss is: 
INFO - 4.419140815734863
INFO - current loss is: 
INFO - 7.899210453033447
INFO - current loss is: 
INFO - 1.302860975265503
INFO - current loss is: 
INFO - 4.2481842041015625
INFO - current loss is: 
INFO - 6.094157695770264
INFO - current loss is: 
INFO - 8.353034973144531
INFO - current loss is: 
INFO - 7.406548976898193
INFO - current loss is: 
INFO - 1.3482807874679565
INFO - current loss is: 
INFO - 1.809858798980713
INFO - current loss is: 
INFO - 3.6817002296447754
INFO - current loss is: 
INFO - 5.976004600524902
INFO - current loss is: 
INFO - 3.181929111480713
INFO - current loss is: 
INFO - 1.7289735078811646
INFO - current loss is: 
INFO - 4.430818557739258
INFO - current loss is: 
INFO - 3.780463457107544
INFO - current loss is: 
INFO - 11.026851654052734
INFO - current loss is: 
INFO - 3.745145320892334
INFO - current loss is: 
INFO - 2.089872360229492
INFO - current loss is: 
INFO - 2.7904675006866455
INFO - current loss is: 
INFO - 6.375783920288086
INFO - current loss is: 
INFO - 7.5834245681762695
INFO - current loss is: 
INFO - 8.867681503295898
INFO - current loss is: 
INFO - 1.984618902206421
INFO - current loss is: 
INFO - 10.416534423828125
INFO - SimpleRnn: epoch num: 
INFO - 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 8.096426010131836
INFO - current loss is: 
INFO - 10.752357482910156
INFO - current loss is: 
INFO - 5.697454929351807
INFO - current loss is: 
INFO - 3.5900039672851562
INFO - current loss is: 
INFO - 1.7683436870574951
INFO - current loss is: 
INFO - 9.11120319366455
INFO - current loss is: 
INFO - 6.408163547515869
INFO - current loss is: 
INFO - 4.538356304168701
INFO - current loss is: 
INFO - 1.2326784133911133
INFO - current loss is: 
INFO - 2.333876132965088
INFO - current loss is: 
INFO - 7.516045570373535
INFO - current loss is: 
INFO - 5.738899230957031
INFO - current loss is: 
INFO - 6.8317551612854
INFO - current loss is: 
INFO - 3.2568130493164062
INFO - current loss is: 
INFO - 4.074509143829346
INFO - current loss is: 
INFO - 6.985888957977295
INFO - current loss is: 
INFO - 4.710892677307129
INFO - current loss is: 
INFO - 12.52202320098877
INFO - current loss is: 
INFO - 3.311650514602661
INFO - current loss is: 
INFO - 7.1057000160217285
INFO - current loss is: 
INFO - 6.670588970184326
INFO - current loss is: 
INFO - 9.158967971801758
INFO - current loss is: 
INFO - 4.470348834991455
INFO - current loss is: 
INFO - 1.427014946937561
INFO - current loss is: 
INFO - 3.361555337905884
INFO - current loss is: 
INFO - 11.905021667480469
INFO - current loss is: 
INFO - 12.04797077178955
INFO - current loss is: 
INFO - 1.5866154432296753
INFO - SimpleRnn: epoch num: 
INFO - 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.5238410234451294
INFO - current loss is: 
INFO - 5.209446907043457
INFO - current loss is: 
INFO - 5.277015686035156
INFO - current loss is: 
INFO - 7.2041192054748535
INFO - current loss is: 
INFO - 2.3534677028656006
INFO - current loss is: 
INFO - 3.285109281539917
INFO - current loss is: 
INFO - 1.194490909576416
INFO - current loss is: 
INFO - 1.8871257305145264
INFO - current loss is: 
INFO - 0.8680628538131714
INFO - current loss is: 
INFO - 1.5453802347183228
INFO - current loss is: 
INFO - 1.1526936292648315
INFO - current loss is: 
INFO - 3.1174378395080566
INFO - current loss is: 
INFO - 2.653582811355591
INFO - current loss is: 
INFO - 2.467893600463867
INFO - current loss is: 
INFO - 1.8304387331008911
INFO - current loss is: 
INFO - 3.5262839794158936
INFO - current loss is: 
INFO - 10.078892707824707
INFO - current loss is: 
INFO - 6.062615871429443
INFO - current loss is: 
INFO - 4.540511608123779
INFO - current loss is: 
INFO - 5.923884868621826
INFO - current loss is: 
INFO - 6.17765474319458
INFO - current loss is: 
INFO - 5.817229747772217
INFO - current loss is: 
INFO - 7.412193775177002
INFO - current loss is: 
INFO - 6.428354263305664
INFO - current loss is: 
INFO - 2.9077436923980713
INFO - current loss is: 
INFO - 2.2091619968414307
INFO - current loss is: 
INFO - 7.860819339752197
INFO - current loss is: 
INFO - 39.99485778808594
INFO - SimpleRnn: epoch num: 
INFO - 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 22.95766258239746
INFO - current loss is: 
INFO - 21.395172119140625
INFO - current loss is: 
INFO - 11.348722457885742
INFO - current loss is: 
INFO - 6.0025410652160645
INFO - current loss is: 
INFO - 1.147900938987732
INFO - current loss is: 
INFO - 7.301257610321045
INFO - current loss is: 
INFO - 5.7078704833984375
INFO - current loss is: 
INFO - 4.834205627441406
INFO - current loss is: 
INFO - 1.8446699380874634
INFO - current loss is: 
INFO - 1.2834643125534058
INFO - current loss is: 
INFO - 3.63413405418396
INFO - current loss is: 
INFO - 2.1401727199554443
INFO - current loss is: 
INFO - 2.4173552989959717
INFO - current loss is: 
INFO - 2.042996883392334
INFO - current loss is: 
INFO - 3.5352189540863037
INFO - current loss is: 
INFO - 2.1615965366363525
INFO - current loss is: 
INFO - 1.5891352891921997
INFO - current loss is: 
INFO - 4.758813858032227
INFO - current loss is: 
INFO - 1.9951574802398682
INFO - current loss is: 
INFO - 6.400392055511475
INFO - current loss is: 
INFO - 2.1439552307128906
INFO - current loss is: 
INFO - 1.6447761058807373
INFO - current loss is: 
INFO - 1.082281470298767
INFO - current loss is: 
INFO - 1.199265718460083
INFO - current loss is: 
INFO - 0.3443629741668701
INFO - current loss is: 
INFO - 1.8451637029647827
INFO - current loss is: 
INFO - 0.6115744709968567
INFO - current loss is: 
INFO - 5.268607139587402
INFO - SimpleRnn: epoch num: 
INFO - 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.081620931625366
INFO - current loss is: 
INFO - 5.280416488647461
INFO - current loss is: 
INFO - 3.7449798583984375
INFO - current loss is: 
INFO - 2.1879067420959473
INFO - current loss is: 
INFO - 0.9747949242591858
INFO - current loss is: 
INFO - 2.9316513538360596
INFO - current loss is: 
INFO - 3.453862428665161
INFO - current loss is: 
INFO - 7.736992359161377
INFO - current loss is: 
INFO - 4.433957576751709
INFO - current loss is: 
INFO - 2.328806161880493
INFO - current loss is: 
INFO - 0.8962390422821045
INFO - current loss is: 
INFO - 7.994156360626221
INFO - current loss is: 
INFO - 11.38472843170166
INFO - current loss is: 
INFO - 10.01230239868164
INFO - current loss is: 
INFO - 10.359341621398926
INFO - current loss is: 
INFO - 5.748435020446777
INFO - current loss is: 
INFO - 1.3939100503921509
INFO - current loss is: 
INFO - 3.7834181785583496
INFO - current loss is: 
INFO - 3.972933292388916
INFO - current loss is: 
INFO - 11.524874687194824
INFO - current loss is: 
INFO - 6.277021884918213
INFO - current loss is: 
INFO - 3.8853213787078857
INFO - current loss is: 
INFO - 0.9865953326225281
INFO - current loss is: 
INFO - 4.710041046142578
INFO - current loss is: 
INFO - 9.991336822509766
INFO - current loss is: 
INFO - 17.589540481567383
INFO - current loss is: 
INFO - 13.917387008666992
INFO - current loss is: 
INFO - 1.6924545764923096
INFO - SimpleRnn: epoch num: 
INFO - 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.1539336442947388
INFO - current loss is: 
INFO - 3.911710500717163
INFO - current loss is: 
INFO - 2.5120325088500977
INFO - current loss is: 
INFO - 2.382730722427368
INFO - current loss is: 
INFO - 1.4602073431015015
INFO - current loss is: 
INFO - 6.596440315246582
INFO - current loss is: 
INFO - 3.2162673473358154
INFO - current loss is: 
INFO - 1.9080246686935425
INFO - current loss is: 
INFO - 0.8889671564102173
INFO - current loss is: 
INFO - 2.652458429336548
INFO - current loss is: 
INFO - 3.3518877029418945
INFO - current loss is: 
INFO - 0.944953441619873
INFO - current loss is: 
INFO - 1.5599912405014038
INFO - current loss is: 
INFO - 2.4039876461029053
INFO - current loss is: 
INFO - 2.766965389251709
INFO - current loss is: 
INFO - 0.868850827217102
INFO - current loss is: 
INFO - 3.9554264545440674
INFO - current loss is: 
INFO - 4.1483917236328125
INFO - current loss is: 
INFO - 4.138955593109131
INFO - current loss is: 
INFO - 5.658030033111572
INFO - current loss is: 
INFO - 2.4495255947113037
INFO - current loss is: 
INFO - 1.282443881034851
INFO - current loss is: 
INFO - 1.167830467224121
INFO - current loss is: 
INFO - 1.0010132789611816
INFO - current loss is: 
INFO - 0.374916672706604
INFO - current loss is: 
INFO - 1.6667355298995972
INFO - current loss is: 
INFO - 0.6294046640396118
INFO - current loss is: 
INFO - 4.373654365539551
INFO - SimpleRnn: epoch num: 
INFO - 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.8865631818771362
INFO - current loss is: 
INFO - 4.799666404724121
INFO - current loss is: 
INFO - 3.608555793762207
INFO - current loss is: 
INFO - 2.0819990634918213
INFO - current loss is: 
INFO - 1.2293891906738281
INFO - current loss is: 
INFO - 3.248483657836914
INFO - current loss is: 
INFO - 1.6734282970428467
INFO - current loss is: 
INFO - 1.8091627359390259
INFO - current loss is: 
INFO - 2.074497699737549
INFO - current loss is: 
INFO - 4.365302085876465
INFO - current loss is: 
INFO - 3.0816614627838135
INFO - current loss is: 
INFO - 2.5581095218658447
INFO - current loss is: 
INFO - 1.8047252893447876
INFO - current loss is: 
INFO - 4.412594318389893
INFO - current loss is: 
INFO - 2.2192845344543457
INFO - current loss is: 
INFO - 0.7054967880249023
INFO - current loss is: 
INFO - 1.5801349878311157
INFO - current loss is: 
INFO - 9.430427551269531
INFO - current loss is: 
INFO - 6.544662952423096
INFO - current loss is: 
INFO - 3.4485671520233154
INFO - current loss is: 
INFO - 2.2373573780059814
INFO - current loss is: 
INFO - 2.877871513366699
INFO - current loss is: 
INFO - 2.016789674758911
INFO - current loss is: 
INFO - 1.0332750082015991
INFO - current loss is: 
INFO - 2.1482722759246826
INFO - current loss is: 
INFO - 7.050252914428711
INFO - current loss is: 
INFO - 5.64554500579834
INFO - current loss is: 
INFO - 1.698799967765808
INFO - SimpleRnn: epoch num: 
INFO - 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.3260695934295654
INFO - current loss is: 
INFO - 3.073819637298584
INFO - current loss is: 
INFO - 1.9892302751541138
INFO - current loss is: 
INFO - 2.7517974376678467
INFO - current loss is: 
INFO - 0.918781042098999
INFO - current loss is: 
INFO - 3.4310665130615234
INFO - current loss is: 
INFO - 1.280216932296753
INFO - current loss is: 
INFO - 1.5442323684692383
INFO - current loss is: 
INFO - 0.606105625629425
INFO - current loss is: 
INFO - 0.9066834449768066
INFO - current loss is: 
INFO - 0.9564390182495117
INFO - current loss is: 
INFO - 1.828439474105835
INFO - current loss is: 
INFO - 1.467612624168396
INFO - current loss is: 
INFO - 2.0535500049591064
INFO - current loss is: 
INFO - 1.5685299634933472
INFO - current loss is: 
INFO - 0.7936930656433105
INFO - current loss is: 
INFO - 1.7186110019683838
INFO - current loss is: 
INFO - 4.737669467926025
INFO - current loss is: 
INFO - 2.6776435375213623
INFO - current loss is: 
INFO - 3.2763330936431885
INFO - current loss is: 
INFO - 2.1235337257385254
INFO - current loss is: 
INFO - 1.3032050132751465
INFO - current loss is: 
INFO - 0.9525795578956604
INFO - current loss is: 
INFO - 1.136803388595581
INFO - current loss is: 
INFO - 0.955137312412262
INFO - current loss is: 
INFO - 1.7884517908096313
INFO - current loss is: 
INFO - 1.2882251739501953
INFO - current loss is: 
INFO - 10.743387222290039
INFO - SimpleRnn: epoch num: 
INFO - 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 3.1400742530822754
INFO - current loss is: 
INFO - 3.2422752380371094
INFO - current loss is: 
INFO - 2.6278178691864014
INFO - current loss is: 
INFO - 2.6463053226470947
INFO - current loss is: 
INFO - 1.9544552564620972
INFO - current loss is: 
INFO - 4.457883358001709
INFO - current loss is: 
INFO - 0.8308377861976624
INFO - current loss is: 
INFO - 4.83256721496582
INFO - current loss is: 
INFO - 6.897196292877197
INFO - current loss is: 
INFO - 8.876826286315918
INFO - current loss is: 
INFO - 7.857818126678467
INFO - current loss is: 
INFO - 1.8399103879928589
INFO - current loss is: 
INFO - 1.236321210861206
INFO - current loss is: 
INFO - 2.1008663177490234
INFO - current loss is: 
INFO - 2.8565495014190674
INFO - current loss is: 
INFO - 1.2725334167480469
INFO - current loss is: 
INFO - 2.0653176307678223
INFO - current loss is: 
INFO - 2.838855504989624
INFO - current loss is: 
INFO - 1.4774422645568848
INFO - current loss is: 
INFO - 3.2884786128997803
INFO - current loss is: 
INFO - 6.132877826690674
INFO - current loss is: 
INFO - 4.630666255950928
INFO - current loss is: 
INFO - 4.682422161102295
INFO - current loss is: 
INFO - 3.3848659992218018
INFO - current loss is: 
INFO - 0.8059705495834351
INFO - current loss is: 
INFO - 2.0734078884124756
INFO - current loss is: 
INFO - 4.609648704528809
INFO - current loss is: 
INFO - 16.629032135009766
INFO - SimpleRnn: epoch num: 
INFO - 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 4.9773125648498535
INFO - current loss is: 
INFO - 3.7698287963867188
INFO - current loss is: 
INFO - 2.135741949081421
INFO - current loss is: 
INFO - 2.327155590057373
INFO - current loss is: 
INFO - 1.6772830486297607
INFO - current loss is: 
INFO - 4.321951389312744
INFO - current loss is: 
INFO - 0.8273071050643921
INFO - current loss is: 
INFO - 3.9932801723480225
INFO - current loss is: 
INFO - 5.402890682220459
INFO - current loss is: 
INFO - 6.7769951820373535
INFO - current loss is: 
INFO - 5.680919647216797
INFO - current loss is: 
INFO - 1.0979975461959839
INFO - current loss is: 
INFO - 1.5860992670059204
INFO - current loss is: 
INFO - 2.707319736480713
INFO - current loss is: 
INFO - 3.848351001739502
INFO - current loss is: 
INFO - 2.2168660163879395
INFO - current loss is: 
INFO - 1.354417324066162
INFO - current loss is: 
INFO - 2.927523374557495
INFO - current loss is: 
INFO - 2.3612191677093506
INFO - current loss is: 
INFO - 6.379585266113281
INFO - current loss is: 
INFO - 2.8582992553710938
INFO - current loss is: 
INFO - 0.9578696489334106
INFO - current loss is: 
INFO - 1.982448935508728
INFO - current loss is: 
INFO - 4.796182155609131
INFO - current loss is: 
INFO - 5.357800483703613
INFO - current loss is: 
INFO - 6.411532402038574
INFO - current loss is: 
INFO - 1.680743932723999
INFO - current loss is: 
INFO - 6.600831508636475
INFO - SimpleRnn: epoch num: 
INFO - 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 4.916080474853516
INFO - current loss is: 
INFO - 7.019886016845703
INFO - current loss is: 
INFO - 3.979823350906372
INFO - current loss is: 
INFO - 2.8191096782684326
INFO - current loss is: 
INFO - 1.029193639755249
INFO - current loss is: 
INFO - 6.571468830108643
INFO - current loss is: 
INFO - 4.791988372802734
INFO - current loss is: 
INFO - 3.530881881713867
INFO - current loss is: 
INFO - 0.9675964117050171
INFO - current loss is: 
INFO - 1.5420209169387817
INFO - current loss is: 
INFO - 4.709838390350342
INFO - current loss is: 
INFO - 3.5679101943969727
INFO - current loss is: 
INFO - 3.9011223316192627
INFO - current loss is: 
INFO - 2.1954333782196045
INFO - current loss is: 
INFO - 2.911001443862915
INFO - current loss is: 
INFO - 5.321859359741211
INFO - current loss is: 
INFO - 3.6238367557525635
INFO - current loss is: 
INFO - 8.501553535461426
INFO - current loss is: 
INFO - 2.7068076133728027
INFO - current loss is: 
INFO - 4.236106872558594
INFO - current loss is: 
INFO - 4.7325263023376465
INFO - current loss is: 
INFO - 5.186988353729248
INFO - current loss is: 
INFO - 2.765019178390503
INFO - current loss is: 
INFO - 0.9835588335990906
INFO - current loss is: 
INFO - 2.354581356048584
INFO - current loss is: 
INFO - 8.554252624511719
INFO - current loss is: 
INFO - 9.025273323059082
INFO - current loss is: 
INFO - 2.226332902908325
INFO - SimpleRnn: epoch num: 
INFO - 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.1191326379776
INFO - current loss is: 
INFO - 3.576047420501709
INFO - current loss is: 
INFO - 3.9582443237304688
INFO - current loss is: 
INFO - 5.501128673553467
INFO - current loss is: 
INFO - 1.947035312652588
INFO - current loss is: 
INFO - 2.6911532878875732
INFO - current loss is: 
INFO - 0.9810112714767456
INFO - current loss is: 
INFO - 1.4517428874969482
INFO - current loss is: 
INFO - 0.7191904187202454
INFO - current loss is: 
INFO - 1.2099642753601074
INFO - current loss is: 
INFO - 0.9977793097496033
INFO - current loss is: 
INFO - 2.0521249771118164
INFO - current loss is: 
INFO - 2.1611409187316895
INFO - current loss is: 
INFO - 1.7701244354248047
INFO - current loss is: 
INFO - 1.3790647983551025
INFO - current loss is: 
INFO - 0.7070174813270569
INFO - current loss is: 
INFO - 1.2725216150283813
INFO - current loss is: 
INFO - 5.864377021789551
INFO - current loss is: 
INFO - 5.289178371429443
INFO - current loss is: 
INFO - 3.2754416465759277
INFO - current loss is: 
INFO - 2.0928328037261963
INFO - current loss is: 
INFO - 3.007404327392578
INFO - current loss is: 
INFO - 4.902402877807617
INFO - current loss is: 
INFO - 4.816028594970703
INFO - current loss is: 
INFO - 1.4046440124511719
INFO - current loss is: 
INFO - 1.303820252418518
INFO - current loss is: 
INFO - 0.9019870162010193
INFO - current loss is: 
INFO - 3.20337176322937
INFO - SimpleRnn: epoch num: 
INFO - 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.0036078691482544
INFO - current loss is: 
INFO - 2.6316347122192383
INFO - current loss is: 
INFO - 1.906290888786316
INFO - current loss is: 
INFO - 1.9717341661453247
INFO - current loss is: 
INFO - 0.7852426767349243
INFO - current loss is: 
INFO - 3.206718921661377
INFO - current loss is: 
INFO - 0.7982613444328308
INFO - current loss is: 
INFO - 1.5538084506988525
INFO - current loss is: 
INFO - 0.5894061923027039
INFO - current loss is: 
INFO - 0.8208316564559937
INFO - current loss is: 
INFO - 1.3962799310684204
INFO - current loss is: 
INFO - 0.9030816555023193
INFO - current loss is: 
INFO - 1.2532706260681152
INFO - current loss is: 
INFO - 1.7440389394760132
INFO - current loss is: 
INFO - 2.7674038410186768
INFO - current loss is: 
INFO - 1.8396353721618652
INFO - current loss is: 
INFO - 1.2099672555923462
INFO - current loss is: 
INFO - 3.0640957355499268
INFO - current loss is: 
INFO - 1.3868733644485474
INFO - current loss is: 
INFO - 3.979677677154541
INFO - current loss is: 
INFO - 1.8927687406539917
INFO - current loss is: 
INFO - 0.9498004913330078
INFO - current loss is: 
INFO - 1.5008572340011597
INFO - current loss is: 
INFO - 1.5706325769424438
INFO - current loss is: 
INFO - 0.39245063066482544
INFO - current loss is: 
INFO - 1.8650537729263306
INFO - current loss is: 
INFO - 3.043102264404297
INFO - current loss is: 
INFO - 11.134414672851562
INFO - SimpleRnn: epoch num: 
INFO - 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.4887988567352295
INFO - current loss is: 
INFO - 2.5632243156433105
INFO - current loss is: 
INFO - 3.478804588317871
INFO - current loss is: 
INFO - 3.9620132446289062
INFO - current loss is: 
INFO - 4.00976037979126
INFO - current loss is: 
INFO - 8.502158164978027
INFO - current loss is: 
INFO - 3.262160301208496
INFO - current loss is: 
INFO - 1.3557459115982056
INFO - current loss is: 
INFO - 2.5672483444213867
INFO - current loss is: 
INFO - 7.771803855895996
INFO - current loss is: 
INFO - 12.168807029724121
INFO - current loss is: 
INFO - 8.278768539428711
INFO - current loss is: 
INFO - 6.712618350982666
INFO - current loss is: 
INFO - 2.8310952186584473
INFO - current loss is: 
INFO - 2.3416311740875244
INFO - current loss is: 
INFO - 5.112723350524902
INFO - current loss is: 
INFO - 4.059730529785156
INFO - current loss is: 
INFO - 9.770009994506836
INFO - current loss is: 
INFO - 4.579315185546875
INFO - current loss is: 
INFO - 2.7077832221984863
INFO - current loss is: 
INFO - 2.2970306873321533
INFO - current loss is: 
INFO - 1.6467843055725098
INFO - current loss is: 
INFO - 0.7475733160972595
INFO - current loss is: 
INFO - 1.561635971069336
INFO - current loss is: 
INFO - 2.4342916011810303
INFO - current loss is: 
INFO - 4.22650146484375
INFO - current loss is: 
INFO - 1.3725227117538452
INFO - current loss is: 
INFO - 5.270330429077148
INFO - all epochs loss are: 
INFO - [671.1591382026672, 12.734917734350477, 9.40988842504365, 3.782204087291445, 4.349092662334442, 7.597764675106321, 4.904557066304343, 5.9361116119793484, 5.375368233237948, 4.594229538525854, 5.7598223728793005, 2.4794906122343883, 3.1168170145579746, 2.114208832383156, 3.9031865490334376, 3.4648483267852237, 4.095430440136364, 2.4841397404670715, 2.0414622021572932, 4.217173965913909]
INFO - train_total_time: 
INFO - -1104.4442780017853
INFO - optimizer_step_time: 
INFO - [0.001997709274291992, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0009982585906982422, 0.0, 0.00099945068359375, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0010013580322265625, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0009982585906982422, 0.0, 0.0, 0.0010008811950683594, 0.0009949207305908203, 0.000997304916381836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010008811950683594, 0.0, 0.0, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.0, 0.001001119613647461, 0.0, 0.0, 0.000982522964477539, 0.0009992122650146484, 0.0010004043579101562, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0, 0.0010008811950683594, 0.0, 0.001001119613647461, 0.0, 0.0009989738464355469, 0.0, 0.0009989738464355469, 0.0010008811950683594, 0.0, 0.0, 0.0010001659393310547, 0.001001596450805664, 0.0, 0.0009992122650146484, 0.0010006427764892578, 0.001020669937133789, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0009984970092773438, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0009975433349609375, 0.00099945068359375, 0.0009996891021728516, 0.0009996891021728516, 0.0, 0.0010004043579101562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0010004043579101562, 0.0009989738464355469, 0.0, 0.0, 0.0010001659393310547, 0.001003265380859375, 0.000993967056274414, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0010006427764892578, 0.000995635986328125, 0.0, 0.000997304916381836, 0.0, 0.0009992122650146484, 0.0010035037994384766, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.0010006427764892578, 0.0009989738464355469, 0.0009996891021728516, 0.0, 0.0, 0.0010001659393310547, 0.0009984970092773438, 0.0010001659393310547, 0.0009996891021728516, 0.0, 0.0010006427764892578, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010006427764892578, 0.0009987354278564453, 0.0, 0.001020193099975586, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0009891986846923828, 0.0, 0.0, 0.0, 0.0009987354278564453, 0.0, 0.0009999275207519531, 0.0, 0.000997304916381836, 0.0, 0.0, 0.0, 0.0, 0.0010006427764892578, 0.0, 0.0, 0.0009992122650146484, 0.00099945068359375, 0.0, 0.0009930133819580078, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0009987354278564453, 0.0, 0.0, 0.0010030269622802734, 0.0009949207305908203, 0.0010001659393310547, 0.0, 0.00099945068359375, 0.0009989738464355469, 0.0, 0.0009992122650146484, 0.0, 0.0009992122650146484, 0.0010001659393310547, 0.0, 0.0009992122650146484, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.001001119613647461, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0009989738464355469, 0.000997304916381836, 0.0010023117065429688, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0010001659393310547, 0.0009982585906982422, 0.0010006427764892578, 0.0009989738464355469, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.001001119613647461, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0010051727294921875, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0009980201721191406, 0.0010001659393310547, 0.0010025501251220703, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.00099945068359375, 0.0, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0010004043579101562, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0010001659393310547, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0010013580322265625, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0010001659393310547, 0.0009996891021728516, 0.0, 0.0, 0.0009996891021728516, 0.0009989738464355469, 0.0, 0.0009989738464355469, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0, 0.0009989738464355469, 0.0009987354278564453, 0.0, 0.0, 0.0, 0.0009989738464355469, 0.00099945068359375, 0.0009999275207519531, 0.0, 0.0, 0.0009996891021728516, 0.0010013580322265625, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010023117065429688, 0.0, 0.0, 0.0, 0.0, 0.0010039806365966797, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.000997781753540039, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.000997781753540039, 0.0010001659393310547, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0009980201721191406, 0.0010006427764892578, 0.0010018348693847656, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009982585906982422, 0.0, 0.0, 0.0, 0.0009982585906982422, 0.0009987354278564453, 0.0, 0.0, 0.0, 0.0009963512420654297, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0009989738464355469, 0.0010008811950683594, 0.0, 0.0, 0.0, 0.0, 0.000993967056274414, 0.0, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.001005411148071289, 0.0, 0.0, 0.0, 0.0009799003601074219, 0.0010027885437011719, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0009992122650146484, 0.00099945068359375, 0.0, 0.0009996891021728516, 0.0, 0.0010006427764892578, 0.0, 0.0, 0.000997304916381836, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.000997304916381836, 0.0010013580322265625, 0.0010039806365966797, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0009975433349609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010027885437011719, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.001001119613647461, 0.0010030269622802734, 0.0, 0.00099945068359375, 0.0, 0.0009999275207519531, 0.0009996891021728516, 0.0, 0.0010006427764892578, 0.0, 0.0010116100311279297, 0.0, 0.0009992122650146484, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0010008811950683594, 0.0009980201721191406, 0.00099945068359375, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0010018348693847656, 0.001001119613647461, 0.0010006427764892578, 0.001001119613647461, 0.0010018348693847656, 0.0, 0.0, 0.001001119613647461, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0009999275207519531, 0.00099945068359375, 0.0010035037994384766, 0.001001596450805664, 0.0009992122650146484, 0.00099945068359375, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0009989738464355469, 0.0, 0.0, 0.0010018348693847656, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.001001119613647461, 0.001001119613647461, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.002001523971557617, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.00099945068359375, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0009992122650146484, 0.0010001659393310547, 0.0, 0.0010004043579101562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0009982585906982422]
INFO - backward_step_time: 
INFO - [0.006995201110839844, 0.01299142837524414, 0.022984743118286133, 0.025989532470703125, 0.040975093841552734, 0.038977861404418945, 0.05596804618835449, 0.060965776443481445, 0.06496167182922363, 0.06995940208435059, 0.10145068168640137, 0.09594321250915527, 0.09094667434692383, 0.10693812370300293, 0.10093951225280762, 0.10993528366088867, 0.15790939331054688, 0.16890192031860352, 0.23786306381225586, 0.1868910789489746, 0.13892030715942383, 0.14492011070251465, 0.15791082382202148, 0.1788935661315918, 0.17342543601989746, 0.17989706993103027, 0.1799025535583496, 0.18189525604248047, 0.17838406562805176, 0.18788862228393555, 0.19688725471496582, 0.19988465309143066, 0.21387696266174316, 0.2688419818878174, 0.23126697540283203, 0.23584604263305664, 0.23088741302490234, 0.2392287254333496, 0.23098516464233398, 0.29083800315856934, 0.27220630645751953, 0.2699103355407715, 0.26193881034851074, 0.2781181335449219, 0.2788398265838623, 0.2852632999420166, 0.3019852638244629, 0.2888336181640625, 0.2950775623321533, 0.31511521339416504, 0.309903621673584, 0.32607078552246094, 0.3368840217590332, 0.3418161869049072, 0.3448028564453125, 0.34380173683166504, 0.3927738666534424, 0.4597353935241699, 0.39577698707580566, 0.39220261573791504, 0.4067649841308594, 0.41564249992370605, 0.4971499443054199, 0.46570682525634766, 0.4202432632446289, 0.40176820755004883, 0.4127624034881592, 0.41475915908813477, 0.4277515411376953, 0.44274210929870605, 0.4527406692504883, 0.4527397155761719, 0.5007119178771973, 0.47072482109069824, 0.46673130989074707, 0.46872806549072266, 0.5067071914672852, 0.49971437454223633, 0.5206975936889648, 0.5077075958251953, 0.5277001857757568, 0.5027103424072266, 0.5137088298797607, 0.5316903591156006, 0.6260783672332764, 0.6336863040924072, 0.766730546951294, 0.6593601703643799, 0.5477943420410156, 0.5596771240234375, 0.5546822547912598, 0.5876610279083252, 0.5866591930389404, 0.6216421127319336, 0.6116504669189453, 0.6116416454315186, 0.6056511402130127, 0.6136481761932373, 0.6216437816619873, 0.6226441860198975, 0.7065937519073486, 0.6106505393981934, 0.6296360492706299, 0.6286358833312988, 0.6356182098388672, 0.6656143665313721, 0.6636130809783936, 0.6496267318725586, 0.6626214981079102, 0.6806073188781738, 0.6687140464782715, 0.7145893573760986, 1.0958747863769531, 0.7514979839324951, 0.7057416439056396, 0.7396364212036133, 0.7435715198516846, 0.7055933475494385, 0.731579065322876, 0.7315783500671387, 0.7425713539123535, 0.7455682754516602, 0.7665586471557617, 0.7915425300598145, 0.8105349540710449, 0.8045361042022705, 0.9625339508056641, 0.9335916042327881, 0.8220396041870117, 1.175163745880127, 1.102921724319458, 0.8456099033355713, 0.9271132946014404, 0.9520001411437988, 1.062389612197876, 0.9314634799957275, 0.8695008754730225, 0.8684985637664795, 0.8595056533813477, 0.8984811305999756, 0.8944888114929199, 0.9454545974731445, 0.901482343673706, 0.9124755859375, 0.9124751091003418, 0.9204704761505127, 1.0384001731872559, 1.1089873313903809, 1.0653960704803467, 0.9724397659301758, 1.00050950050354, 1.3547699451446533, 0.9434559345245361, 0.957449197769165, 1.046403169631958, 1.0224111080169678, 0.9724397659301758, 1.0084176063537598, 0.9894304275512695, 1.0550458431243896, 1.0523836612701416, 1.1643338203430176, 2.044264078140259, 1.1139612197875977, 1.1128818988800049, 1.3615009784698486, 1.0906004905700684, 1.1089167594909668, 1.196727991104126, 1.5251531600952148, 1.1043641567230225, 1.077380657196045, 1.078378438949585, 1.1202583312988281, 1.101370096206665, 1.1993134021759033, 1.195214033126831, 1.2712743282318115, 1.4157130718231201, 1.2830379009246826, 1.2250161170959473, 1.2510807514190674, 1.1517457962036133, 1.1493792533874512, 1.178227186203003, 1.169605016708374, 1.1639378070831299, 1.1739239692687988, 1.205021619796753, 1.2109870910644531, 1.2941339015960693, 1.3326728343963623, 1.3071300983428955, 1.2975842952728271, 1.3937463760375977, 1.242863655090332, 1.225724220275879, 1.232813835144043, 1.3088293075561523, 1.2450265884399414, 1.396716833114624, 1.593113660812378, 1.4978857040405273, 1.4409418106079102, 1.3162517547607422, 1.3260774612426758, 1.405909538269043, 1.4110462665557861, 1.426809549331665, 1.3652925491333008, 1.3457796573638916, 1.421781063079834, 1.499274492263794, 1.3150439262390137, 1.3401646614074707, 1.4629948139190674, 1.3959705829620361, 1.3790085315704346, 1.534698247909546, 1.4641551971435547, 1.4112322330474854, 1.4159610271453857, 1.4379541873931885, 1.5629754066467285, 1.5331199169158936, 1.6225855350494385, 1.4897048473358154, 1.5461757183074951, 1.6636364459991455, 1.7240064144134521, 1.809995412826538, 1.7246668338775635, 1.706089735031128, 1.6506156921386719, 1.5721101760864258, 1.5968163013458252, 1.5671007633209229, 1.6950223445892334, 1.550107717514038, 1.550658941268921, 2.6987366676330566, 2.0985562801361084, 1.5300054550170898, 1.7885029315948486, 1.9695653915405273, 1.5431373119354248, 1.560187578201294, 1.4741520881652832, 1.5371148586273193, 1.533118486404419, 1.4861445426940918, 1.6710376739501953, 1.4824724197387695, 1.4901447296142578, 1.5601096153259277, 1.6513824462890625, 1.7050933837890625, 1.715031385421753, 1.9309022426605225, 1.8901700973510742, 1.7140116691589355, 1.598092794418335, 1.60807466506958, 1.6700387001037598, 1.5221266746520996, 1.5581040382385254, 1.5520884990692139, 1.5700960159301758, 1.5900912284851074, 1.8499729633331299, 1.7949817180633545, 1.6200652122497559, 1.5954599380493164, 1.772157907485962, 1.813084363937378, 1.6020584106445312, 1.6510505676269531, 1.7330079078674316, 1.6880507469177246, 1.6470556259155273, 1.6730625629425049, 1.7789866924285889, 1.6480517387390137, 1.705016851425171, 1.6650424003601074, 1.7415080070495605, 1.8941750526428223, 1.8799183368682861, 1.75700044631958, 1.7110188007354736, 1.687028408050537, 1.6840286254882812, 1.7569916248321533, 1.742995262145996, 1.7250125408172607, 1.750992774963379, 1.7709805965423584, 1.827948808670044, 1.735004186630249, 1.8216843605041504, 1.9039037227630615, 1.8289554119110107, 1.8919105529785156, 1.8611717224121094, 1.7609870433807373, 1.8029627799987793, 1.7999622821807861, 1.7849717140197754, 1.831953525543213, 1.9118969440460205, 2.079324245452881, 1.980907917022705, 1.8569302558898926, 1.806960105895996, 1.822951316833496, 1.824951410293579, 1.8039631843566895, 1.8969082832336426, 2.2158849239349365, 2.059966564178467, 1.932887315750122, 1.9828588962554932, 1.932887077331543, 1.9009056091308594, 1.9928531646728516, 2.067324161529541, 2.03482985496521, 2.066333055496216, 1.9918537139892578, 1.9628772735595703, 2.3956265449523926, 1.9838578701019287, 1.8819167613983154, 1.9548749923706055, 1.922893762588501, 2.045823097229004, 1.9798409938812256, 1.9348866939544678, 1.9708638191223145, 2.370638132095337, 2.015841007232666, 1.9958491325378418, 2.1121363639831543, 2.1547605991363525, 2.1667661666870117, 2.099799871444702, 2.3362019062042236, 2.1148977279663086, 2.366666078567505, 2.014840841293335, 2.066810131072998, 2.049818515777588, 2.0028443336486816, 2.075805187225342, 2.045802116394043, 2.059813976287842, 2.3866324424743652, 2.2806968688964844, 2.254709243774414, 2.3561606407165527, 2.144177198410034, 2.26470947265625, 2.158757448196411, 2.2457051277160645, 2.4295811653137207, 2.2746918201446533, 2.3967769145965576, 2.2637035846710205, 2.3311784267425537, 3.04424786567688, 2.178745985031128, 2.2027313709259033, 2.2097277641296387, 2.192739963531494, 2.4511475563049316, 2.1537609100341797, 2.161773920059204, 2.277688503265381, 2.1578266620635986, 2.1957359313964844, 2.320665121078491, 2.1877408027648926, 2.234717845916748, 2.1877434253692627, 2.340653657913208, 2.2277181148529053, 2.200714588165283, 2.2417099475860596, 2.223722457885742, 2.2557013034820557, 2.552050828933716, 2.410520315170288, 2.325646162033081, 2.3276591300964355, 3.238142967224121, 2.6100847721099854, 3.0354182720184326, 2.5985074043273926, 2.2836833000183105, 2.3906304836273193, 2.338653564453125, 2.3096680641174316, 2.308670997619629, 2.4435946941375732, 2.350646495819092, 2.3586490154266357, 2.3806097507476807, 2.329660415649414, 2.3346357345581055, 2.388624429702759, 2.3916239738464355, 2.3696370124816895, 2.320646286010742, 2.4895670413970947, 2.4056148529052734, 2.5807197093963623, 2.419607400894165, 2.348649501800537, 2.4196081161499023, 2.383607864379883, 2.455566167831421, 2.4925456047058105, 2.439598321914673, 2.4286022186279297, 2.411612033843994, 2.4196066856384277, 2.5025625228881836, 2.4405953884124756, 2.4016194343566895, 2.4995622634887695, 2.451589822769165, 2.4355993270874023, 2.49656343460083, 2.454587459564209, 2.4955644607543945, 2.6454780101776123, 2.5605289936065674, 2.4895665645599365, 2.5125534534454346, 2.4775750637054443, 2.6314823627471924, 2.582515001296997, 2.5335421562194824, 2.5895090103149414, 2.622488498687744, 2.559525966644287, 2.571519136428833, 2.5914902687072754, 2.7084391117095947, 2.560577869415283, 2.5745186805725098, 3.079226493835449, 2.6244919300079346, 2.61749529838562, 2.5455358028411865, 2.5935070514678955, 2.702446937561035, 2.664466619491577, 2.665464401245117, 2.7084219455718994, 2.632485866546631, 2.6204941272735596, 2.643479347229004, 2.6514720916748047, 2.735426425933838, 2.6334853172302246, 2.73042893409729, 2.722433090209961, 2.6824567317962646, 2.7134182453155518, 2.8623523712158203, 2.7024455070495605, 2.9747705459594727, 2.8054637908935547, 2.726433515548706, 2.7604124546051025, 2.7434215545654297, 2.9003312587738037, 2.9023430347442627, 2.8083853721618652, 2.785398244857788, 2.8463633060455322, 2.902329921722412, 2.8243746757507324, 2.799386739730835, 2.765408992767334, 2.8393661975860596, 2.792393445968628, 2.926316499710083, 3.207155466079712, 2.9133238792419434, 2.9902822971343994, 2.9822821617126465, 2.942307472229004, 2.9782874584198, 2.953303813934326, 2.9732906818389893, 3.01226806640625, 2.972289562225342, 3.0282578468322754, 3.0352542400360107, 3.0042717456817627, 3.030256748199463, 3.816805124282837, 3.074211359024048, 3.0252599716186523, 3.0162646770477295, 3.1581833362579346, 3.032254934310913, 3.091221570968628, 3.3480732440948486, 3.7668368816375732, 3.0372519493103027, 3.1751718521118164, 3.5179779529571533, 3.246641159057617, 3.3650734424591064, 3.267564535140991, 3.2691195011138916, 4.202582597732544, 3.4829964637756348, 3.3126378059387207, 3.1311981678009033, 3.854883909225464, 3.814602851867676, 4.10514497756958, 3.5359644889831543, 3.315091848373413, 3.2581348419189453, 3.37007474899292, 3.751857042312622, 3.248138189315796, 3.493009090423584, 3.387051582336426, 3.2481307983398438, 3.337080240249634, 3.412036895751953, 3.393202781677246, 3.2161498069763184, 3.2621231079101562, 3.365086555480957, 3.6988890171051025, 3.367065906524658, 4.13163685798645, 3.348072052001953, 3.848790407180786, 3.412071466445923, 3.380053758621216, 3.3850505352020264, 3.2601242065429688, 3.273120403289795, 3.6169190406799316, 3.4462952613830566, 3.5409634113311768, 3.34708309173584, 3.60292911529541, 3.534044027328491, 3.571838140487671, 3.6758852005004883, 3.4958531856536865]
INFO - optimizer_zero_grad_time: 
INFO - [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009922981262207031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000997304916381836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009775161743164062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009982585906982422, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009982585906982422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009984970092773438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010018348693847656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009953975677490234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009987354278564453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516]
INFO - epoch_step_time: 
INFO - [11.086935997009277, 13.75972318649292, 19.23668599128723, 24.995750188827515, 31.955259561538696, 36.39062690734863, 40.498040199279785, 45.445969104766846, 53.46093034744263, 52.38052296638489, 55.18311882019043, 60.92829251289368, 66.11672592163086, 70.31936144828796, 74.1258192062378, 75.71345639228821, 81.30286574363708, 88.18379163742065, 100.52515053749084, 102.83425569534302]
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 599, in <module>
    
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 497, in HyperParameter_Optimizations
    predictad_value_list = []
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 416, in RunNetworkArch
    
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 240, in PredictSimpleRnn
    model = RnnSimpleModel(input_size, hidden_size, output_size)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 185, in Evaluate
    
TypeError: string indices must be integers
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:35:55
INFO - ******************MLNX*******************
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:36:21
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 248, in PredictSimpleRnn
    loss_fn = GeneralModelFn.loss_fn
AttributeError: type object 'GeneralModelFn' has no attribute 'loss_fn'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:39:13
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
features value
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 185, in Evaluate
    data_batch, labels_batch = data_batch['features'], labels_batch['value']
TypeError: string indices must be integers
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:40:48
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([[45.7000, 46.3500],
        [46.3500, 47.0500],
        [47.0500, 47.7000],
        [47.7000, 47.1500],
        [47.1500, 47.4000],
        [47.4000, 47.1500],
        [47.1500, 47.8000],
        [47.8000, 48.0000],
        [48.0000, 45.8000],
        [45.8000, 46.1500],
        [46.1500, 47.1000],
        [47.1000, 47.5000],
        [47.5000, 48.1500],
        [48.1500, 48.1500],
        [48.1500, 47.9000],
        [47.9000, 48.0500],
        [48.0500, 47.9500],
        [47.9500, 47.5000],
        [47.5000, 47.9500],
        [47.9500, 47.7000],
        [47.7000, 47.7000],
        [47.7000, 46.9000],
        [46.9000, 46.4000],
        [46.4000, 46.6500],
        [46.6500, 45.1000],
        [45.1000, 45.1500],
        [45.1500, 46.0000],
        [46.0000, 44.8000],
        [44.8000, 43.9000],
        [43.9000, 43.5000],
        [43.5000, 43.8000],
        [43.8000, 43.6000],
        [43.6000, 44.2500],
        [44.2500, 44.0500],
        [44.0500, 44.1000],
        [44.1000, 43.8000],
        [43.8000, 43.2500],
        [43.2500, 43.8500],
        [43.8500, 42.5000],
        [42.5000, 43.3000],
        [43.3000, 42.5000],
        [42.5000, 43.0500],
        [43.0500, 42.9000],
        [42.9000, 43.6000],
        [43.6000, 43.8500],
        [43.8500, 44.3500],
        [44.3500, 45.0000],
        [45.0000, 44.8000],
        [44.8000, 45.0000],
        [45.0000, 45.0500],
        [45.0500, 44.7000],
        [44.7000, 44.9500],
        [44.9500, 44.6000],
        [44.6000, 44.5000],
        [44.5000, 44.9500],
        [44.9500, 45.3000],
        [45.3000, 44.7500],
        [44.7500, 45.7500],
        [45.7500, 47.0500],
        [47.0500, 46.9500],
        [46.9500, 47.2000],
        [47.2000, 46.7500],
        [46.7500, 46.0000],
        [46.0000, 45.2000]], dtype=torch.float64) tensor([[47.0500],
        [47.7000],
        [47.1500],
        [47.4000],
        [47.1500],
        [47.8000],
        [48.0000],
        [45.8000],
        [46.1500],
        [47.1000],
        [47.5000],
        [48.1500],
        [48.1500],
        [47.9000],
        [48.0500],
        [47.9500],
        [47.5000],
        [47.9500],
        [47.7000],
        [47.7000],
        [46.9000],
        [46.4000],
        [46.6500],
        [45.1000],
        [45.1500],
        [46.0000],
        [44.8000],
        [43.9000],
        [43.5000],
        [43.8000],
        [43.6000],
        [44.2500],
        [44.0500],
        [44.1000],
        [43.8000],
        [43.2500],
        [43.8500],
        [42.5000],
        [43.3000],
        [42.5000],
        [43.0500],
        [42.9000],
        [43.6000],
        [43.8500],
        [44.3500],
        [45.0000],
        [44.8000],
        [45.0000],
        [45.0500],
        [44.7000],
        [44.9500],
        [44.6000],
        [44.5000],
        [44.9500],
        [45.3000],
        [44.7500],
        [45.7500],
        [47.0500],
        [46.9500],
        [47.2000],
        [46.7500],
        [46.0000],
        [45.2000],
        [45.1000]], dtype=torch.float64)
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 196, in Evaluate
    output_batch = model(data_batch)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 37, in forward
    out, self.h_0 = self.rnn(x, self.h_0)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\rnn.py", line 192, in forward
    output, hidden = func(input, self.all_weights, hx, batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 324, in forward
    return func(input, *fargs, **fkwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 244, in forward
    nexth, output = func(input, hidden, weight, batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 87, in forward
    hy, output = inner(input, hidden[l], weight[l], batch_sizes)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 116, in forward
    hidden = inner(input[i], hidden, *weight)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\_functions\rnn.py", line 17, in RNNReLUCell
    hy = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1024, in linear
    return torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of type torch.FloatTensor but found type torch.DoubleTensor for argument #4 'mat1'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:41:26
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 196, in Evaluate
    loss = loss_fn(output_batch, labels_batch)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\GeneralModelFn.py", line 33, in loss_fn
    return -torch.sum(outputs[range(num_examples), labels])/num_examples
RuntimeError: tensors used as indices must be long or byte tensors
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:42:10
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([[[28.6246],
         [41.8739],
         [46.6681],
         [47.9508],
         [48.1511],
         [48.2806],
         [48.6012],
         [48.9622],
         [48.2725],
         [47.5680],
         [47.7924],
         [48.3164],
         [48.8564],
         [49.1943],
         [49.2014],
         [49.1886],
         [49.1891],
         [48.9935],
         [48.9799],
         [49.0006],
         [48.9457],
         [48.6089],
         [48.0826],
         [47.8673],
         [47.2680],
         [46.6699],
         [46.8072],
         [46.6342],
         [45.8981],
         [45.2300],
         [45.0252],
         [44.9766],
         [45.1630],
         [45.3159],
         [45.3311],
         [45.2244],
         [44.8935],
         [44.8680],
         [44.5001],
         [44.3280],
         [44.1668],
         [44.1239],
         [44.1929],
         [44.4541],
         [44.8254],
         [45.2134],
         [45.7265],
         [45.9949],
         [46.1048],
         [46.2087],
         [46.1254],
         [46.1005],
         [46.0231],
         [45.8678],
         [45.9612],
         [46.2554],
         [46.2377],
         [46.4651],
         [47.3160],
         [47.9277],
         [48.1941],
         [48.1560],
         [47.7304],
         [47.0732]]], grad_fn=<ThAddBackward>)
tensor([[47.0500],
        [47.7000],
        [47.1500],
        [47.4000],
        [47.1500],
        [47.8000],
        [48.0000],
        [45.8000],
        [46.1500],
        [47.1000],
        [47.5000],
        [48.1500],
        [48.1500],
        [47.9000],
        [48.0500],
        [47.9500],
        [47.5000],
        [47.9500],
        [47.7000],
        [47.7000],
        [46.9000],
        [46.4000],
        [46.6500],
        [45.1000],
        [45.1500],
        [46.0000],
        [44.8000],
        [43.9000],
        [43.5000],
        [43.8000],
        [43.6000],
        [44.2500],
        [44.0500],
        [44.1000],
        [43.8000],
        [43.2500],
        [43.8500],
        [42.5000],
        [43.3000],
        [42.5000],
        [43.0500],
        [42.9000],
        [43.6000],
        [43.8500],
        [44.3500],
        [45.0000],
        [44.8000],
        [45.0000],
        [45.0500],
        [44.7000],
        [44.9500],
        [44.6000],
        [44.5000],
        [44.9500],
        [45.3000],
        [44.7500],
        [45.7500],
        [47.0500],
        [46.9500],
        [47.2000],
        [46.7500],
        [46.0000],
        [45.2000],
        [45.1000]])
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 198, in Evaluate
    loss = loss_fn(output_batch, labels_batch)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\GeneralModelFn.py", line 33, in loss_fn
    return -torch.sum(outputs[range(num_examples), labels])/num_examples
RuntimeError: tensors used as indices must be long or byte tensors
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:43:19
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 200, in Evaluate
    loss = loss_fn(output_batch, labels_batch)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\GeneralModelFn.py", line 33, in loss_fn
    return -torch.sum(outputs[range(num_examples), labels])/num_examples
RuntimeError: tensors used as indices must be long or byte tensors
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:44:45
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([29.2329, 42.0468, 46.7043, 47.9334, 48.1411, 48.2771, 48.5983, 48.9608,
        48.2723, 47.5678, 47.7924, 48.3164, 48.8564, 49.1943, 49.2014, 49.1886,
        49.1891, 48.9935, 48.9799, 49.0006, 48.9457, 48.6089, 48.0826, 47.8673,
        47.2680, 46.6699, 46.8072, 46.6342, 45.8981, 45.2300, 45.0252, 44.9766,
        45.1630, 45.3159, 45.3311, 45.2244, 44.8935, 44.8680, 44.5001, 44.3280,
        44.1668, 44.1239, 44.1929, 44.4541, 44.8254, 45.2134, 45.7265, 45.9949,
        46.1048, 46.2087, 46.1254, 46.1005, 46.0231, 45.8678, 45.9612, 46.2554,
        46.2377, 46.4651, 47.3160, 47.9277, 48.1941, 48.1560, 47.7304, 47.0732],
       grad_fn=<ThAddBackward>)
tensor([47.0500, 47.7000, 47.1500, 47.4000, 47.1500, 47.8000, 48.0000, 45.8000,
        46.1500, 47.1000, 47.5000, 48.1500, 48.1500, 47.9000, 48.0500, 47.9500,
        47.5000, 47.9500, 47.7000, 47.7000, 46.9000, 46.4000, 46.6500, 45.1000,
        45.1500, 46.0000, 44.8000, 43.9000, 43.5000, 43.8000, 43.6000, 44.2500,
        44.0500, 44.1000, 43.8000, 43.2500, 43.8500, 42.5000, 43.3000, 42.5000,
        43.0500, 42.9000, 43.6000, 43.8500, 44.3500, 45.0000, 44.8000, 45.0000,
        45.0500, 44.7000, 44.9500, 44.6000, 44.5000, 44.9500, 45.3000, 44.7500,
        45.7500, 47.0500, 46.9500, 47.2000, 46.7500, 46.0000, 45.2000, 45.1000])
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 203, in Evaluate
    loss = loss_fn(output_batch, labels_batch)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\GeneralModelFn.py", line 33, in loss_fn
    return -torch.sum(outputs[range(num_examples), labels])/num_examples
IndexError: too many indices for tensor of dimension 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:47:32
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([28.0931, 41.6544, 46.5710, 47.9679, 48.1307, 48.2700, 48.5955, 48.9607,
        48.2725, 47.5679, 47.7924, 48.3163, 48.8564, 49.1943, 49.2014, 49.1886,
        49.1891, 48.9935, 48.9798, 49.0006, 48.9457, 48.6089, 48.0826, 47.8673,
        47.2680, 46.6699, 46.8072, 46.6342, 45.8981, 45.2300, 45.0252, 44.9766,
        45.1630, 45.3159, 45.3311, 45.2244, 44.8935, 44.8680, 44.5001, 44.3280,
        44.1668, 44.1239, 44.1929, 44.4541, 44.8254, 45.2134, 45.7265, 45.9949,
        46.1048, 46.2087, 46.1254, 46.1005, 46.0231, 45.8678, 45.9612, 46.2554,
        46.2377, 46.4651, 47.3160, 47.9277, 48.1941, 48.1560, 47.7304, 47.0732],
       grad_fn=<ThAddBackward>)
tensor([47.0500, 47.7000, 47.1500, 47.4000, 47.1500, 47.8000, 48.0000, 45.8000,
        46.1500, 47.1000, 47.5000, 48.1500, 48.1500, 47.9000, 48.0500, 47.9500,
        47.5000, 47.9500, 47.7000, 47.7000, 46.9000, 46.4000, 46.6500, 45.1000,
        45.1500, 46.0000, 44.8000, 43.9000, 43.5000, 43.8000, 43.6000, 44.2500,
        44.0500, 44.1000, 43.8000, 43.2500, 43.8500, 42.5000, 43.3000, 42.5000,
        43.0500, 42.9000, 43.6000, 43.8500, 44.3500, 45.0000, 44.8000, 45.0000,
        45.0500, 44.7000, 44.9500, 44.6000, 44.5000, 44.9500, 45.3000, 44.7500,
        45.7500, 47.0500, 46.9500, 47.2000, 46.7500, 46.0000, 45.2000, 45.1000])
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 211, in Evaluate
    for metric in metrics}
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 211, in <dictcomp>
    for metric in metrics}
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\GeneralModelFn.py", line 44, in accuracy
    outputs = np.argmax(outputs, axis=1)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 963, in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\fromnumeric.py", line 57, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:53:02
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([27.8507, 41.7255, 46.5704, 47.8947, 48.1456, 48.2895, 48.6043, 48.9636,
        48.2734, 47.5680, 47.7925, 48.3164, 48.8564, 49.1943, 49.2014, 49.1886,
        49.1891, 48.9935, 48.9799, 49.0006, 48.9457, 48.6089, 48.0826, 47.8673,
        47.2680, 46.6699, 46.8072, 46.6342, 45.8981, 45.2300, 45.0252, 44.9766,
        45.1630, 45.3159, 45.3311, 45.2244, 44.8935, 44.8680, 44.5001, 44.3280,
        44.1668, 44.1239, 44.1929, 44.4541, 44.8254, 45.2134, 45.7265, 45.9949,
        46.1048, 46.2087, 46.1254, 46.1005, 46.0231, 45.8678, 45.9612, 46.2554,
        46.2377, 46.4651, 47.3160, 47.9277, 48.1941, 48.1560, 47.7304, 47.0732],
       grad_fn=<ThAddBackward>)
tensor([47.0500, 47.7000, 47.1500, 47.4000, 47.1500, 47.8000, 48.0000, 45.8000,
        46.1500, 47.1000, 47.5000, 48.1500, 48.1500, 47.9000, 48.0500, 47.9500,
        47.5000, 47.9500, 47.7000, 47.7000, 46.9000, 46.4000, 46.6500, 45.1000,
        45.1500, 46.0000, 44.8000, 43.9000, 43.5000, 43.8000, 43.6000, 44.2500,
        44.0500, 44.1000, 43.8000, 43.2500, 43.8500, 42.5000, 43.3000, 42.5000,
        43.0500, 42.9000, 43.6000, 43.8500, 44.3500, 45.0000, 44.8000, 45.0000,
        45.0500, 44.7000, 44.9500, 44.6000, 44.5000, 44.9500, 45.3000, 44.7500,
        45.7500, 47.0500, 46.9500, 47.2000, 46.7500, 46.0000, 45.2000, 45.1000])
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:213: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
tensor([46.6021, 46.2714, 46.0804, 45.6374, 45.1572, 45.1275, 45.2910, 45.4740,
        45.2816, 44.8573, 44.5185, 44.7188, 45.1505, 45.4046, 45.5412, 45.9205,
        46.8739, 47.6201, 47.9406, 47.8473, 47.5755, 47.5514, 47.8119, 47.8126,
        47.7933, 48.0299, 48.2462, 48.2531, 48.1547, 47.9958, 47.8006, 47.4289,
        47.0704, 47.1254, 47.4580, 47.5835, 48.0940, 48.6403, 48.6149, 48.9674,
        49.4919, 49.5335, 49.1752, 48.8660, 48.0553, 47.4777, 47.0149, 46.4314,
        46.0866, 45.7862, 45.2946, 44.7788, 44.5765, 44.7081, 45.0087, 45.7017,
        46.4639, 46.8323, 47.2007, 47.9982, 48.2437, 47.6915, 46.5453, 45.8315],
       grad_fn=<ThAddBackward>)
tensor([44.7000, 44.7500, 43.7500, 43.6000, 44.0500, 44.1500, 44.4000, 43.6000,
        43.2500, 43.0000, 43.9500, 44.2000, 44.3000, 44.4000, 45.2000, 46.7500,
        46.7500, 46.9500, 46.3500, 46.1500, 46.4500, 46.9000, 46.3500, 46.7000,
        47.1000, 47.1500, 46.9500, 46.8500, 46.6000, 46.4000, 45.7500, 45.6000,
        46.1500, 46.5500, 46.3000, 47.7000, 47.7000, 47.1500, 48.5000, 48.6000,
        48.1500, 47.5500, 47.5000, 45.7000, 46.1500, 45.1500, 44.7000, 44.6500,
        44.2000, 43.5000, 43.1000, 43.3000, 43.6500, 44.0500, 45.3000, 45.8000,
        45.7500, 46.4500, 47.7000, 46.7500, 45.8000, 44.0500, 44.4000, 46.0000])
tensor([46.3312, 47.0383, 48.8342, 50.1141, 50.2653, 50.2520, 50.1429, 49.9846,
        50.2449, 50.7768, 51.4849, 54.0194, 56.5645, 58.1810, 58.8139, 59.1564,
        59.0580, 59.3686, 60.6099, 60.8901, 60.1260, 59.9945, 61.0758, 62.1237,
        62.6858, 62.2650, 62.1881, 62.6850, 63.2508, 64.1262, 64.5876, 64.9256,
        65.2428, 65.2611, 65.2929, 65.3716, 65.4433, 65.5710, 65.7848, 65.9379,
        65.9872, 65.8828, 65.7859, 66.1980, 65.5875, 65.3476, 66.2682, 66.3774,
        65.7991, 65.7637, 66.2459, 67.0513, 67.4090, 67.1898, 67.2905, 67.2209,
        67.0374, 66.9473, 66.5325, 66.1479, 65.5101, 64.2880, 63.3923, 62.8751],
       grad_fn=<ThAddBackward>)
tensor([46.2000, 50.1000, 49.1500, 49.1000, 49.1000, 48.8000, 48.6500, 49.5500,
        50.0500, 51.0500, 56.5000, 57.1000, 58.3000, 57.8500, 58.6000, 57.5000,
        59.1000, 60.9000, 59.2500, 58.2500, 59.3500, 61.4000, 61.6500, 62.0500,
        60.3000, 61.7000, 62.0500, 62.7500, 64.0500, 63.6000, 64.4000, 64.4000,
        64.1500, 64.4000, 64.4000, 64.5000, 64.7000, 65.0000, 65.0000, 65.0000,
        64.7000, 64.7500, 65.8500, 63.1500, 64.9500, 66.3000, 64.7500, 64.3000,
        65.1000, 65.7500, 66.9000, 66.3500, 65.9000, 66.7000, 65.8500, 66.0000,
        65.8500, 64.9500, 64.9500, 63.6500, 61.9500, 61.9000, 61.3500, 59.3000])
tensor([61.7730, 61.4636, 62.1061, 62.4689, 62.8332, 63.6347, 63.5934, 63.5534,
        63.8236, 66.6647, 69.0529, 69.6757, 69.7743, 69.7317, 69.0220, 68.8746,
        69.3136, 69.7744, 71.1489, 72.5269, 73.1682, 73.5271, 73.3472, 73.5506,
        73.8919, 74.4818, 74.4282, 75.1376, 76.0893, 75.7913, 74.6628, 74.9804,
        74.6065, 72.8452, 72.7105, 72.7755, 73.1679, 73.9989, 74.5786, 74.6298,
        74.4915, 75.2982, 76.7362, 77.7772, 78.3435, 78.7825, 78.8400, 78.7729,
        78.5161, 77.6619, 76.5941, 75.7674, 75.9645, 77.8718, 79.2068, 79.4064,
        79.4826, 80.0034, 80.9283, 81.7182, 82.5605, 82.9410, 83.4440, 84.1281],
       grad_fn=<ThAddBackward>)
tensor([60.9000, 61.7000, 61.4500, 62.3000, 63.5000, 61.8500, 62.9500, 62.9500,
        70.0500, 68.7000, 69.1000, 68.8000, 68.8000, 66.9500, 68.4500, 68.6500,
        69.2500, 72.0000, 72.4500, 72.6000, 72.9500, 71.9500, 73.3000, 73.0500,
        74.4000, 72.8500, 75.8000, 75.6000, 74.1000, 72.6000, 75.4500, 72.1500,
        70.2000, 72.8500, 71.2000, 73.2000, 73.7000, 74.1000, 73.5000, 73.5500,
        75.7000, 77.1500, 77.5500, 77.9000, 78.3500, 77.8000, 77.9500, 77.2500,
        75.7500, 74.8000, 74.2500, 75.8000, 79.4000, 78.6500, 78.6000, 78.8000,
        79.9000, 81.0000, 81.4500, 82.6500, 82.1000, 83.4500, 83.8500, 83.6500])
tensor([84.4005, 84.6996, 84.6718, 84.9785, 86.2844, 86.5611, 86.2835, 86.1814,
        86.2632, 86.2706, 86.5480, 86.8163, 87.0118, 86.7462, 86.7869, 87.0793,
        88.0195, 88.6771, 88.1258, 87.1189, 86.3898, 86.2158, 86.4642, 86.8204,
        86.8488, 86.1194, 85.4259, 86.7681, 87.7648, 87.5575, 85.7867, 84.5917,
        83.7764, 84.0212, 84.7655, 85.8504, 86.0653, 86.1259, 86.5632, 86.1217,
        85.9001, 85.8295, 84.9963, 84.1770, 84.2577, 84.8888, 84.8804, 84.2240,
        84.0083, 84.0717, 83.7581, 83.7944, 83.5539, 82.5830, 81.1195, 80.0031,
        80.2114, 80.6973, 80.4874, 80.8155, 81.7753, 82.4683, 82.7180, 82.3010],
       grad_fn=<ThAddBackward>)
tensor([84.3500, 83.5500, 84.9000, 87.0500, 85.1000, 85.5500, 85.3000, 85.7000,
        85.3500, 86.3000, 86.1000, 86.5000, 85.4000, 86.5000, 86.4500, 88.6000,
        88.0000, 86.4500, 85.5000, 85.1500, 85.5000, 86.0000, 86.3500, 85.9000,
        84.3500, 84.3000, 88.3500, 86.8500, 86.5000, 82.5500, 83.7500, 81.8000,
        84.3500, 84.3000, 86.4500, 84.6500, 85.9000, 86.1500, 84.4000, 85.4500,
        84.7500, 83.1000, 82.9000, 83.9500, 84.7000, 83.6000, 82.7500, 83.4000,
        83.2500, 82.4500, 83.4000, 82.1000, 80.7000, 78.8000, 78.5000, 80.1500,
        80.0000, 79.1500, 80.8500, 81.8500, 82.0500, 82.0000, 80.8000, 80.8000])
tensor([81.8339, 81.4882, 80.9619, 80.5423, 80.5116, 80.3514, 80.3347, 81.0369,
        81.4028, 81.9938, 82.7909, 83.1978, 83.0013, 82.6565, 83.1228, 82.4694,
        80.8136, 79.1115, 77.6810, 77.0718, 77.0625, 77.6009, 78.0567, 78.0939,
        78.0148, 78.2492, 78.6450, 79.0782, 78.6614, 78.0434, 77.7257, 75.9229,
        74.3290, 74.1510, 74.3230, 75.1916, 76.6698, 77.1124, 76.2578, 75.0575,
        73.7063, 72.2790, 71.3452, 71.7195, 72.4380, 73.2616, 73.6748, 73.0132,
        72.1295, 72.2551, 72.4583, 70.2656, 70.6857, 76.3878],
       grad_fn=<ThAddBackward>)
tensor([80.3000, 79.5500, 79.4500, 79.8000, 79.1500, 79.7000, 81.1500, 80.4500,
        82.1500, 82.5500, 82.6000, 81.7500, 81.6500, 83.2000, 80.0000, 78.5500,
        76.7000, 75.7000, 76.0000, 76.2500, 77.4500, 77.3500, 77.1500, 77.1000,
        77.8000, 78.1000, 78.6500, 76.8500, 76.9000, 76.6000, 72.4000, 72.8000,
        73.4500, 73.4500, 75.5300, 77.1800, 75.9000, 74.2900, 73.1400, 71.4600,
        70.0600, 69.8400, 71.7200, 71.8900, 73.2700, 72.7400, 71.0800, 70.5800,
        71.9900, 71.3400, 65.9800, 72.7700, 83.3000, 80.7100])
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 609, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 507, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 426, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 215, in Evaluate
    logging.INFO("evaluation_summary: ")
TypeError: 'int' object is not callable
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 07:54:23
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([29.2847, 42.2077, 46.6907, 47.9473, 48.1815, 48.2915, 48.6048, 48.9616,
        48.2726, 47.5680, 47.7925, 48.3164, 48.8564, 49.1943, 49.2014, 49.1886,
        49.1891, 48.9935, 48.9798, 49.0006, 48.9457, 48.6089, 48.0826, 47.8673,
        47.2680, 46.6699, 46.8072, 46.6342, 45.8981, 45.2300, 45.0252, 44.9766,
        45.1630, 45.3159, 45.3311, 45.2244, 44.8935, 44.8680, 44.5001, 44.3280,
        44.1668, 44.1239, 44.1929, 44.4541, 44.8254, 45.2134, 45.7265, 45.9949,
        46.1048, 46.2087, 46.1254, 46.1005, 46.0231, 45.8678, 45.9612, 46.2554,
        46.2377, 46.4651, 47.3160, 47.9277, 48.1941, 48.1560, 47.7304, 47.0732],
       grad_fn=<ThAddBackward>)
tensor([47.0500, 47.7000, 47.1500, 47.4000, 47.1500, 47.8000, 48.0000, 45.8000,
        46.1500, 47.1000, 47.5000, 48.1500, 48.1500, 47.9000, 48.0500, 47.9500,
        47.5000, 47.9500, 47.7000, 47.7000, 46.9000, 46.4000, 46.6500, 45.1000,
        45.1500, 46.0000, 44.8000, 43.9000, 43.5000, 43.8000, 43.6000, 44.2500,
        44.0500, 44.1000, 43.8000, 43.2500, 43.8500, 42.5000, 43.3000, 42.5000,
        43.0500, 42.9000, 43.6000, 43.8500, 44.3500, 45.0000, 44.8000, 45.0000,
        45.0500, 44.7000, 44.9500, 44.6000, 44.5000, 44.9500, 45.3000, 44.7500,
        45.7500, 47.0500, 46.9500, 47.2000, 46.7500, 46.0000, 45.2000, 45.1000])
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:213: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
tensor([46.6021, 46.2714, 46.0804, 45.6374, 45.1572, 45.1275, 45.2910, 45.4740,
        45.2816, 44.8573, 44.5185, 44.7188, 45.1505, 45.4046, 45.5412, 45.9205,
        46.8739, 47.6201, 47.9406, 47.8473, 47.5755, 47.5514, 47.8119, 47.8126,
        47.7933, 48.0299, 48.2462, 48.2531, 48.1547, 47.9958, 47.8006, 47.4289,
        47.0704, 47.1254, 47.4580, 47.5835, 48.0940, 48.6403, 48.6149, 48.9674,
        49.4919, 49.5335, 49.1752, 48.8660, 48.0553, 47.4777, 47.0149, 46.4314,
        46.0866, 45.7862, 45.2946, 44.7788, 44.5765, 44.7081, 45.0087, 45.7017,
        46.4639, 46.8323, 47.2007, 47.9982, 48.2437, 47.6915, 46.5453, 45.8315],
       grad_fn=<ThAddBackward>)
tensor([44.7000, 44.7500, 43.7500, 43.6000, 44.0500, 44.1500, 44.4000, 43.6000,
        43.2500, 43.0000, 43.9500, 44.2000, 44.3000, 44.4000, 45.2000, 46.7500,
        46.7500, 46.9500, 46.3500, 46.1500, 46.4500, 46.9000, 46.3500, 46.7000,
        47.1000, 47.1500, 46.9500, 46.8500, 46.6000, 46.4000, 45.7500, 45.6000,
        46.1500, 46.5500, 46.3000, 47.7000, 47.7000, 47.1500, 48.5000, 48.6000,
        48.1500, 47.5500, 47.5000, 45.7000, 46.1500, 45.1500, 44.7000, 44.6500,
        44.2000, 43.5000, 43.1000, 43.3000, 43.6500, 44.0500, 45.3000, 45.8000,
        45.7500, 46.4500, 47.7000, 46.7500, 45.8000, 44.0500, 44.4000, 46.0000])
tensor([46.3312, 47.0383, 48.8342, 50.1141, 50.2653, 50.2520, 50.1429, 49.9846,
        50.2449, 50.7768, 51.4849, 54.0194, 56.5645, 58.1810, 58.8139, 59.1564,
        59.0580, 59.3686, 60.6099, 60.8901, 60.1260, 59.9945, 61.0758, 62.1237,
        62.6858, 62.2650, 62.1881, 62.6850, 63.2508, 64.1262, 64.5876, 64.9256,
        65.2428, 65.2611, 65.2929, 65.3716, 65.4433, 65.5710, 65.7848, 65.9379,
        65.9872, 65.8828, 65.7859, 66.1980, 65.5875, 65.3476, 66.2682, 66.3774,
        65.7991, 65.7637, 66.2459, 67.0513, 67.4090, 67.1898, 67.2905, 67.2209,
        67.0374, 66.9473, 66.5325, 66.1479, 65.5101, 64.2880, 63.3923, 62.8751],
       grad_fn=<ThAddBackward>)
tensor([46.2000, 50.1000, 49.1500, 49.1000, 49.1000, 48.8000, 48.6500, 49.5500,
        50.0500, 51.0500, 56.5000, 57.1000, 58.3000, 57.8500, 58.6000, 57.5000,
        59.1000, 60.9000, 59.2500, 58.2500, 59.3500, 61.4000, 61.6500, 62.0500,
        60.3000, 61.7000, 62.0500, 62.7500, 64.0500, 63.6000, 64.4000, 64.4000,
        64.1500, 64.4000, 64.4000, 64.5000, 64.7000, 65.0000, 65.0000, 65.0000,
        64.7000, 64.7500, 65.8500, 63.1500, 64.9500, 66.3000, 64.7500, 64.3000,
        65.1000, 65.7500, 66.9000, 66.3500, 65.9000, 66.7000, 65.8500, 66.0000,
        65.8500, 64.9500, 64.9500, 63.6500, 61.9500, 61.9000, 61.3500, 59.3000])
tensor([61.7730, 61.4636, 62.1061, 62.4689, 62.8332, 63.6347, 63.5934, 63.5534,
        63.8236, 66.6647, 69.0529, 69.6757, 69.7743, 69.7317, 69.0220, 68.8746,
        69.3136, 69.7744, 71.1489, 72.5269, 73.1682, 73.5271, 73.3472, 73.5506,
        73.8919, 74.4818, 74.4282, 75.1376, 76.0893, 75.7913, 74.6628, 74.9804,
        74.6065, 72.8452, 72.7105, 72.7755, 73.1679, 73.9989, 74.5786, 74.6298,
        74.4915, 75.2982, 76.7362, 77.7772, 78.3435, 78.7825, 78.8400, 78.7729,
        78.5161, 77.6619, 76.5941, 75.7674, 75.9645, 77.8718, 79.2068, 79.4064,
        79.4826, 80.0034, 80.9283, 81.7182, 82.5605, 82.9410, 83.4440, 84.1281],
       grad_fn=<ThAddBackward>)
tensor([60.9000, 61.7000, 61.4500, 62.3000, 63.5000, 61.8500, 62.9500, 62.9500,
        70.0500, 68.7000, 69.1000, 68.8000, 68.8000, 66.9500, 68.4500, 68.6500,
        69.2500, 72.0000, 72.4500, 72.6000, 72.9500, 71.9500, 73.3000, 73.0500,
        74.4000, 72.8500, 75.8000, 75.6000, 74.1000, 72.6000, 75.4500, 72.1500,
        70.2000, 72.8500, 71.2000, 73.2000, 73.7000, 74.1000, 73.5000, 73.5500,
        75.7000, 77.1500, 77.5500, 77.9000, 78.3500, 77.8000, 77.9500, 77.2500,
        75.7500, 74.8000, 74.2500, 75.8000, 79.4000, 78.6500, 78.6000, 78.8000,
        79.9000, 81.0000, 81.4500, 82.6500, 82.1000, 83.4500, 83.8500, 83.6500])
tensor([84.4005, 84.6996, 84.6718, 84.9785, 86.2844, 86.5611, 86.2835, 86.1814,
        86.2632, 86.2706, 86.5480, 86.8163, 87.0118, 86.7462, 86.7869, 87.0793,
        88.0195, 88.6771, 88.1258, 87.1189, 86.3898, 86.2158, 86.4642, 86.8204,
        86.8488, 86.1194, 85.4259, 86.7681, 87.7648, 87.5575, 85.7867, 84.5917,
        83.7764, 84.0212, 84.7655, 85.8504, 86.0653, 86.1259, 86.5632, 86.1217,
        85.9001, 85.8295, 84.9963, 84.1770, 84.2577, 84.8888, 84.8804, 84.2240,
        84.0083, 84.0717, 83.7581, 83.7944, 83.5539, 82.5830, 81.1195, 80.0031,
        80.2114, 80.6973, 80.4874, 80.8155, 81.7753, 82.4683, 82.7180, 82.3010],
       grad_fn=<ThAddBackward>)
tensor([84.3500, 83.5500, 84.9000, 87.0500, 85.1000, 85.5500, 85.3000, 85.7000,
        85.3500, 86.3000, 86.1000, 86.5000, 85.4000, 86.5000, 86.4500, 88.6000,
        88.0000, 86.4500, 85.5000, 85.1500, 85.5000, 86.0000, 86.3500, 85.9000,
        84.3500, 84.3000, 88.3500, 86.8500, 86.5000, 82.5500, 83.7500, 81.8000,
        84.3500, 84.3000, 86.4500, 84.6500, 85.9000, 86.1500, 84.4000, 85.4500,
        84.7500, 83.1000, 82.9000, 83.9500, 84.7000, 83.6000, 82.7500, 83.4000,
        83.2500, 82.4500, 83.4000, 82.1000, 80.7000, 78.8000, 78.5000, 80.1500,
        80.0000, 79.1500, 80.8500, 81.8500, 82.0500, 82.0000, 80.8000, 80.8000])
tensor([81.8339, 81.4882, 80.9619, 80.5423, 80.5116, 80.3514, 80.3347, 81.0369,
        81.4028, 81.9938, 82.7909, 83.1978, 83.0013, 82.6565, 83.1228, 82.4694,
        80.8136, 79.1115, 77.6810, 77.0718, 77.0625, 77.6009, 78.0567, 78.0939,
        78.0148, 78.2492, 78.6450, 79.0782, 78.6614, 78.0434, 77.7257, 75.9229,
        74.3290, 74.1510, 74.3230, 75.1916, 76.6698, 77.1124, 76.2578, 75.0575,
        73.7063, 72.2790, 71.3452, 71.7195, 72.4380, 73.2616, 73.6748, 73.0132,
        72.1295, 72.2551, 72.4583, 70.2656, 70.6857, 76.3878],
       grad_fn=<ThAddBackward>)
tensor([80.3000, 79.5500, 79.4500, 79.8000, 79.1500, 79.7000, 81.1500, 80.4500,
        82.1500, 82.5500, 82.6000, 81.7500, 81.6500, 83.2000, 80.0000, 78.5500,
        76.7000, 75.7000, 76.0000, 76.2500, 77.4500, 77.3500, 77.1500, 77.1000,
        77.8000, 78.1000, 78.6500, 76.8500, 76.9000, 76.6000, 72.4000, 72.8000,
        73.4500, 73.4500, 75.5300, 77.1800, 75.9000, 74.2900, 73.1400, 71.4600,
        70.0600, 69.8400, 71.7200, 71.8900, 73.2700, 72.7400, 71.0800, 70.5800,
        71.9900, 71.3400, 65.9800, 72.7700, 83.3000, 80.7100])
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 608, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 506, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 425, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    output_batch, labels_batch, evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 215, in Evaluate
    logging.INFO("evaluation_summary: ")
TypeError: 'int' object is not callable
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 08:00:41
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([28.7112, 42.1007, 46.9160, 47.9590, 48.1583, 48.2838, 48.6025, 48.9638,
        48.2733, 47.5681, 47.7925, 48.3164, 48.8564, 49.1943, 49.2014, 49.1886,
        49.1891, 48.9935, 48.9798, 49.0006, 48.9457, 48.6089, 48.0826, 47.8673,
        47.2680, 46.6699, 46.8072, 46.6342, 45.8981, 45.2300, 45.0252, 44.9766,
        45.1630, 45.3159, 45.3311, 45.2244, 44.8935, 44.8680, 44.5001, 44.3280,
        44.1668, 44.1239, 44.1929, 44.4541, 44.8254, 45.2134, 45.7265, 45.9949,
        46.1048, 46.2087, 46.1254, 46.1005, 46.0231, 45.8678, 45.9612, 46.2554,
        46.2377, 46.4651, 47.3160, 47.9277, 48.1941, 48.1560, 47.7304, 47.0732],
       grad_fn=<ThAddBackward>)
tensor([47.0500, 47.7000, 47.1500, 47.4000, 47.1500, 47.8000, 48.0000, 45.8000,
        46.1500, 47.1000, 47.5000, 48.1500, 48.1500, 47.9000, 48.0500, 47.9500,
        47.5000, 47.9500, 47.7000, 47.7000, 46.9000, 46.4000, 46.6500, 45.1000,
        45.1500, 46.0000, 44.8000, 43.9000, 43.5000, 43.8000, 43.6000, 44.2500,
        44.0500, 44.1000, 43.8000, 43.2500, 43.8500, 42.5000, 43.3000, 42.5000,
        43.0500, 42.9000, 43.6000, 43.8500, 44.3500, 45.0000, 44.8000, 45.0000,
        45.0500, 44.7000, 44.9500, 44.6000, 44.5000, 44.9500, 45.3000, 44.7500,
        45.7500, 47.0500, 46.9500, 47.2000, 46.7500, 46.0000, 45.2000, 45.1000])
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:213: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
tensor([46.6021, 46.2714, 46.0804, 45.6374, 45.1572, 45.1275, 45.2910, 45.4740,
        45.2816, 44.8573, 44.5185, 44.7188, 45.1505, 45.4046, 45.5412, 45.9205,
        46.8739, 47.6201, 47.9406, 47.8473, 47.5755, 47.5514, 47.8119, 47.8126,
        47.7933, 48.0299, 48.2462, 48.2531, 48.1547, 47.9958, 47.8006, 47.4289,
        47.0704, 47.1254, 47.4580, 47.5835, 48.0940, 48.6403, 48.6149, 48.9674,
        49.4919, 49.5335, 49.1752, 48.8660, 48.0553, 47.4777, 47.0149, 46.4314,
        46.0866, 45.7862, 45.2946, 44.7788, 44.5765, 44.7081, 45.0087, 45.7017,
        46.4639, 46.8323, 47.2007, 47.9982, 48.2437, 47.6915, 46.5453, 45.8315],
       grad_fn=<ThAddBackward>)
tensor([44.7000, 44.7500, 43.7500, 43.6000, 44.0500, 44.1500, 44.4000, 43.6000,
        43.2500, 43.0000, 43.9500, 44.2000, 44.3000, 44.4000, 45.2000, 46.7500,
        46.7500, 46.9500, 46.3500, 46.1500, 46.4500, 46.9000, 46.3500, 46.7000,
        47.1000, 47.1500, 46.9500, 46.8500, 46.6000, 46.4000, 45.7500, 45.6000,
        46.1500, 46.5500, 46.3000, 47.7000, 47.7000, 47.1500, 48.5000, 48.6000,
        48.1500, 47.5500, 47.5000, 45.7000, 46.1500, 45.1500, 44.7000, 44.6500,
        44.2000, 43.5000, 43.1000, 43.3000, 43.6500, 44.0500, 45.3000, 45.8000,
        45.7500, 46.4500, 47.7000, 46.7500, 45.8000, 44.0500, 44.4000, 46.0000])
tensor([46.3312, 47.0383, 48.8342, 50.1141, 50.2653, 50.2520, 50.1429, 49.9846,
        50.2449, 50.7768, 51.4849, 54.0194, 56.5645, 58.1810, 58.8139, 59.1564,
        59.0580, 59.3686, 60.6099, 60.8901, 60.1260, 59.9945, 61.0758, 62.1237,
        62.6858, 62.2650, 62.1881, 62.6850, 63.2508, 64.1262, 64.5876, 64.9256,
        65.2428, 65.2611, 65.2929, 65.3716, 65.4433, 65.5710, 65.7848, 65.9379,
        65.9872, 65.8828, 65.7859, 66.1980, 65.5875, 65.3476, 66.2682, 66.3774,
        65.7991, 65.7637, 66.2459, 67.0513, 67.4090, 67.1898, 67.2905, 67.2209,
        67.0374, 66.9473, 66.5325, 66.1479, 65.5101, 64.2880, 63.3923, 62.8751],
       grad_fn=<ThAddBackward>)
tensor([46.2000, 50.1000, 49.1500, 49.1000, 49.1000, 48.8000, 48.6500, 49.5500,
        50.0500, 51.0500, 56.5000, 57.1000, 58.3000, 57.8500, 58.6000, 57.5000,
        59.1000, 60.9000, 59.2500, 58.2500, 59.3500, 61.4000, 61.6500, 62.0500,
        60.3000, 61.7000, 62.0500, 62.7500, 64.0500, 63.6000, 64.4000, 64.4000,
        64.1500, 64.4000, 64.4000, 64.5000, 64.7000, 65.0000, 65.0000, 65.0000,
        64.7000, 64.7500, 65.8500, 63.1500, 64.9500, 66.3000, 64.7500, 64.3000,
        65.1000, 65.7500, 66.9000, 66.3500, 65.9000, 66.7000, 65.8500, 66.0000,
        65.8500, 64.9500, 64.9500, 63.6500, 61.9500, 61.9000, 61.3500, 59.3000])
tensor([61.7730, 61.4636, 62.1061, 62.4689, 62.8332, 63.6347, 63.5934, 63.5534,
        63.8236, 66.6647, 69.0529, 69.6757, 69.7743, 69.7317, 69.0220, 68.8746,
        69.3136, 69.7744, 71.1489, 72.5269, 73.1682, 73.5271, 73.3472, 73.5506,
        73.8919, 74.4818, 74.4282, 75.1376, 76.0893, 75.7913, 74.6628, 74.9804,
        74.6065, 72.8452, 72.7105, 72.7755, 73.1679, 73.9989, 74.5786, 74.6298,
        74.4915, 75.2982, 76.7362, 77.7772, 78.3435, 78.7825, 78.8400, 78.7729,
        78.5161, 77.6619, 76.5941, 75.7674, 75.9645, 77.8718, 79.2068, 79.4064,
        79.4826, 80.0034, 80.9283, 81.7182, 82.5605, 82.9410, 83.4440, 84.1281],
       grad_fn=<ThAddBackward>)
tensor([60.9000, 61.7000, 61.4500, 62.3000, 63.5000, 61.8500, 62.9500, 62.9500,
        70.0500, 68.7000, 69.1000, 68.8000, 68.8000, 66.9500, 68.4500, 68.6500,
        69.2500, 72.0000, 72.4500, 72.6000, 72.9500, 71.9500, 73.3000, 73.0500,
        74.4000, 72.8500, 75.8000, 75.6000, 74.1000, 72.6000, 75.4500, 72.1500,
        70.2000, 72.8500, 71.2000, 73.2000, 73.7000, 74.1000, 73.5000, 73.5500,
        75.7000, 77.1500, 77.5500, 77.9000, 78.3500, 77.8000, 77.9500, 77.2500,
        75.7500, 74.8000, 74.2500, 75.8000, 79.4000, 78.6500, 78.6000, 78.8000,
        79.9000, 81.0000, 81.4500, 82.6500, 82.1000, 83.4500, 83.8500, 83.6500])
tensor([84.4005, 84.6996, 84.6718, 84.9785, 86.2844, 86.5611, 86.2835, 86.1814,
        86.2632, 86.2706, 86.5480, 86.8163, 87.0118, 86.7462, 86.7869, 87.0793,
        88.0195, 88.6771, 88.1258, 87.1189, 86.3898, 86.2158, 86.4642, 86.8204,
        86.8488, 86.1194, 85.4259, 86.7681, 87.7648, 87.5575, 85.7867, 84.5917,
        83.7764, 84.0212, 84.7655, 85.8504, 86.0653, 86.1259, 86.5632, 86.1217,
        85.9001, 85.8295, 84.9963, 84.1770, 84.2577, 84.8888, 84.8804, 84.2240,
        84.0083, 84.0717, 83.7581, 83.7944, 83.5539, 82.5830, 81.1195, 80.0031,
        80.2114, 80.6973, 80.4874, 80.8155, 81.7753, 82.4683, 82.7180, 82.3010],
       grad_fn=<ThAddBackward>)
tensor([84.3500, 83.5500, 84.9000, 87.0500, 85.1000, 85.5500, 85.3000, 85.7000,
        85.3500, 86.3000, 86.1000, 86.5000, 85.4000, 86.5000, 86.4500, 88.6000,
        88.0000, 86.4500, 85.5000, 85.1500, 85.5000, 86.0000, 86.3500, 85.9000,
        84.3500, 84.3000, 88.3500, 86.8500, 86.5000, 82.5500, 83.7500, 81.8000,
        84.3500, 84.3000, 86.4500, 84.6500, 85.9000, 86.1500, 84.4000, 85.4500,
        84.7500, 83.1000, 82.9000, 83.9500, 84.7000, 83.6000, 82.7500, 83.4000,
        83.2500, 82.4500, 83.4000, 82.1000, 80.7000, 78.8000, 78.5000, 80.1500,
        80.0000, 79.1500, 80.8500, 81.8500, 82.0500, 82.0000, 80.8000, 80.8000])
tensor([81.8339, 81.4882, 80.9619, 80.5423, 80.5116, 80.3514, 80.3347, 81.0369,
        81.4028, 81.9938, 82.7909, 83.1978, 83.0013, 82.6565, 83.1228, 82.4694,
        80.8136, 79.1115, 77.6810, 77.0718, 77.0625, 77.6009, 78.0567, 78.0939,
        78.0148, 78.2492, 78.6450, 79.0782, 78.6614, 78.0434, 77.7257, 75.9229,
        74.3290, 74.1510, 74.3230, 75.1916, 76.6698, 77.1124, 76.2578, 75.0575,
        73.7063, 72.2790, 71.3452, 71.7195, 72.4380, 73.2616, 73.6748, 73.0132,
        72.1295, 72.2551, 72.4583, 70.2656, 70.6857, 76.3878],
       grad_fn=<ThAddBackward>)
tensor([80.3000, 79.5500, 79.4500, 79.8000, 79.1500, 79.7000, 81.1500, 80.4500,
        82.1500, 82.5500, 82.6000, 81.7500, 81.6500, 83.2000, 80.0000, 78.5500,
        76.7000, 75.7000, 76.0000, 76.2500, 77.4500, 77.3500, 77.1500, 77.1000,
        77.8000, 78.1000, 78.6500, 76.8500, 76.9000, 76.6000, 72.4000, 72.8000,
        73.4500, 73.4500, 75.5300, 77.1800, 75.9000, 74.2900, 73.1400, 71.4600,
        70.0600, 69.8400, 71.7200, 71.8900, 73.2700, 72.7400, 71.0800, 70.5800,
        71.9900, 71.3400, 65.9800, 72.7700, 83.3000, 80.7100])
INFO - evaluation_summary: 
INFO - [tensor(-0.8396), tensor(-1.2354), tensor(-0.6102), tensor(-0.2307), tensor(-0.8662), tensor(-0.9032)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 608, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 506, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 425, in RunNetworkArch
    y_pred = PredictSimpleRnn(Data['x_ho_data'],Data['y_ho_data'],model_params,file_path)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 251, in PredictSimpleRnn
    output_batch, labels_batch, evaluation_summary = SimpleRNN.Evaluate(model,loss_fn,test_loader, metrics, cuda = model_params.use_cuda)
  File "C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py", line 218, in Evaluate
    metrics_mean = {metric:np.mean([x[metric] for x in evaluation_summary]) for metric in evaluation_summary[0]}
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\torch\tensor.py", line 381, in __iter__
    raise TypeError('iteration over a 0-d tensor')
TypeError: iteration over a 0-d tensor
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 08:08:08
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
tensor([28.0211, 41.6833, 46.6387, 47.9323, 48.1776, 48.2936, 48.6049, 48.9633,
        48.2729, 47.5682, 47.7926, 48.3164, 48.8564, 49.1943, 49.2014, 49.1886,
        49.1892, 48.9935, 48.9798, 49.0006, 48.9457, 48.6089, 48.0826, 47.8673,
        47.2680, 46.6699, 46.8072, 46.6342, 45.8981, 45.2300, 45.0252, 44.9766,
        45.1630, 45.3159, 45.3311, 45.2244, 44.8935, 44.8680, 44.5001, 44.3280,
        44.1668, 44.1239, 44.1929, 44.4541, 44.8254, 45.2134, 45.7265, 45.9949,
        46.1048, 46.2087, 46.1254, 46.1005, 46.0231, 45.8678, 45.9612, 46.2554,
        46.2377, 46.4651, 47.3160, 47.9277, 48.1941, 48.1560, 47.7304, 47.0732],
       grad_fn=<ThAddBackward>)
tensor([47.0500, 47.7000, 47.1500, 47.4000, 47.1500, 47.8000, 48.0000, 45.8000,
        46.1500, 47.1000, 47.5000, 48.1500, 48.1500, 47.9000, 48.0500, 47.9500,
        47.5000, 47.9500, 47.7000, 47.7000, 46.9000, 46.4000, 46.6500, 45.1000,
        45.1500, 46.0000, 44.8000, 43.9000, 43.5000, 43.8000, 43.6000, 44.2500,
        44.0500, 44.1000, 43.8000, 43.2500, 43.8500, 42.5000, 43.3000, 42.5000,
        43.0500, 42.9000, 43.6000, 43.8500, 44.3500, 45.0000, 44.8000, 45.0000,
        45.0500, 44.7000, 44.9500, 44.6000, 44.5000, 44.9500, 45.3000, 44.7500,
        45.7500, 47.0500, 46.9500, 47.2000, 46.7500, 46.0000, 45.2000, 45.1000])
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:213: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
tensor([46.6021, 46.2714, 46.0804, 45.6374, 45.1572, 45.1275, 45.2910, 45.4740,
        45.2816, 44.8573, 44.5185, 44.7188, 45.1505, 45.4046, 45.5412, 45.9205,
        46.8739, 47.6201, 47.9406, 47.8473, 47.5755, 47.5514, 47.8119, 47.8126,
        47.7933, 48.0299, 48.2462, 48.2531, 48.1547, 47.9958, 47.8006, 47.4289,
        47.0704, 47.1254, 47.4580, 47.5835, 48.0940, 48.6403, 48.6149, 48.9674,
        49.4919, 49.5335, 49.1752, 48.8660, 48.0553, 47.4777, 47.0149, 46.4314,
        46.0866, 45.7862, 45.2946, 44.7788, 44.5765, 44.7081, 45.0087, 45.7017,
        46.4639, 46.8323, 47.2007, 47.9982, 48.2437, 47.6915, 46.5453, 45.8315],
       grad_fn=<ThAddBackward>)
tensor([44.7000, 44.7500, 43.7500, 43.6000, 44.0500, 44.1500, 44.4000, 43.6000,
        43.2500, 43.0000, 43.9500, 44.2000, 44.3000, 44.4000, 45.2000, 46.7500,
        46.7500, 46.9500, 46.3500, 46.1500, 46.4500, 46.9000, 46.3500, 46.7000,
        47.1000, 47.1500, 46.9500, 46.8500, 46.6000, 46.4000, 45.7500, 45.6000,
        46.1500, 46.5500, 46.3000, 47.7000, 47.7000, 47.1500, 48.5000, 48.6000,
        48.1500, 47.5500, 47.5000, 45.7000, 46.1500, 45.1500, 44.7000, 44.6500,
        44.2000, 43.5000, 43.1000, 43.3000, 43.6500, 44.0500, 45.3000, 45.8000,
        45.7500, 46.4500, 47.7000, 46.7500, 45.8000, 44.0500, 44.4000, 46.0000])
tensor([46.3312, 47.0383, 48.8342, 50.1141, 50.2653, 50.2520, 50.1429, 49.9846,
        50.2449, 50.7768, 51.4849, 54.0194, 56.5645, 58.1810, 58.8139, 59.1564,
        59.0580, 59.3686, 60.6099, 60.8901, 60.1260, 59.9945, 61.0758, 62.1237,
        62.6858, 62.2650, 62.1881, 62.6850, 63.2508, 64.1262, 64.5876, 64.9256,
        65.2428, 65.2611, 65.2929, 65.3716, 65.4433, 65.5710, 65.7848, 65.9379,
        65.9872, 65.8828, 65.7859, 66.1980, 65.5875, 65.3476, 66.2682, 66.3774,
        65.7991, 65.7637, 66.2459, 67.0513, 67.4090, 67.1898, 67.2905, 67.2209,
        67.0374, 66.9473, 66.5325, 66.1479, 65.5101, 64.2880, 63.3923, 62.8751],
       grad_fn=<ThAddBackward>)
tensor([46.2000, 50.1000, 49.1500, 49.1000, 49.1000, 48.8000, 48.6500, 49.5500,
        50.0500, 51.0500, 56.5000, 57.1000, 58.3000, 57.8500, 58.6000, 57.5000,
        59.1000, 60.9000, 59.2500, 58.2500, 59.3500, 61.4000, 61.6500, 62.0500,
        60.3000, 61.7000, 62.0500, 62.7500, 64.0500, 63.6000, 64.4000, 64.4000,
        64.1500, 64.4000, 64.4000, 64.5000, 64.7000, 65.0000, 65.0000, 65.0000,
        64.7000, 64.7500, 65.8500, 63.1500, 64.9500, 66.3000, 64.7500, 64.3000,
        65.1000, 65.7500, 66.9000, 66.3500, 65.9000, 66.7000, 65.8500, 66.0000,
        65.8500, 64.9500, 64.9500, 63.6500, 61.9500, 61.9000, 61.3500, 59.3000])
tensor([61.7730, 61.4636, 62.1061, 62.4689, 62.8332, 63.6347, 63.5934, 63.5534,
        63.8236, 66.6647, 69.0529, 69.6757, 69.7743, 69.7317, 69.0220, 68.8746,
        69.3136, 69.7744, 71.1489, 72.5269, 73.1682, 73.5271, 73.3472, 73.5506,
        73.8919, 74.4818, 74.4282, 75.1376, 76.0893, 75.7913, 74.6628, 74.9804,
        74.6065, 72.8452, 72.7105, 72.7755, 73.1679, 73.9989, 74.5786, 74.6298,
        74.4915, 75.2982, 76.7362, 77.7772, 78.3435, 78.7825, 78.8400, 78.7729,
        78.5161, 77.6619, 76.5941, 75.7674, 75.9645, 77.8718, 79.2068, 79.4064,
        79.4826, 80.0034, 80.9283, 81.7182, 82.5605, 82.9410, 83.4440, 84.1281],
       grad_fn=<ThAddBackward>)
tensor([60.9000, 61.7000, 61.4500, 62.3000, 63.5000, 61.8500, 62.9500, 62.9500,
        70.0500, 68.7000, 69.1000, 68.8000, 68.8000, 66.9500, 68.4500, 68.6500,
        69.2500, 72.0000, 72.4500, 72.6000, 72.9500, 71.9500, 73.3000, 73.0500,
        74.4000, 72.8500, 75.8000, 75.6000, 74.1000, 72.6000, 75.4500, 72.1500,
        70.2000, 72.8500, 71.2000, 73.2000, 73.7000, 74.1000, 73.5000, 73.5500,
        75.7000, 77.1500, 77.5500, 77.9000, 78.3500, 77.8000, 77.9500, 77.2500,
        75.7500, 74.8000, 74.2500, 75.8000, 79.4000, 78.6500, 78.6000, 78.8000,
        79.9000, 81.0000, 81.4500, 82.6500, 82.1000, 83.4500, 83.8500, 83.6500])
tensor([84.4005, 84.6996, 84.6718, 84.9785, 86.2844, 86.5611, 86.2835, 86.1814,
        86.2632, 86.2706, 86.5480, 86.8163, 87.0118, 86.7462, 86.7869, 87.0793,
        88.0195, 88.6771, 88.1258, 87.1189, 86.3898, 86.2158, 86.4642, 86.8204,
        86.8488, 86.1194, 85.4259, 86.7681, 87.7648, 87.5575, 85.7867, 84.5917,
        83.7764, 84.0212, 84.7655, 85.8504, 86.0653, 86.1259, 86.5632, 86.1217,
        85.9001, 85.8295, 84.9963, 84.1770, 84.2577, 84.8888, 84.8804, 84.2240,
        84.0083, 84.0717, 83.7581, 83.7944, 83.5539, 82.5830, 81.1195, 80.0031,
        80.2114, 80.6973, 80.4874, 80.8155, 81.7753, 82.4683, 82.7180, 82.3010],
       grad_fn=<ThAddBackward>)
tensor([84.3500, 83.5500, 84.9000, 87.0500, 85.1000, 85.5500, 85.3000, 85.7000,
        85.3500, 86.3000, 86.1000, 86.5000, 85.4000, 86.5000, 86.4500, 88.6000,
        88.0000, 86.4500, 85.5000, 85.1500, 85.5000, 86.0000, 86.3500, 85.9000,
        84.3500, 84.3000, 88.3500, 86.8500, 86.5000, 82.5500, 83.7500, 81.8000,
        84.3500, 84.3000, 86.4500, 84.6500, 85.9000, 86.1500, 84.4000, 85.4500,
        84.7500, 83.1000, 82.9000, 83.9500, 84.7000, 83.6000, 82.7500, 83.4000,
        83.2500, 82.4500, 83.4000, 82.1000, 80.7000, 78.8000, 78.5000, 80.1500,
        80.0000, 79.1500, 80.8500, 81.8500, 82.0500, 82.0000, 80.8000, 80.8000])
tensor([81.8339, 81.4882, 80.9619, 80.5423, 80.5116, 80.3514, 80.3347, 81.0369,
        81.4028, 81.9938, 82.7909, 83.1978, 83.0013, 82.6565, 83.1228, 82.4694,
        80.8136, 79.1115, 77.6810, 77.0718, 77.0625, 77.6009, 78.0567, 78.0939,
        78.0148, 78.2492, 78.6450, 79.0782, 78.6614, 78.0434, 77.7257, 75.9229,
        74.3290, 74.1510, 74.3230, 75.1916, 76.6698, 77.1124, 76.2578, 75.0575,
        73.7063, 72.2790, 71.3452, 71.7195, 72.4380, 73.2616, 73.6748, 73.0132,
        72.1295, 72.2551, 72.4583, 70.2656, 70.6857, 76.3878],
       grad_fn=<ThAddBackward>)
tensor([80.3000, 79.5500, 79.4500, 79.8000, 79.1500, 79.7000, 81.1500, 80.4500,
        82.1500, 82.5500, 82.6000, 81.7500, 81.6500, 83.2000, 80.0000, 78.5500,
        76.7000, 75.7000, 76.0000, 76.2500, 77.4500, 77.3500, 77.1500, 77.1000,
        77.8000, 78.1000, 78.6500, 76.8500, 76.9000, 76.6000, 72.4000, 72.8000,
        73.4500, 73.4500, 75.5300, 77.1800, 75.9000, 74.2900, 73.1400, 71.4600,
        70.0600, 69.8400, 71.7200, 71.8900, 73.2700, 72.7400, 71.0800, 70.5800,
        71.9900, 71.3400, 65.9800, 72.7700, 83.3000, 80.7100])
INFO - evaluation_summary: 
INFO - [tensor(-0.8180), tensor(-1.2354), tensor(-0.6102), tensor(-0.2307), tensor(-0.8662), tensor(-0.9032)]
error mean simple: 31.7526971288
error mean LSTM: 31.7526971288
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 608, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 515, in HyperParameter_Optimizations
    buy_vector = next_predicted_value > curr_real_value
ValueError: operands could not be broadcast together with shapes (53,) (373,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 08:12:20
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8450), tensor(-1.2354), tensor(-0.6102), tensor(-0.2307), tensor(-0.8662), tensor(-0.9032)]
error mean simple: 31.7526971288
error mean LSTM: 31.7526971288
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 608, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 515, in HyperParameter_Optimizations
    buy_vector = next_predicted_value > curr_real_value
ValueError: operands could not be broadcast together with shapes (53,) (373,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/15/18 08:12:46
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8237), tensor(-1.2354), tensor(-0.6102), tensor(-0.2307), tensor(-0.8662), tensor(-0.9032)]
error mean simple: 31.7526971288
error mean LSTM: 31.7526971288
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 608, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 515, in HyperParameter_Optimizations
    buy_vector = next_predicted_value > curr_real_value
ValueError: operands could not be broadcast together with shapes (53,) (373,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - ******************MLNX*******************
hello all, todays date & time is: 11/15/18 08:12:56
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8083), tensor(-1.2354), tensor(-0.6102), tensor(-0.2307), tensor(-0.8662), tensor(-0.9032)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 608, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 506, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 428, in RunNetworkArch
    logging.debug("y_pred shape is: " + str(y_pred.shape))
AttributeError: 'list' object has no attribute 'shape'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 07:50:52
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8515), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 607, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 437, in RunNetworkArch
    print("error mean simple: " + str(error.mean()))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\_methods.py", line 70, in _mean
    ret = umr_sum(arr, axis, dtype, out, keepdims)
ValueError: operands could not be broadcast together with shapes (64,) (54,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 07:54:12
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 27.80420494  41.94369507  46.89208221  47.92192459  48.11697006
  48.46783447  48.91739655  48.26756287  47.56631851  47.7912178
  48.31547165  48.8560257   49.19419861  49.20139694  49.18861008
  49.18914413  48.99349976  48.97984314  49.00055695  48.94570541
  48.60892868  48.08259201  47.86726379  47.26801682  46.66992188
  46.80718231  46.63415146  45.8980751   45.23001862  45.02515793
  44.97658539  45.16300583  45.31594086  45.33114624  45.22442245
  44.89346313  44.86798096  44.5001297   44.32802582  44.16678619
  44.12387085  44.19285965  44.45407867  44.82538605  45.21342468
  45.72652435  45.99487305  46.10478973  46.20868683  46.12539673
  46.10051346  46.02314758  45.86779785  45.96118927  46.25537872
  46.23765182  46.46508026  47.31603241  47.92772675  48.19409943
  48.15602112  47.73035049  47.07318115  46.60210419]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 46.27139282  46.08036804  45.63742447  45.15720749  45.12750244
  45.29095078  45.47400284  45.28155899  44.85729218  44.5185051
  44.71883392  45.15047836  45.40463257  45.54123688  45.92049408
  46.87389374  47.62013245  47.94057465  47.84734726  47.57553482
  47.5513916   47.81190872  47.81257629  47.79330063  48.02994919
  48.24617004  48.25312042  48.15468597  47.99581909  47.80056381
  47.42893219  47.0704155   47.1253624   47.45800781  47.58348465
  48.09402466  48.64032745  48.61489487  48.96735764  49.49185181
  49.53350067  49.17517471  48.86601639  48.05529785  47.47767639
  47.01493835  46.43143082  46.0865593   45.78619003  45.29459763
  44.77883911  44.57646179  44.70811081  45.00868988  45.70165253
  46.46389008  46.83230972  47.20069504  47.99824905  48.24368668
  47.69145203  46.54530334  45.83154297  46.33115768]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.03834915  48.83424759  50.11414337  50.26530457  50.25201416
  50.14289093  49.98461151  50.24485779  50.77676392  51.48490906
  54.01944733  56.56445312  58.18095398  58.81385422  59.15640259
  59.05800629  59.36856079  60.6098938   60.89014816  60.12604523
  59.99454117  61.07582092  62.12374878  62.6857605   62.26501083
  62.18805695  62.68503189  63.25075531  64.12621307  64.58760071
  64.92562103  65.24277496  65.26106262  65.2929306   65.37155151
  65.4433136   65.57098389  65.78475952  65.93792725  65.98719788
  65.88284302  65.78590393  66.19798279  65.58751678  65.34759521
  66.26817322  66.37741089  65.79914093  65.76367188  66.24588776
  67.05129242  67.40901184  67.1897583   67.29045105  67.22093964
  67.03741455  66.94734192  66.53248596  66.14793396  65.51006317
  64.28795624  63.39225769  62.87510681  61.772995  ]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 61.46358871  62.10612106  62.46888351  62.83324432  63.63466263
  63.59341431  63.55343628  63.82362747  66.66474152  69.05291748
  69.67568207  69.77429199  69.73170471  69.02196503  68.87464905
  69.31362915  69.77435303  71.14889526  72.52685547  73.16815948
  73.52709198  73.34719849  73.55056763  73.89192963  74.48179626
  74.42819214  75.1375885   76.08927155  75.79130554  74.66284943
  74.98036194  74.60654449  72.84524536  72.71048737  72.77552795
  73.16785431  73.99887848  74.57858276  74.62976837  74.49150848
  75.29824066  76.73622894  77.77723694  78.34346771  78.7824707
  78.84004974  78.7728653   78.51612091  77.66194153  76.59414673
  75.76738739  75.96445465  77.87181854  79.20675659  79.40638733
  79.48257446  80.00340271  80.92831421  81.71820831  82.56045532
  82.94097137  83.44403076  84.12808228  84.40054321]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 84.6995697   84.67182922  84.97848511  86.2844162   86.56105804
  86.2834549   86.18138885  86.26318359  86.27055359  86.54801941
  86.81628418  87.01177216  86.74624634  86.78691101  87.07928467
  88.01948547  88.67714691  88.12580872  87.11889648  86.38978577
  86.21582031  86.46420288  86.82036591  86.84880066  86.11936951
  85.42591858  86.76811218  87.76483154  87.55750275  85.78671265
  84.59173584  83.77635193  84.02116394  84.76545715  85.85036469
  86.06533813  86.12587738  86.56320953  86.12168121  85.90010071
  85.82948303  84.99633026  84.17698669  84.25773621  84.8888092
  84.88038635  84.22395325  84.00830078  84.07171631  83.75810242
  83.79444122  83.55387878  82.58300781  81.11953735  80.00306702
  80.21144104  80.6973114   80.48743439  80.81546021  81.77532959
  82.46826935  82.7179718   82.30104828  81.83392334]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 81.48815918  80.96191406  80.5422821   80.51158142  80.35139465
  80.3347168   81.03686523  81.40278625  81.99382019  82.79089355
  83.19781494  83.00131226  82.65647888  83.12281799  82.46942139
  80.81359863  79.11146545  77.68095398  77.07177734  77.0625      77.6008606
  78.05667877  78.09391785  78.01477814  78.2492218   78.64504242
  79.07824707  78.66143036  78.04338074  77.72571564  75.92287445
  74.32904816  74.15103149  74.32304382  75.19158173  76.66983032
  77.11244965  76.25784302  75.05747986  73.7063446   72.27903748
  71.34518433  71.71949005  72.43800354  73.2615509   73.67480469
  73.01315308  72.12953186  72.25509644  72.45826721  70.26557922
  70.68572998  76.38780212  80.40393829]
INFO - evaluation_summary: 
INFO - [tensor(-0.8288), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 607, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 437, in RunNetworkArch
    print("error mean simple: " + str(error.mean()))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\_methods.py", line 70, in _mean
    ret = umr_sum(arr, axis, dtype, out, keepdims)
ValueError: operands could not be broadcast together with shapes (64,) (54,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 07:55:33
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 28.85879326  42.58993149  46.90048218  48.0017128   48.11296082
  48.47080994  48.92262268  48.26885986  47.56700897  47.79141235
  48.31552505  48.85606384  49.19421005  49.20140457  49.18861008
  49.18914795  48.99349976  48.97984695  49.00055695  48.94570923
  48.6089325   48.08259201  47.86726761  47.268013    46.66992188
  46.80718231  46.63415146  45.8980751   45.23001862  45.02515793
  44.97658539  45.16300583  45.31594086  45.33114624  45.22442245
  44.89346313  44.86798096  44.5001297   44.32802582  44.16678619
  44.12387085  44.19285965  44.45407867  44.82538605  45.21342468
  45.72652435  45.99487305  46.10478973  46.20868683  46.12539673
  46.10051346  46.02314758  45.86779785  45.96118927  46.25537872
  46.23765182  46.46508026  47.31603241  47.92772675  48.19409943
  48.15602112  47.73035049  47.07318115  46.60210419]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 46.27139282  46.08036804  45.63742447  45.15720749  45.12750244
  45.29095078  45.47400284  45.28155899  44.85729218  44.5185051
  44.71883392  45.15047836  45.40463257  45.54123688  45.92049408
  46.87389374  47.62013245  47.94057465  47.84734726  47.57553482
  47.5513916   47.81190872  47.81257629  47.79330063  48.02994919
  48.24617004  48.25312042  48.15468597  47.99581909  47.80056381
  47.42893219  47.0704155   47.1253624   47.45800781  47.58348465
  48.09402466  48.64032745  48.61489487  48.96735764  49.49185181
  49.53350067  49.17517471  48.86601639  48.05529785  47.47767639
  47.01493835  46.43143082  46.0865593   45.78619003  45.29459763
  44.77883911  44.57646179  44.70811081  45.00868988  45.70165253
  46.46389008  46.83230972  47.20069504  47.99824905  48.24368668
  47.69145203  46.54530334  45.83154297  46.33115768]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.03834915  48.83424759  50.11414337  50.26530457  50.25201416
  50.14289093  49.98461151  50.24485779  50.77676392  51.48490906
  54.01944733  56.56445312  58.18095398  58.81385422  59.15640259
  59.05800629  59.36856079  60.6098938   60.89014816  60.12604523
  59.99454117  61.07582092  62.12374878  62.6857605   62.26501083
  62.18805695  62.68503189  63.25075531  64.12621307  64.58760071
  64.92562103  65.24277496  65.26106262  65.2929306   65.37155151
  65.4433136   65.57098389  65.78475952  65.93792725  65.98719788
  65.88284302  65.78590393  66.19798279  65.58751678  65.34759521
  66.26817322  66.37741089  65.79914093  65.76367188  66.24588776
  67.05129242  67.40901184  67.1897583   67.29045105  67.22093964
  67.03741455  66.94734192  66.53248596  66.14793396  65.51006317
  64.28795624  63.39225769  62.87510681  61.772995  ]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 61.46358871  62.10612106  62.46888351  62.83324432  63.63466263
  63.59341431  63.55343628  63.82362747  66.66474152  69.05291748
  69.67568207  69.77429199  69.73170471  69.02196503  68.87464905
  69.31362915  69.77435303  71.14889526  72.52685547  73.16815948
  73.52709198  73.34719849  73.55056763  73.89192963  74.48179626
  74.42819214  75.1375885   76.08927155  75.79130554  74.66284943
  74.98036194  74.60654449  72.84524536  72.71048737  72.77552795
  73.16785431  73.99887848  74.57858276  74.62976837  74.49150848
  75.29824066  76.73622894  77.77723694  78.34346771  78.7824707
  78.84004974  78.7728653   78.51612091  77.66194153  76.59414673
  75.76738739  75.96445465  77.87181854  79.20675659  79.40638733
  79.48257446  80.00340271  80.92831421  81.71820831  82.56045532
  82.94097137  83.44403076  84.12808228  84.40054321]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 84.6995697   84.67182922  84.97848511  86.2844162   86.56105804
  86.2834549   86.18138885  86.26318359  86.27055359  86.54801941
  86.81628418  87.01177216  86.74624634  86.78691101  87.07928467
  88.01948547  88.67714691  88.12580872  87.11889648  86.38978577
  86.21582031  86.46420288  86.82036591  86.84880066  86.11936951
  85.42591858  86.76811218  87.76483154  87.55750275  85.78671265
  84.59173584  83.77635193  84.02116394  84.76545715  85.85036469
  86.06533813  86.12587738  86.56320953  86.12168121  85.90010071
  85.82948303  84.99633026  84.17698669  84.25773621  84.8888092
  84.88038635  84.22395325  84.00830078  84.07171631  83.75810242
  83.79444122  83.55387878  82.58300781  81.11953735  80.00306702
  80.21144104  80.6973114   80.48743439  80.81546021  81.77532959
  82.46826935  82.7179718   82.30104828  81.83392334]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 81.48815918  80.96191406  80.5422821   80.51158142  80.35139465
  80.3347168   81.03686523  81.40278625  81.99382019  82.79089355
  83.19781494  83.00131226  82.65647888  83.12281799  82.46942139
  80.81359863  79.11146545  77.68095398  77.07177734  77.0625      77.6008606
  78.05667877  78.09391785  78.01477814  78.2492218   78.64504242
  79.07824707  78.66143036  78.04338074  77.72571564  75.92287445
  74.32904816  74.15103149  74.32304382  75.19158173  76.66983032
  77.11244965  76.25784302  75.05747986  73.7063446   72.27903748
  71.34518433  71.71949005  72.43800354  73.2615509   73.67480469
  73.01315308  72.12953186  72.25509644  72.45826721  70.26557922
  70.68572998  76.38780212  80.40393829]
INFO - evaluation_summary: 
INFO - [tensor(-0.8568), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32), array([ 50.09999847,  49.15000153,  49.09999847,  49.09999847,
        48.79999924,  48.65000153,  49.54999924,  50.04999924,
        51.04999924,  56.5       ,  57.09999847,  58.29999924,
        57.84999847,  58.59999847,  57.5       ,  59.09999847,
        60.90000153,  59.25      ,  58.25      ,  59.34999847,
        61.40000153,  61.65000153,  62.04999924,  60.29999924,
        61.70000076,  62.04999924,  62.75      ,  64.05000305,
        63.59999847,  64.40000153,  64.40000153,  64.15000153,
        64.40000153,  64.40000153,  64.5       ,  64.69999695,
        65.        ,  65.        ,  65.        ,  64.69999695,
        64.75      ,  65.84999847,  63.15000153,  64.94999695,
        66.30000305,  64.75      ,  64.30000305,  65.09999847,
        65.75      ,  66.90000153,  66.34999847,  65.90000153,
        66.69999695,  65.84999847,  66.        ,  65.84999847,
        64.94999695,  64.94999695,  63.65000153,  61.95000076,
        61.90000153,  61.34999847,  59.29999924,  60.90000153], dtype=float32), array([ 61.70000076,  61.45000076,  62.29999924,  63.5       ,
        61.84999847,  62.95000076,  62.95000076,  70.05000305,
        68.69999695,  69.09999847,  68.80000305,  68.80000305,
        66.94999695,  68.44999695,  68.65000153,  69.25      ,
        72.        ,  72.44999695,  72.59999847,  72.94999695,
        71.94999695,  73.30000305,  73.05000305,  74.40000153,
        72.84999847,  75.80000305,  75.59999847,  74.09999847,
        72.59999847,  75.44999695,  72.15000153,  70.19999695,
        72.84999847,  71.19999695,  73.19999695,  73.69999695,
        74.09999847,  73.5       ,  73.55000305,  75.69999695,
        77.15000153,  77.55000305,  77.90000153,  78.34999847,
        77.80000305,  77.94999695,  77.25      ,  75.75      ,
        74.80000305,  74.25      ,  75.80000305,  79.40000153,
        78.65000153,  78.59999847,  78.80000305,  79.90000153,
        81.        ,  81.44999695,  82.65000153,  82.09999847,
        83.44999695,  83.84999847,  83.65000153,  84.34999847], dtype=float32), array([ 83.55000305,  84.90000153,  87.05000305,  85.09999847,
        85.55000305,  85.30000305,  85.69999695,  85.34999847,
        86.30000305,  86.09999847,  86.5       ,  85.40000153,
        86.5       ,  86.44999695,  88.59999847,  88.        ,
        86.44999695,  85.5       ,  85.15000153,  85.5       ,
        86.        ,  86.34999847,  85.90000153,  84.34999847,
        84.30000305,  88.34999847,  86.84999847,  86.5       ,
        82.55000305,  83.75      ,  81.80000305,  84.34999847,
        84.30000305,  86.44999695,  84.65000153,  85.90000153,
        86.15000153,  84.40000153,  85.44999695,  84.75      ,
        83.09999847,  82.90000153,  83.94999695,  84.69999695,
        83.59999847,  82.75      ,  83.40000153,  83.25      ,
        82.44999695,  83.40000153,  82.09999847,  80.69999695,
        78.80000305,  78.5       ,  80.15000153,  80.        ,
        79.15000153,  80.84999847,  81.84999847,  82.05000305,
        82.        ,  80.80000305,  80.80000305,  80.30000305], dtype=float32), array([ 79.55000305,  79.44999695,  79.80000305,  79.15000153,
        79.69999695,  81.15000153,  80.44999695,  82.15000153,
        82.55000305,  82.59999847,  81.75      ,  81.65000153,
        83.19999695,  80.        ,  78.55000305,  76.69999695,
        75.69999695,  76.        ,  76.25      ,  77.44999695,
        77.34999847,  77.15000153,  77.09999847,  77.80000305,
        78.09999847,  78.65000153,  76.84999847,  76.90000153,
        76.59999847,  72.40000153,  72.80000305,  73.44999695,
        73.44999695,  75.52999878,  77.18000031,  75.90000153,
        74.29000092,  73.13999939,  71.45999908,  70.05999756,
        69.83999634,  71.72000122,  71.88999939,  73.26999664,
        72.73999786,  71.08000183,  70.58000183,  71.98999786,
        71.33999634,  65.98000336,  72.76999664,  83.30000305,
        80.70999908,  82.38999939], dtype=float32)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 607, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 437, in RunNetworkArch
    print("error mean simple: " + str(error.mean()))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\_methods.py", line 70, in _mean
    ret = umr_sum(arr, axis, dtype, out, keepdims)
ValueError: operands could not be broadcast together with shapes (64,) (54,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 07:56:24
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32)]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32)]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32), array([ 50.09999847,  49.15000153,  49.09999847,  49.09999847,
        48.79999924,  48.65000153,  49.54999924,  50.04999924,
        51.04999924,  56.5       ,  57.09999847,  58.29999924,
        57.84999847,  58.59999847,  57.5       ,  59.09999847,
        60.90000153,  59.25      ,  58.25      ,  59.34999847,
        61.40000153,  61.65000153,  62.04999924,  60.29999924,
        61.70000076,  62.04999924,  62.75      ,  64.05000305,
        63.59999847,  64.40000153,  64.40000153,  64.15000153,
        64.40000153,  64.40000153,  64.5       ,  64.69999695,
        65.        ,  65.        ,  65.        ,  64.69999695,
        64.75      ,  65.84999847,  63.15000153,  64.94999695,
        66.30000305,  64.75      ,  64.30000305,  65.09999847,
        65.75      ,  66.90000153,  66.34999847,  65.90000153,
        66.69999695,  65.84999847,  66.        ,  65.84999847,
        64.94999695,  64.94999695,  63.65000153,  61.95000076,
        61.90000153,  61.34999847,  59.29999924,  60.90000153], dtype=float32)]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32), array([ 50.09999847,  49.15000153,  49.09999847,  49.09999847,
        48.79999924,  48.65000153,  49.54999924,  50.04999924,
        51.04999924,  56.5       ,  57.09999847,  58.29999924,
        57.84999847,  58.59999847,  57.5       ,  59.09999847,
        60.90000153,  59.25      ,  58.25      ,  59.34999847,
        61.40000153,  61.65000153,  62.04999924,  60.29999924,
        61.70000076,  62.04999924,  62.75      ,  64.05000305,
        63.59999847,  64.40000153,  64.40000153,  64.15000153,
        64.40000153,  64.40000153,  64.5       ,  64.69999695,
        65.        ,  65.        ,  65.        ,  64.69999695,
        64.75      ,  65.84999847,  63.15000153,  64.94999695,
        66.30000305,  64.75      ,  64.30000305,  65.09999847,
        65.75      ,  66.90000153,  66.34999847,  65.90000153,
        66.69999695,  65.84999847,  66.        ,  65.84999847,
        64.94999695,  64.94999695,  63.65000153,  61.95000076,
        61.90000153,  61.34999847,  59.29999924,  60.90000153], dtype=float32), array([ 61.70000076,  61.45000076,  62.29999924,  63.5       ,
        61.84999847,  62.95000076,  62.95000076,  70.05000305,
        68.69999695,  69.09999847,  68.80000305,  68.80000305,
        66.94999695,  68.44999695,  68.65000153,  69.25      ,
        72.        ,  72.44999695,  72.59999847,  72.94999695,
        71.94999695,  73.30000305,  73.05000305,  74.40000153,
        72.84999847,  75.80000305,  75.59999847,  74.09999847,
        72.59999847,  75.44999695,  72.15000153,  70.19999695,
        72.84999847,  71.19999695,  73.19999695,  73.69999695,
        74.09999847,  73.5       ,  73.55000305,  75.69999695,
        77.15000153,  77.55000305,  77.90000153,  78.34999847,
        77.80000305,  77.94999695,  77.25      ,  75.75      ,
        74.80000305,  74.25      ,  75.80000305,  79.40000153,
        78.65000153,  78.59999847,  78.80000305,  79.90000153,
        81.        ,  81.44999695,  82.65000153,  82.09999847,
        83.44999695,  83.84999847,  83.65000153,  84.34999847], dtype=float32)]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32), array([ 50.09999847,  49.15000153,  49.09999847,  49.09999847,
        48.79999924,  48.65000153,  49.54999924,  50.04999924,
        51.04999924,  56.5       ,  57.09999847,  58.29999924,
        57.84999847,  58.59999847,  57.5       ,  59.09999847,
        60.90000153,  59.25      ,  58.25      ,  59.34999847,
        61.40000153,  61.65000153,  62.04999924,  60.29999924,
        61.70000076,  62.04999924,  62.75      ,  64.05000305,
        63.59999847,  64.40000153,  64.40000153,  64.15000153,
        64.40000153,  64.40000153,  64.5       ,  64.69999695,
        65.        ,  65.        ,  65.        ,  64.69999695,
        64.75      ,  65.84999847,  63.15000153,  64.94999695,
        66.30000305,  64.75      ,  64.30000305,  65.09999847,
        65.75      ,  66.90000153,  66.34999847,  65.90000153,
        66.69999695,  65.84999847,  66.        ,  65.84999847,
        64.94999695,  64.94999695,  63.65000153,  61.95000076,
        61.90000153,  61.34999847,  59.29999924,  60.90000153], dtype=float32), array([ 61.70000076,  61.45000076,  62.29999924,  63.5       ,
        61.84999847,  62.95000076,  62.95000076,  70.05000305,
        68.69999695,  69.09999847,  68.80000305,  68.80000305,
        66.94999695,  68.44999695,  68.65000153,  69.25      ,
        72.        ,  72.44999695,  72.59999847,  72.94999695,
        71.94999695,  73.30000305,  73.05000305,  74.40000153,
        72.84999847,  75.80000305,  75.59999847,  74.09999847,
        72.59999847,  75.44999695,  72.15000153,  70.19999695,
        72.84999847,  71.19999695,  73.19999695,  73.69999695,
        74.09999847,  73.5       ,  73.55000305,  75.69999695,
        77.15000153,  77.55000305,  77.90000153,  78.34999847,
        77.80000305,  77.94999695,  77.25      ,  75.75      ,
        74.80000305,  74.25      ,  75.80000305,  79.40000153,
        78.65000153,  78.59999847,  78.80000305,  79.90000153,
        81.        ,  81.44999695,  82.65000153,  82.09999847,
        83.44999695,  83.84999847,  83.65000153,  84.34999847], dtype=float32), array([ 83.55000305,  84.90000153,  87.05000305,  85.09999847,
        85.55000305,  85.30000305,  85.69999695,  85.34999847,
        86.30000305,  86.09999847,  86.5       ,  85.40000153,
        86.5       ,  86.44999695,  88.59999847,  88.        ,
        86.44999695,  85.5       ,  85.15000153,  85.5       ,
        86.        ,  86.34999847,  85.90000153,  84.34999847,
        84.30000305,  88.34999847,  86.84999847,  86.5       ,
        82.55000305,  83.75      ,  81.80000305,  84.34999847,
        84.30000305,  86.44999695,  84.65000153,  85.90000153,
        86.15000153,  84.40000153,  85.44999695,  84.75      ,
        83.09999847,  82.90000153,  83.94999695,  84.69999695,
        83.59999847,  82.75      ,  83.40000153,  83.25      ,
        82.44999695,  83.40000153,  82.09999847,  80.69999695,
        78.80000305,  78.5       ,  80.15000153,  80.        ,
        79.15000153,  80.84999847,  81.84999847,  82.05000305,
        82.        ,  80.80000305,  80.80000305,  80.30000305], dtype=float32)]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32), array([ 50.09999847,  49.15000153,  49.09999847,  49.09999847,
        48.79999924,  48.65000153,  49.54999924,  50.04999924,
        51.04999924,  56.5       ,  57.09999847,  58.29999924,
        57.84999847,  58.59999847,  57.5       ,  59.09999847,
        60.90000153,  59.25      ,  58.25      ,  59.34999847,
        61.40000153,  61.65000153,  62.04999924,  60.29999924,
        61.70000076,  62.04999924,  62.75      ,  64.05000305,
        63.59999847,  64.40000153,  64.40000153,  64.15000153,
        64.40000153,  64.40000153,  64.5       ,  64.69999695,
        65.        ,  65.        ,  65.        ,  64.69999695,
        64.75      ,  65.84999847,  63.15000153,  64.94999695,
        66.30000305,  64.75      ,  64.30000305,  65.09999847,
        65.75      ,  66.90000153,  66.34999847,  65.90000153,
        66.69999695,  65.84999847,  66.        ,  65.84999847,
        64.94999695,  64.94999695,  63.65000153,  61.95000076,
        61.90000153,  61.34999847,  59.29999924,  60.90000153], dtype=float32), array([ 61.70000076,  61.45000076,  62.29999924,  63.5       ,
        61.84999847,  62.95000076,  62.95000076,  70.05000305,
        68.69999695,  69.09999847,  68.80000305,  68.80000305,
        66.94999695,  68.44999695,  68.65000153,  69.25      ,
        72.        ,  72.44999695,  72.59999847,  72.94999695,
        71.94999695,  73.30000305,  73.05000305,  74.40000153,
        72.84999847,  75.80000305,  75.59999847,  74.09999847,
        72.59999847,  75.44999695,  72.15000153,  70.19999695,
        72.84999847,  71.19999695,  73.19999695,  73.69999695,
        74.09999847,  73.5       ,  73.55000305,  75.69999695,
        77.15000153,  77.55000305,  77.90000153,  78.34999847,
        77.80000305,  77.94999695,  77.25      ,  75.75      ,
        74.80000305,  74.25      ,  75.80000305,  79.40000153,
        78.65000153,  78.59999847,  78.80000305,  79.90000153,
        81.        ,  81.44999695,  82.65000153,  82.09999847,
        83.44999695,  83.84999847,  83.65000153,  84.34999847], dtype=float32), array([ 83.55000305,  84.90000153,  87.05000305,  85.09999847,
        85.55000305,  85.30000305,  85.69999695,  85.34999847,
        86.30000305,  86.09999847,  86.5       ,  85.40000153,
        86.5       ,  86.44999695,  88.59999847,  88.        ,
        86.44999695,  85.5       ,  85.15000153,  85.5       ,
        86.        ,  86.34999847,  85.90000153,  84.34999847,
        84.30000305,  88.34999847,  86.84999847,  86.5       ,
        82.55000305,  83.75      ,  81.80000305,  84.34999847,
        84.30000305,  86.44999695,  84.65000153,  85.90000153,
        86.15000153,  84.40000153,  85.44999695,  84.75      ,
        83.09999847,  82.90000153,  83.94999695,  84.69999695,
        83.59999847,  82.75      ,  83.40000153,  83.25      ,
        82.44999695,  83.40000153,  82.09999847,  80.69999695,
        78.80000305,  78.5       ,  80.15000153,  80.        ,
        79.15000153,  80.84999847,  81.84999847,  82.05000305,
        82.        ,  80.80000305,  80.80000305,  80.30000305], dtype=float32), array([ 79.55000305,  79.44999695,  79.80000305,  79.15000153,
        79.69999695,  81.15000153,  80.44999695,  82.15000153,
        82.55000305,  82.59999847,  81.75      ,  81.65000153,
        83.19999695,  80.        ,  78.55000305,  76.69999695,
        75.69999695,  76.        ,  76.25      ,  77.44999695,
        77.34999847,  77.15000153,  77.09999847,  77.80000305,
        78.09999847,  78.65000153,  76.84999847,  76.90000153,
        76.59999847,  72.40000153,  72.80000305,  73.44999695,
        73.44999695,  75.52999878,  77.18000031,  75.90000153,
        74.29000092,  73.13999939,  71.45999908,  70.05999756,
        69.83999634,  71.72000122,  71.88999939,  73.26999664,
        72.73999786,  71.08000183,  70.58000183,  71.98999786,
        71.33999634,  65.98000336,  72.76999664,  83.30000305,
        80.70999908,  82.38999939], dtype=float32)]
INFO - evaluation_summary: 
INFO - [tensor(-0.8493), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[array([ 47.70000076,  47.15000153,  47.40000153,  47.15000153,
        47.79999924,  48.        ,  45.79999924,  46.15000153,
        47.09999847,  47.5       ,  48.15000153,  48.15000153,
        47.90000153,  48.04999924,  47.95000076,  47.5       ,
        47.95000076,  47.70000076,  47.70000076,  46.90000153,
        46.40000153,  46.65000153,  45.09999847,  45.15000153,
        46.        ,  44.79999924,  43.90000153,  43.5       ,
        43.79999924,  43.59999847,  44.25      ,  44.04999924,
        44.09999847,  43.79999924,  43.25      ,  43.84999847,
        42.5       ,  43.29999924,  42.5       ,  43.04999924,
        42.90000153,  43.59999847,  43.84999847,  44.34999847,
        45.        ,  44.79999924,  45.        ,  45.04999924,
        44.70000076,  44.95000076,  44.59999847,  44.5       ,
        44.95000076,  45.29999924,  44.75      ,  45.75      ,
        47.04999924,  46.95000076,  47.20000076,  46.75      ,
        46.        ,  45.20000076,  45.09999847,  44.70000076], dtype=float32), array([ 44.75      ,  43.75      ,  43.59999847,  44.04999924,
        44.15000153,  44.40000153,  43.59999847,  43.25      ,
        43.        ,  43.95000076,  44.20000076,  44.29999924,
        44.40000153,  45.20000076,  46.75      ,  46.75      ,
        46.95000076,  46.34999847,  46.15000153,  46.45000076,
        46.90000153,  46.34999847,  46.70000076,  47.09999847,
        47.15000153,  46.95000076,  46.84999847,  46.59999847,
        46.40000153,  45.75      ,  45.59999847,  46.15000153,
        46.54999924,  46.29999924,  47.70000076,  47.70000076,
        47.15000153,  48.5       ,  48.59999847,  48.15000153,
        47.54999924,  47.5       ,  45.70000076,  46.15000153,
        45.15000153,  44.70000076,  44.65000153,  44.20000076,
        43.5       ,  43.09999847,  43.29999924,  43.65000153,
        44.04999924,  45.29999924,  45.79999924,  45.75      ,
        46.45000076,  47.70000076,  46.75      ,  45.79999924,
        44.04999924,  44.40000153,  46.        ,  46.20000076], dtype=float32), array([ 50.09999847,  49.15000153,  49.09999847,  49.09999847,
        48.79999924,  48.65000153,  49.54999924,  50.04999924,
        51.04999924,  56.5       ,  57.09999847,  58.29999924,
        57.84999847,  58.59999847,  57.5       ,  59.09999847,
        60.90000153,  59.25      ,  58.25      ,  59.34999847,
        61.40000153,  61.65000153,  62.04999924,  60.29999924,
        61.70000076,  62.04999924,  62.75      ,  64.05000305,
        63.59999847,  64.40000153,  64.40000153,  64.15000153,
        64.40000153,  64.40000153,  64.5       ,  64.69999695,
        65.        ,  65.        ,  65.        ,  64.69999695,
        64.75      ,  65.84999847,  63.15000153,  64.94999695,
        66.30000305,  64.75      ,  64.30000305,  65.09999847,
        65.75      ,  66.90000153,  66.34999847,  65.90000153,
        66.69999695,  65.84999847,  66.        ,  65.84999847,
        64.94999695,  64.94999695,  63.65000153,  61.95000076,
        61.90000153,  61.34999847,  59.29999924,  60.90000153], dtype=float32), array([ 61.70000076,  61.45000076,  62.29999924,  63.5       ,
        61.84999847,  62.95000076,  62.95000076,  70.05000305,
        68.69999695,  69.09999847,  68.80000305,  68.80000305,
        66.94999695,  68.44999695,  68.65000153,  69.25      ,
        72.        ,  72.44999695,  72.59999847,  72.94999695,
        71.94999695,  73.30000305,  73.05000305,  74.40000153,
        72.84999847,  75.80000305,  75.59999847,  74.09999847,
        72.59999847,  75.44999695,  72.15000153,  70.19999695,
        72.84999847,  71.19999695,  73.19999695,  73.69999695,
        74.09999847,  73.5       ,  73.55000305,  75.69999695,
        77.15000153,  77.55000305,  77.90000153,  78.34999847,
        77.80000305,  77.94999695,  77.25      ,  75.75      ,
        74.80000305,  74.25      ,  75.80000305,  79.40000153,
        78.65000153,  78.59999847,  78.80000305,  79.90000153,
        81.        ,  81.44999695,  82.65000153,  82.09999847,
        83.44999695,  83.84999847,  83.65000153,  84.34999847], dtype=float32), array([ 83.55000305,  84.90000153,  87.05000305,  85.09999847,
        85.55000305,  85.30000305,  85.69999695,  85.34999847,
        86.30000305,  86.09999847,  86.5       ,  85.40000153,
        86.5       ,  86.44999695,  88.59999847,  88.        ,
        86.44999695,  85.5       ,  85.15000153,  85.5       ,
        86.        ,  86.34999847,  85.90000153,  84.34999847,
        84.30000305,  88.34999847,  86.84999847,  86.5       ,
        82.55000305,  83.75      ,  81.80000305,  84.34999847,
        84.30000305,  86.44999695,  84.65000153,  85.90000153,
        86.15000153,  84.40000153,  85.44999695,  84.75      ,
        83.09999847,  82.90000153,  83.94999695,  84.69999695,
        83.59999847,  82.75      ,  83.40000153,  83.25      ,
        82.44999695,  83.40000153,  82.09999847,  80.69999695,
        78.80000305,  78.5       ,  80.15000153,  80.        ,
        79.15000153,  80.84999847,  81.84999847,  82.05000305,
        82.        ,  80.80000305,  80.80000305,  80.30000305], dtype=float32), array([ 79.55000305,  79.44999695,  79.80000305,  79.15000153,
        79.69999695,  81.15000153,  80.44999695,  82.15000153,
        82.55000305,  82.59999847,  81.75      ,  81.65000153,
        83.19999695,  80.        ,  78.55000305,  76.69999695,
        75.69999695,  76.        ,  76.25      ,  77.44999695,
        77.34999847,  77.15000153,  77.09999847,  77.80000305,
        78.09999847,  78.65000153,  76.84999847,  76.90000153,
        76.59999847,  72.40000153,  72.80000305,  73.44999695,
        73.44999695,  75.52999878,  77.18000031,  75.90000153,
        74.29000092,  73.13999939,  71.45999908,  70.05999756,
        69.83999634,  71.72000122,  71.88999939,  73.26999664,
        72.73999786,  71.08000183,  70.58000183,  71.98999786,
        71.33999634,  65.98000336,  72.76999664,  83.30000305,
        80.70999908,  82.38999939], dtype=float32)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 607, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 437, in RunNetworkArch
    print("error mean simple: " + str(error.mean()))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\core\_methods.py", line 70, in _mean
    ret = umr_sum(arr, axis, dtype, out, keepdims)
ValueError: operands could not be broadcast together with shapes (64,) (54,) 
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:02:47
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
INFO - evaluation_summary: 
INFO - [tensor(-0.8660), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
error mean simple: 29.3981197007
error mean LSTM: 29.3981197007
hello from PredictSimpleRnn
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
INFO - evaluation_summary: 
INFO - [tensor(-0.8741), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
error mean simple: 29.3981197007
error mean LSTM: 29.3981197007
hello from PredictSimpleRnn
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
INFO - evaluation_summary: 
INFO - [tensor(-0.8465), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
error mean simple: 29.3981197007
error mean LSTM: 29.3981197007
hello from PredictSimpleRnn
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
INFO - evaluation_summary: 
INFO - [tensor(-0.8645), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
error mean simple: 29.3981197007
error mean LSTM: 29.3981197007
hello from PredictSimpleRnn
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
INFO - evaluation_summary: 
INFO - [tensor(-0.8351), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
error mean simple: 29.3981197007
error mean LSTM: 29.3981197007
hello from PredictSimpleRnn
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076]
[ 44.75        43.75        43.59999847  44.04999924  44.15000153
  44.40000153  43.59999847  43.25        43.          43.95000076
  44.20000076  44.29999924  44.40000153  45.20000076  46.75        46.75
  46.95000076  46.34999847  46.15000153  46.45000076  46.90000153
  46.34999847  46.70000076  47.09999847  47.15000153  46.95000076
  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076]
[ 50.09999847  49.15000153  49.09999847  49.09999847  48.79999924
  48.65000153  49.54999924  50.04999924  51.04999924  56.5         57.09999847
  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153]
[ 61.70000076  61.45000076  62.29999924  63.5         61.84999847
  62.95000076  62.95000076  70.05000305  68.69999695  69.09999847
  68.80000305  68.80000305  66.94999695  68.44999695  68.65000153  69.25
  72.          72.44999695  72.59999847  72.94999695  71.94999695
  73.30000305  73.05000305  74.40000153  72.84999847  75.80000305
  75.59999847  74.09999847  72.59999847  75.44999695  72.15000153
  70.19999695  72.84999847  71.19999695  73.19999695  73.69999695
  74.09999847  73.5         73.55000305  75.69999695  77.15000153
  77.55000305  77.90000153  78.34999847  77.80000305  77.94999695  77.25
  75.75        74.80000305  74.25        75.80000305  79.40000153
  78.65000153  78.59999847  78.80000305  79.90000153  81.          81.44999695
  82.65000153  82.09999847  83.44999695  83.84999847  83.65000153
  84.34999847]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847]
[ 83.55000305  84.90000153  87.05000305  85.09999847  85.55000305
  85.30000305  85.69999695  85.34999847  86.30000305  86.09999847  86.5
  85.40000153  86.5         86.44999695  88.59999847  88.          86.44999695
  85.5         85.15000153  85.5         86.          86.34999847
  85.90000153  84.34999847  84.30000305  88.34999847  86.84999847  86.5
  82.55000305  83.75        81.80000305  84.34999847  84.30000305
  86.44999695  84.65000153  85.90000153  86.15000153  84.40000153
  85.44999695  84.75        83.09999847  82.90000153  83.94999695
  84.69999695  83.59999847  82.75        83.40000153  83.25        82.44999695
  83.40000153  82.09999847  80.69999695  78.80000305  78.5         80.15000153
  80.          79.15000153  80.84999847  81.84999847  82.05000305  82.
  80.80000305  80.80000305  80.30000305]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305]
[ 79.55000305  79.44999695  79.80000305  79.15000153  79.69999695
  81.15000153  80.44999695  82.15000153  82.55000305  82.59999847  81.75
  81.65000153  83.19999695  80.          78.55000305  76.69999695
  75.69999695  76.          76.25        77.44999695  77.34999847
  77.15000153  77.09999847  77.80000305  78.09999847  78.65000153
  76.84999847  76.90000153  76.59999847  72.40000153  72.80000305
  73.44999695  73.44999695  75.52999878  77.18000031  75.90000153
  74.29000092  73.13999939  71.45999908  70.05999756  69.83999634
  71.72000122  71.88999939  73.26999664  72.73999786  71.08000183
  70.58000183  71.98999786  71.33999634  65.98000336  72.76999664
  83.30000305  80.70999908  82.38999939]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
INFO - evaluation_summary: 
INFO - [tensor(-0.8515), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
[ 47.70000076  47.15000153  47.40000153  47.15000153  47.79999924  48.
  45.79999924  46.15000153  47.09999847  47.5         48.15000153
  48.15000153  47.90000153  48.04999924  47.95000076  47.5         47.95000076
  47.70000076  47.70000076  46.90000153  46.40000153  46.65000153
  45.09999847  45.15000153  46.          44.79999924  43.90000153  43.5
  43.79999924  43.59999847  44.25        44.04999924  44.09999847
  43.79999924  43.25        43.84999847  42.5         43.29999924  42.5
  43.04999924  42.90000153  43.59999847  43.84999847  44.34999847  45.
  44.79999924  45.          45.04999924  44.70000076  44.95000076
  44.59999847  44.5         44.95000076  45.29999924  44.75        45.75
  47.04999924  46.95000076  47.20000076  46.75        46.          45.20000076
  45.09999847  44.70000076  44.75        43.75        43.59999847
  44.04999924  44.15000153  44.40000153  43.59999847  43.25        43.
  43.95000076  44.20000076  44.29999924  44.40000153  45.20000076  46.75
  46.75        46.95000076  46.34999847  46.15000153  46.45000076
  46.90000153  46.34999847  46.70000076  47.09999847  47.15000153
  46.95000076  46.84999847  46.59999847  46.40000153  45.75        45.59999847
  46.15000153  46.54999924  46.29999924  47.70000076  47.70000076
  47.15000153  48.5         48.59999847  48.15000153  47.54999924  47.5
  45.70000076  46.15000153  45.15000153  44.70000076  44.65000153
  44.20000076  43.5         43.09999847  43.29999924  43.65000153
  44.04999924  45.29999924  45.79999924  45.75        46.45000076
  47.70000076  46.75        45.79999924  44.04999924  44.40000153  46.
  46.20000076  50.09999847  49.15000153  49.09999847  49.09999847
  48.79999924  48.65000153  49.54999924  50.04999924  51.04999924  56.5
  57.09999847  58.29999924  57.84999847  58.59999847  57.5         59.09999847
  60.90000153  59.25        58.25        59.34999847  61.40000153
  61.65000153  62.04999924  60.29999924  61.70000076  62.04999924  62.75
  64.05000305  63.59999847  64.40000153  64.40000153  64.15000153
  64.40000153  64.40000153  64.5         64.69999695  65.          65.          65.
  64.69999695  64.75        65.84999847  63.15000153  64.94999695
  66.30000305  64.75        64.30000305  65.09999847  65.75        66.90000153
  66.34999847  65.90000153  66.69999695  65.84999847  66.          65.84999847
  64.94999695  64.94999695  63.65000153  61.95000076  61.90000153
  61.34999847  59.29999924  60.90000153  61.70000076  61.45000076
  62.29999924  63.5         61.84999847  62.95000076  62.95000076
  70.05000305  68.69999695  69.09999847  68.80000305  68.80000305
  66.94999695  68.44999695  68.65000153  69.25        72.          72.44999695
  72.59999847  72.94999695  71.94999695  73.30000305  73.05000305
  74.40000153  72.84999847  75.80000305  75.59999847  74.09999847
  72.59999847  75.44999695  72.15000153  70.19999695  72.84999847
  71.19999695  73.19999695  73.69999695  74.09999847  73.5         73.55000305
  75.69999695  77.15000153  77.55000305  77.90000153  78.34999847
  77.80000305  77.94999695  77.25        75.75        74.80000305  74.25
  75.80000305  79.40000153  78.65000153  78.59999847  78.80000305
  79.90000153  81.          81.44999695  82.65000153  82.09999847
  83.44999695  83.84999847  83.65000153  84.34999847  83.55000305
  84.90000153  87.05000305  85.09999847  85.55000305  85.30000305
  85.69999695  85.34999847  86.30000305  86.09999847  86.5         85.40000153
  86.5         86.44999695  88.59999847  88.          86.44999695  85.5
  85.15000153  85.5         86.          86.34999847  85.90000153
  84.34999847  84.30000305  88.34999847  86.84999847  86.5         82.55000305
  83.75        81.80000305  84.34999847  84.30000305  86.44999695
  84.65000153  85.90000153  86.15000153  84.40000153  85.44999695  84.75
  83.09999847  82.90000153  83.94999695  84.69999695  83.59999847  82.75
  83.40000153  83.25        82.44999695  83.40000153  82.09999847
  80.69999695  78.80000305  78.5         80.15000153  80.          79.15000153
  80.84999847  81.84999847  82.05000305  82.          80.80000305
  80.80000305  80.30000305  79.55000305  79.44999695  79.80000305
  79.15000153  79.69999695  81.15000153  80.44999695  82.15000153
  82.55000305  82.59999847  81.75        81.65000153  83.19999695  80.
  78.55000305  76.69999695  75.69999695  76.          76.25        77.44999695
  77.34999847  77.15000153  77.09999847  77.80000305  78.09999847
  78.65000153  76.84999847  76.90000153  76.59999847  72.40000153
  72.80000305  73.44999695  73.44999695  75.52999878  77.18000031
  75.90000153  74.29000092  73.13999939  71.45999908  70.05999756
  69.83999634  71.72000122  71.88999939  73.26999664  72.73999786
  71.08000183  70.58000183  71.98999786  71.33999634  65.98000336
  72.76999664  83.30000305  80.70999908  82.38999939]
error mean simple: 29.3981197007
error mean LSTM: 29.3981197007
      false_buy_ratio_in_down ...   std
0.01                 0.489583 ...     0
0.05                 0.489583 ...     0
0.20                 0.489583 ...     0
0.25                 0.489583 ...     0
0.30                 0.489583 ...     0
0.50                 0.489583 ...     0

[6 rows x 8 columns]
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:11:06
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8141), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 607, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 432, in RunNetworkArch
    for i in len(y_pred):
TypeError: 'int' object is not iterable
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 431
    i=0
      ^
IndentationError: unindent does not match any outer indentation level
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 435
    i++
      ^
SyntaxError: invalid syntax
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:11:53
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8449), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.7779827118
[ 47.7]
42.2362594604
[ 47.15]
46.7534828186
[ 47.4]
47.8722457886
[ 47.15]
48.0866012573
[ 47.8]
48.4542884827
[ 48.]
48.9185791016
[ 45.8]
48.2661628723
[ 46.15]
47.5657196045
[ 47.1]
47.7909889221
[ 47.5]
48.3153648376
[ 48.15]
48.8560142517
[ 48.15]
49.194190979
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5505151411
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8353), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.5303344727
[ 47.7]
41.9441490173
[ 47.15]
46.6881637573
[ 47.4]
47.8689117432
[ 47.15]
48.0765991211
[ 47.8]
48.4583206177
[ 48.]
48.9177017212
[ 45.8]
48.2664337158
[ 46.15]
47.5660095215
[ 47.1]
47.7910232544
[ 47.5]
48.3153991699
[ 48.15]
48.8560218811
[ 48.15]
49.194190979
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.9798469543
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5530241072
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8838), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.9112377167
[ 47.7]
43.0201187134
[ 47.15]
47.1505584717
[ 47.4]
47.9896850586
[ 47.15]
48.1056060791
[ 47.8]
48.47838974
[ 48.]
48.9270858765
[ 45.8]
48.2695732117
[ 46.15]
47.5670318604
[ 47.1]
47.7913131714
[ 47.5]
48.3155517578
[ 48.15]
48.8560905457
[ 48.15]
49.1942214966
[ 47.9]
49.2014083862
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5413851094
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8609), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.9733886719
[ 47.7]
42.5945129395
[ 47.15]
47.0152549744
[ 47.4]
48.0075874329
[ 47.15]
48.1382369995
[ 47.8]
48.4677886963
[ 48.]
48.9232406616
[ 45.8]
48.2686309814
[ 46.15]
47.5666503906
[ 47.1]
47.7914352417
[ 47.5]
48.3155021667
[ 48.15]
48.8560600281
[ 48.15]
49.1942062378
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798469543
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680130005
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5475561665
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8521), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.9163131714
[ 47.7]
42.2834701538
[ 47.15]
46.8734321594
[ 47.4]
47.9746704102
[ 47.15]
48.1165771484
[ 47.8]
48.4650688171
[ 48.]
48.9226913452
[ 45.8]
48.267906189
[ 46.15]
47.5665512085
[ 47.1]
47.7913398743
[ 47.5]
48.3154792786
[ 48.15]
48.8560562134
[ 48.15]
49.1942062378
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5494155269
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8704), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.1882228851
[ 47.7]
42.8487930298
[ 47.15]
47.1042480469
[ 47.4]
48.0491333008
[ 47.15]
48.1299324036
[ 47.8]
48.4770889282
[ 48.]
48.9267196655
[ 45.8]
48.2702636719
[ 46.15]
47.5673599243
[ 47.1]
47.7915878296
[ 47.5]
48.3155899048
[ 48.15]
48.856098175
[ 48.15]
49.1942253113
[ 47.9]
49.2014083862
[ 48.05]
49.1886138916
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.545346881
      false_buy_ratio_in_down ...   std
0.01                 0.516418 ...     0
0.05                 0.516418 ...     0
0.20                 0.517857 ...     0
0.25                 0.516418 ...     0
0.30                 0.516418 ...     0
0.50                 0.516418 ...     0

[6 rows x 8 columns]INFO - *****************finished executing******************

C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:14:50
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8789), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.4412078857
[ 47.7]
43.0572052002
[ 47.15]
47.2166748047
[ 47.4]
48.0107574463
[ 47.15]
48.1348114014
[ 47.8]
48.4812355042
[ 48.]
48.9281845093
[ 45.8]
48.2699127197
[ 46.15]
47.5670928955
[ 47.1]
47.7914505005
[ 47.5]
48.3155822754
[ 48.15]
48.8561019897
[ 48.15]
49.1942253113
[ 47.9]
49.2014083862
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5432079494
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8369), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
27.9455184937
[ 47.7]
42.4284667969
[ 47.15]
46.8035430908
[ 47.4]
47.9471588135
[ 47.15]
48.0961456299
[ 47.8]
48.4499893188
[ 48.]
48.9165115356
[ 45.8]
48.2663955688
[ 46.15]
47.5656356812
[ 47.1]
47.7910270691
[ 47.5]
48.315322876
[ 48.15]
48.8559875488
[ 48.15]
49.1941833496
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5532273936
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8403), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.6212749481
[ 47.7]
41.9775466919
[ 47.15]
46.7679710388
[ 47.4]
47.943862915
[ 47.15]
48.1127090454
[ 47.8]
48.4629211426
[ 48.]
48.9168395996
[ 45.8]
48.2659072876
[ 46.15]
47.5657958984
[ 47.1]
47.791179657
[ 47.5]
48.3154220581
[ 48.15]
48.8560218811
[ 48.15]
49.194190979
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5522690197
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8481), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.7208137512
[ 47.7]
42.1943778992
[ 47.15]
46.8010482788
[ 47.4]
48.0265350342
[ 47.15]
48.1506156921
[ 47.8]
48.4792785645
[ 48.]
48.9262390137
[ 45.8]
48.2686653137
[ 46.15]
47.5668792725
[ 47.1]
47.7916259766
[ 47.5]
48.3155937195
[ 48.15]
48.856098175
[ 48.15]
49.1942138672
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5507270515
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8832), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.706495285
[ 47.7]
42.9595222473
[ 47.15]
47.3294639587
[ 47.4]
48.0417175293
[ 47.15]
48.1164131165
[ 47.8]
48.4657974243
[ 48.]
48.9244537354
[ 45.8]
48.2704048157
[ 46.15]
47.5675201416
[ 47.1]
47.7915306091
[ 47.5]
48.3155326843
[ 48.15]
48.8560714722
[ 48.15]
49.1942214966
[ 47.9]
49.2014083862
[ 48.05]
49.1886138916
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680130005
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.542329962
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8601), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.4027233124
[ 47.7]
42.3373641968
[ 47.15]
46.8419952393
[ 47.4]
47.994972229
[ 47.15]
48.1069030762
[ 47.8]
48.4649925232
[ 48.]
48.9189910889
[ 45.8]
48.2666473389
[ 46.15]
47.5662727356
[ 47.1]
47.7912597656
[ 47.5]
48.3154716492
[ 48.15]
48.8560371399
[ 48.15]
49.194190979
[ 47.9]
49.2013969421
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5470581719
      false_buy_ratio_in_down ...   std
0.01                 0.517857 ...     0
0.05                 0.516418 ...     0
0.20                 0.516418 ...     0
0.25                 0.516418 ...     0
0.30                 0.517857 ...     0
0.50                 0.516418 ...     0

[6 rows x 8 columns]
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:15:40
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8489), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.7467975616
[ 47.7]
42.4955825806
[ 47.15]
46.7352714539
[ 47.4]
47.9348373413
[ 47.15]
48.0823478699
[ 47.8]
48.4424667358
[ 48.]
48.9139175415
[ 45.8]
48.2652053833
[ 46.15]
47.5659408569
[ 47.1]
47.791053772
[ 47.5]
48.3153343201
[ 48.15]
48.8559875488
[ 48.15]
49.1941757202
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.54949742
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8566), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.315076828
[ 47.7]
42.3137435913
[ 47.15]
46.836933136
[ 47.4]
47.9144515991
[ 47.15]
48.0904769897
[ 47.8]
48.4565200806
[ 48.]
48.9205589294
[ 45.8]
48.2672424316
[ 46.15]
47.5660400391
[ 47.1]
47.7911987305
[ 47.5]
48.3154067993
[ 48.15]
48.8560333252
[ 48.15]
49.1941986084
[ 47.9]
49.2013969421
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5476256484
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8374), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.0617771149
[ 47.7]
42.326385498
[ 47.15]
46.7812728882
[ 47.4]
47.9417991638
[ 47.15]
48.1145401001
[ 47.8]
48.4679641724
[ 48.]
48.9206428528
[ 45.8]
48.2670478821
[ 46.15]
47.5660591125
[ 47.1]
47.7912254333
[ 47.5]
48.3154678345
[ 48.15]
48.856048584
[ 48.15]
49.1941986084
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5531800495
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8419), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.2821331024
[ 47.7]
42.3293304443
[ 47.15]
46.8279342651
[ 47.4]
47.9908828735
[ 47.15]
48.0998497009
[ 47.8]
48.4541320801
[ 48.]
48.9175262451
[ 45.8]
48.2669372559
[ 46.15]
47.566318512
[ 47.1]
47.7912750244
[ 47.5]
48.3154067993
[ 48.15]
48.8560218811
[ 48.15]
49.194190979
[ 47.9]
49.2013969421
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5521028464
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8356), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.3255004883
[ 47.7]
42.1321334839
[ 47.15]
46.6622505188
[ 47.4]
47.9043312073
[ 47.15]
48.0978050232
[ 47.8]
48.4573745728
[ 48.]
48.9195251465
[ 45.8]
48.2663803101
[ 46.15]
47.5658721924
[ 47.1]
47.7910614014
[ 47.5]
48.3153877258
[ 48.15]
48.8560218811
[ 48.15]
49.1941947937
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5531043412
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8713), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.3267154694
[ 47.7]
42.9071922302
[ 47.15]
47.0171661377
[ 47.4]
48.0279693604
[ 47.15]
48.1138343811
[ 47.8]
48.4648094177
[ 48.]
48.9251594543
[ 45.8]
48.2687225342
[ 46.15]
47.5667953491
[ 47.1]
47.791431427
[ 47.5]
48.3154678345
[ 48.15]
48.8560676575
[ 48.15]
49.1942138672
[ 47.9]
49.2014045715
[ 48.05]
49.1886062622
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798469543
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5446199996
      false_buy_ratio_in_down ...   std
0.01                 0.263415 ...     0
0.05                 0.263415 ...     0
0.20                 0.263415 ...     0
0.25                 0.263415 ...     0
0.30                 0.263415 ...     0
0.50                 0.263415 ...     0

[6 rows x 8 columns]
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:19:25
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8497), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.0177249908
[ 47.7]
42.1904525757
[ 47.15]
46.7537460327
[ 47.4]
47.9668731689
[ 47.15]
48.0928077698
[ 47.8]
48.4607315063
[ 48.]
48.919128418
[ 45.8]
48.2665863037
[ 46.15]
47.5660324097
[ 47.1]
47.7911911011
[ 47.5]
48.3154144287
[ 48.15]
48.8560333252
[ 48.15]
49.194190979
[ 47.9]
49.2013969421
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680130005
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5495809659
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8546), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.3193035126
[ 47.7]
42.0582046509
[ 47.15]
46.8504943848
[ 47.4]
47.9663619995
[ 47.15]
48.1290130615
[ 47.8]
48.4742355347
[ 48.]
48.9213104248
[ 45.8]
48.2670211792
[ 46.15]
47.5663604736
[ 47.1]
47.791305542
[ 47.5]
48.3155288696
[ 48.15]
48.8560600281
[ 48.15]
49.1941986084
[ 47.9]
49.2013969421
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5486550815
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8518), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.9210605621
[ 47.7]
42.2394714355
[ 47.15]
46.9153442383
[ 47.4]
47.9653167725
[ 47.15]
48.100982666
[ 47.8]
48.4716567993
[ 48.]
48.9211425781
[ 45.8]
48.2680969238
[ 46.15]
47.566280365
[ 47.1]
47.791229248
[ 47.5]
48.3154792786
[ 48.15]
48.8560523987
[ 48.15]
49.1942062378
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5495413463
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8731), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.8208961487
[ 47.7]
42.7642059326
[ 47.15]
46.946105957
[ 47.4]
47.9201278687
[ 47.15]
48.0635871887
[ 47.8]
48.4615097046
[ 48.]
48.9208602905
[ 45.8]
48.2677955627
[ 46.15]
47.5661849976
[ 47.1]
47.7910575867
[ 47.5]
48.3153915405
[ 48.15]
48.8560333252
[ 48.15]
49.1941986084
[ 47.9]
49.2013969421
[ 48.05]
49.1886062622
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5432473382
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8475), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.6841850281
[ 47.7]
42.2409515381
[ 47.15]
46.8472518921
[ 47.4]
47.9835929871
[ 47.15]
48.1145935059
[ 47.8]
48.470790863
[ 48.]
48.9205322266
[ 45.8]
48.2666702271
[ 46.15]
47.5662994385
[ 47.1]
47.7912254333
[ 47.5]
48.3154830933
[ 48.15]
48.856048584
[ 48.15]
49.1941986084
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5506681795
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8426), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.4023094177
[ 47.7]
42.1260681152
[ 47.15]
46.848449707
[ 47.4]
48.0179481506
[ 47.15]
48.148727417
[ 47.8]
48.4791564941
[ 48.]
48.9225463867
[ 45.8]
48.2676544189
[ 46.15]
47.5664367676
[ 47.1]
47.7914886475
[ 47.5]
48.3155593872
[ 48.15]
48.8560791016
[ 48.15]
49.1942062378
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5523817939
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                 0.263415              0.440217              0.531429   
0.05                 0.263415              0.440217              0.531429   
0.20                 0.263415              0.440217              0.531429   
0.25                 0.263415              0.440217              0.531429   
0.30                 0.263415              0.440217              0.531429   
0.50                 0.263415              0.440217              0.531429   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01    2.327837   0.001066             0.007793                 0.245509   0  
0.05    2.326644   0.001066             0.007793                 0.245509   0  
0.20    2.327310   0.001066             0.007793                 0.245509   0  
0.25    2.318589   0.001066             0.007793                 0.245509   0  
0.30    2.329176   0.001066             0.007793                 0.245509   0  
0.50    2.331854   0.001066             0.007793                 0.245509   0  
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:20:38
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8675), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.2994728088
[ 47.7]
42.7406234741
[ 47.15]
46.9708404541
[ 47.4]
48.0359420776
[ 47.15]
48.1048583984
[ 47.8]
48.463142395
[ 48.]
48.9231491089
[ 45.8]
48.2678222656
[ 46.15]
47.5666656494
[ 47.1]
47.791343689
[ 47.5]
48.315448761
[ 48.15]
48.8560523987
[ 48.15]
49.1942024231
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5455329225
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8476), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.7752132416
[ 47.7]
42.277130127
[ 47.15]
46.7650299072
[ 47.4]
47.9266586304
[ 47.15]
48.1271209717
[ 47.8]
48.4743347168
[ 48.]
48.9222564697
[ 45.8]
48.2667961121
[ 46.15]
47.5659942627
[ 47.1]
47.7912139893
[ 47.5]
48.3155059814
[ 48.15]
48.8560562134
[ 48.15]
49.1941986084
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680130005
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5502552227
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8329), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.2648620605
[ 47.7]
41.8909187317
[ 47.15]
46.7624511719
[ 47.4]
47.9118728638
[ 47.15]
48.1086196899
[ 47.8]
48.4709854126
[ 48.]
48.9164810181
[ 45.8]
48.265953064
[ 46.15]
47.5658493042
[ 47.1]
47.7911262512
[ 47.5]
48.3154754639
[ 48.15]
48.8560333252
[ 48.15]
49.194190979
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5542651121
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8536), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.9530124664
[ 47.7]
42.3877944946
[ 47.15]
46.8696022034
[ 47.4]
47.9389572144
[ 47.15]
48.1058311462
[ 47.8]
48.4710845947
[ 48.]
48.9240570068
[ 45.8]
48.2679977417
[ 46.15]
47.5664749146
[ 47.1]
47.7912063599
[ 47.5]
48.3154907227
[ 48.15]
48.8560638428
[ 48.15]
49.1942062378
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798469543
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5488210564
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8524), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.9223194122
[ 47.7]
42.366985321
[ 47.15]
46.8520927429
[ 47.4]
47.9415168762
[ 47.15]
48.1133041382
[ 47.8]
48.4652481079
[ 48.]
48.9155540466
[ 45.8]
48.2646713257
[ 46.15]
47.5652809143
[ 47.1]
47.7910308838
[ 47.5]
48.3154296875
[ 48.15]
48.8560142517
[ 48.15]
49.1941833496
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5490783519
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8712), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.7317848206
[ 47.7]
42.4990692139
[ 47.15]
46.9880180359
[ 47.4]
48.0251693726
[ 47.15]
48.1283340454
[ 47.8]
48.4799499512
[ 48.]
48.9248390198
[ 45.8]
48.2687454224
[ 46.15]
47.566947937
[ 47.1]
47.7914505005
[ 47.5]
48.3155670166
[ 48.15]
48.856086731
[ 48.15]
49.1942138672
[ 47.9]
49.2014083862
[ 48.05]
49.1886138916
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798469543
[ 47.7]
49.0005569458
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680130005
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5446304882
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                 0.516418              0.092391                  0.88   
0.05                 0.516418              0.092391                  0.88   
0.20                 0.516418              0.092391                  0.88   
0.25                 0.516418              0.092391                  0.88   
0.30                 0.516418              0.092391                  0.88   
0.50                 0.516418              0.092391                  0.88   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01    2.322421   0.001853             0.007793                 0.945946   0  
0.05    2.328701   0.001853             0.007793                 0.945946   0  
0.20    2.333520   0.001853             0.007793                 0.945946   0  
0.25    2.326440   0.001853             0.007793                 0.945946   0  
0.30    2.326775   0.001853             0.007793                 0.945946   0  
0.50    2.321453   0.001853             0.007793                 0.945946   0  
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:22:33
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8355), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.0700683594
[ 47.7]
42.0451126099
[ 47.15]
46.8654670715
[ 47.4]
47.9906616211
[ 47.15]
48.140914917
[ 47.8]
48.4631195068
[ 48.]
48.9200630188
[ 45.8]
48.2671852112
[ 46.15]
47.5663223267
[ 47.1]
47.7914352417
[ 47.5]
48.3155059814
[ 48.15]
48.8560447693
[ 48.15]
49.1941986084
[ 47.9]
49.2013969421
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5542214844
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8567), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.9393501282
[ 47.7]
42.4720916748
[ 47.15]
46.9861297607
[ 47.4]
47.9772758484
[ 47.15]
48.0965805054
[ 47.8]
48.4577407837
[ 48.]
48.9201812744
[ 45.8]
48.2674865723
[ 46.15]
47.5663490295
[ 47.1]
47.7911643982
[ 47.5]
48.3154144287
[ 48.15]
48.8560256958
[ 48.15]
49.1941986084
[ 47.9]
49.2013969421
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.9798469543
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5483327855
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8504), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.0957164764
[ 47.7]
42.0531921387
[ 47.15]
46.8557319641
[ 47.4]
47.9491539001
[ 47.15]
48.1133842468
[ 47.8]
48.4641418457
[ 48.]
48.9191017151
[ 45.8]
48.2670669556
[ 46.15]
47.5660629272
[ 47.1]
47.791267395
[ 47.5]
48.3154678345
[ 48.15]
48.8560409546
[ 48.15]
49.1941986084
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5496881921
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8671), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.3368244171
[ 47.7]
42.706489563
[ 47.15]
46.9404792786
[ 47.4]
48.0010910034
[ 47.15]
48.1304359436
[ 47.8]
48.473777771
[ 48.]
48.9237747192
[ 45.8]
48.2678794861
[ 46.15]
47.5664596558
[ 47.1]
47.7914199829
[ 47.5]
48.3155288696
[ 48.15]
48.8560791016
[ 48.15]
49.1942062378
[ 47.9]
49.2013969421
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5455584168
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8613), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
29.1320552826
[ 47.7]
42.5714111328
[ 47.15]
46.9475135803
[ 47.4]
47.9987831116
[ 47.15]
48.1034622192
[ 47.8]
48.4665985107
[ 48.]
48.9233932495
[ 45.8]
48.2680358887
[ 46.15]
47.5667572021
[ 47.1]
47.7913551331
[ 47.5]
48.3154983521
[ 48.15]
48.8560638428
[ 48.15]
49.1942100525
[ 47.9]
49.2014007568
[ 48.05]
49.1886100769
[ 47.95]
49.1891479492
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089286804
[ 46.4]
48.0825920105
[ 46.65]
47.8672637939
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5470684988
hello from PredictSimpleRnn
INFO - evaluation_summary: 
INFO - [tensor(-0.8385), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.5177097321
[ 47.7]
42.0410385132
[ 47.15]
46.7484283447
[ 47.4]
47.8890838623
[ 47.15]
48.1096878052
[ 47.8]
48.4675559998
[ 48.]
48.9167175293
[ 45.8]
48.2662658691
[ 46.15]
47.5656471252
[ 47.1]
47.7910461426
[ 47.5]
48.3154449463
[ 48.15]
48.8560218811
[ 48.15]
49.194190979
[ 47.9]
49.2013931274
[ 48.05]
49.1886062622
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.9798431396
[ 47.7]
49.0005531311
[ 47.7]
48.9457054138
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5525143155
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                 0.263415              0.440217              0.531429   
0.05                 0.263415              0.440217              0.531429   
0.20                 0.263415              0.440217              0.531429   
0.25                 0.263415              0.440217              0.531429   
0.30                 0.263415              0.440217              0.531429   
0.50                 0.263415              0.440217              0.531429   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01    2.333773   0.001066             0.007793                 0.245509   0  
0.05    2.325445   0.001066             0.007793                 0.245509   0  
0.20    2.327640   0.001066             0.007793                 0.245509   0  
0.25    2.322585   0.001066             0.007793                 0.245509   0  
0.30    2.324255   0.001066             0.007793                 0.245509   0  
0.50    2.331189   0.001066             0.007793                 0.245509   0  
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:24:47
INFO - ******************MLNX*******************
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(-0.8299), tensor(-1.2078), tensor(-0.6218), tensor(-0.2179), tensor(-0.8894), tensor(-0.8380)]
28.410194397
[ 47.7]
41.7513542175
[ 47.15]
46.6206817627
[ 47.4]
47.8896636963
[ 47.15]
48.092956543
[ 47.8]
48.4587554932
[ 48.]
48.9145507813
[ 45.8]
48.2652664185
[ 46.15]
47.5655555725
[ 47.1]
47.7910461426
[ 47.5]
48.315410614
[ 48.15]
48.8560066223
[ 48.15]
49.1941833496
[ 47.9]
49.2013931274
[ 48.05]
49.1886100769
[ 47.95]
49.1891441345
[ 47.5]
48.9934997559
[ 47.95]
48.979850769
[ 47.7]
49.0005569458
[ 47.7]
48.9457092285
[ 46.9]
48.6089324951
[ 46.4]
48.0825920105
[ 46.65]
47.8672676086
[ 45.1]
47.2680168152
[ 45.15]
46.669921875
[ 46.]
46.807182312
[ 44.8]
46.6341514587
[ 43.9]
45.8980751038
[ 43.5]
45.2300186157
[ 43.8]
45.0251579285
[ 43.6]
44.9765853882
[ 44.25]
45.1630058289
[ 44.05]
45.3159408569
[ 44.1]
45.3311462402
[ 43.8]
45.2244224548
[ 43.25]
44.8934631348
[ 43.85]
44.867980957
[ 42.5]
44.5001296997
[ 43.3]
44.3280258179
[ 42.5]
44.1667861938
[ 43.05]
44.1238708496
[ 42.9]
44.1928596497
[ 43.6]
44.4540786743
[ 43.85]
44.8253860474
[ 44.35]
45.2134246826
[ 45.]
45.726524353
[ 44.8]
45.9948730469
[ 45.]
46.1047897339
[ 45.05]
46.2086868286
[ 44.7]
46.1253967285
[ 44.95]
46.1005134583
[ 44.6]
46.023147583
[ 44.5]
45.8677978516
[ 44.95]
45.96118927
[ 45.3]
46.2553787231
[ 44.75]
46.237651825
[ 45.75]
46.4650802612
[ 47.05]
47.3160324097
[ 46.95]
47.9277267456
[ 47.2]
48.1940994263
[ 46.75]
48.1560211182
[ 46.]
47.7303504944
[ 45.2]
47.0731811523
[ 45.1]
46.602104187
[ 44.7]
46.2713928223
[ 44.75]
46.080368042
[ 43.75]
45.637424469
[ 43.6]
45.157207489
[ 44.05]
45.1275024414
[ 44.15]
45.2909507751
[ 44.4]
45.4740028381
[ 43.6]
45.2815589905
[ 43.25]
44.8572921753
[ 43.]
44.5185050964
[ 43.95]
44.7188339233
[ 44.2]
45.150478363
[ 44.3]
45.4046325684
[ 44.4]
45.5412368774
[ 45.2]
45.9204940796
[ 46.75]
46.8738937378
[ 46.75]
47.6201324463
[ 46.95]
47.940574646
[ 46.35]
47.8473472595
[ 46.15]
47.5755348206
[ 46.45]
47.5513916016
[ 46.9]
47.8119087219
[ 46.35]
47.8125762939
[ 46.7]
47.7933006287
[ 47.1]
48.0299491882
[ 47.15]
48.2461700439
[ 46.95]
48.2531204224
[ 46.85]
48.1546859741
[ 46.6]
47.9958190918
[ 46.4]
47.8005638123
[ 45.75]
47.4289321899
[ 45.6]
47.0704154968
[ 46.15]
47.1253623962
[ 46.55]
47.4580078125
[ 46.3]
47.5834846497
[ 47.7]
48.0940246582
[ 47.7]
48.6403274536
[ 47.15]
48.6148948669
[ 48.5]
48.9673576355
[ 48.6]
49.4918518066
[ 48.15]
49.5335006714
[ 47.55]
49.1751747131
[ 47.5]
48.8660163879
[ 45.7]
48.0552978516
[ 46.15]
47.4776763916
[ 45.15]
47.0149383545
[ 44.7]
46.4314308167
[ 44.65]
46.0865592957
[ 44.2]
45.786190033
[ 43.5]
45.2945976257
[ 43.1]
44.7788391113
[ 43.3]
44.576461792
[ 43.65]
44.7081108093
[ 44.05]
45.0086898804
[ 45.3]
45.7016525269
[ 45.8]
46.4638900757
[ 45.75]
46.8323097229
[ 46.45]
47.2006950378
[ 47.7]
47.998249054
[ 46.75]
48.243686676
[ 45.8]
47.6914520264
[ 44.05]
46.5453033447
[ 44.4]
45.8315429688
[ 46.]
46.3311576843
[ 46.2]
47.0383491516
[ 50.1]
48.8342475891
[ 49.15]
50.1141433716
[ 49.1]
50.2653045654
[ 49.1]
50.2520141602
[ 48.8]
50.1428909302
[ 48.65]
49.9846115112
[ 49.55]
50.2448577881
[ 50.05]
50.776763916
[ 51.05]
51.4849090576
[ 56.5]
54.0194473267
[ 57.1]
56.564453125
[ 58.3]
58.1809539795
[ 57.85]
58.8138542175
[ 58.6]
59.1564025879
[ 57.5]
59.0580062866
[ 59.1]
59.368560791
[ 60.9]
60.6098937988
[ 59.25]
60.8901481628
[ 58.25]
60.1260452271
[ 59.35]
59.9945411682
[ 61.4]
61.0758209229
[ 61.65]
62.1237487793
[ 62.05]
62.685760498
[ 60.3]
62.2650108337
[ 61.7]
62.1880569458
[ 62.05]
62.6850318909
[ 62.75]
63.2507553101
[ 64.05]
64.1262130737
[ 63.6]
64.587600708
[ 64.4]
64.9256210327
[ 64.4]
65.2427749634
[ 64.15]
65.2610626221
[ 64.4]
65.292930603
[ 64.4]
65.3715515137
[ 64.5]
65.4433135986
[ 64.7]
65.5709838867
[ 65.]
65.7847595215
[ 65.]
65.9379272461
[ 65.]
65.987197876
[ 64.7]
65.8828430176
[ 64.75]
65.7859039307
[ 65.85]
66.1979827881
[ 63.15]
65.5875167847
[ 64.95]
65.3475952148
[ 66.3]
66.2681732178
[ 64.75]
66.3774108887
[ 64.3]
65.7991409302
[ 65.1]
65.763671875
[ 65.75]
66.2458877563
[ 66.9]
67.0512924194
[ 66.35]
67.4090118408
[ 65.9]
67.1897583008
[ 66.7]
67.2904510498
[ 65.85]
67.2209396362
[ 66.]
67.0374145508
[ 65.85]
66.9473419189
[ 64.95]
66.5324859619
[ 64.95]
66.14793396
[ 63.65]
65.5100631714
[ 61.95]
64.2879562378
[ 61.9]
63.3922576904
[ 61.35]
62.8751068115
[ 59.3]
61.7729949951
[ 60.9]
61.4635887146
[ 61.7]
62.1061210632
[ 61.45]
62.4688835144
[ 62.3]
62.8332443237
[ 63.5]
63.6346626282
[ 61.85]
63.5934143066
[ 62.95]
63.5534362793
[ 62.95]
63.8236274719
[ 70.05]
66.6647415161
[ 68.7]
69.0529174805
[ 69.1]
69.6756820679
[ 68.8]
69.7742919922
[ 68.8]
69.7317047119
[ 66.95]
69.0219650269
[ 68.45]
68.8746490479
[ 68.65]
69.3136291504
[ 69.25]
69.7743530273
[ 72.]
71.1488952637
[ 72.45]
72.5268554688
[ 72.6]
73.1681594849
[ 72.95]
73.52709198
[ 71.95]
73.3471984863
[ 73.3]
73.550567627
[ 73.05]
73.8919296265
[ 74.4]
74.4817962646
[ 72.85]
74.4281921387
[ 75.8]
75.137588501
[ 75.6]
76.0892715454
[ 74.1]
75.791305542
[ 72.6]
74.6628494263
[ 75.45]
74.9803619385
[ 72.15]
74.6065444946
[ 70.2]
72.8452453613
[ 72.85]
72.7104873657
[ 71.2]
72.7755279541
[ 73.2]
73.1678543091
[ 73.7]
73.998878479
[ 74.1]
74.5785827637
[ 73.5]
74.6297683716
[ 73.55]
74.4915084839
[ 75.7]
75.2982406616
[ 77.15]
76.7362289429
[ 77.55]
77.7772369385
[ 77.9]
78.3434677124
[ 78.35]
78.7824707031
[ 77.8]
78.8400497437
[ 77.95]
78.7728652954
[ 77.25]
78.5161209106
[ 75.75]
77.6619415283
[ 74.8]
76.5941467285
[ 74.25]
75.7673873901
[ 75.8]
75.9644546509
[ 79.4]
77.8718185425
[ 78.65]
79.2067565918
[ 78.6]
79.4063873291
[ 78.8]
79.4825744629
[ 79.9]
80.00340271
[ 81.]
80.928314209
[ 81.45]
81.718208313
[ 82.65]
82.5604553223
[ 82.1]
82.9409713745
[ 83.45]
83.4440307617
[ 83.85]
84.1280822754
[ 83.65]
84.4005432129
[ 84.35]
84.6995697021
[ 83.55]
84.6718292236
[ 84.9]
84.9784851074
[ 87.05]
86.2844161987
[ 85.1]
86.5610580444
[ 85.55]
86.283454895
[ 85.3]
86.181388855
[ 85.7]
86.2631835938
[ 85.35]
86.2705535889
[ 86.3]
86.5480194092
[ 86.1]
86.8162841797
[ 86.5]
87.0117721558
[ 85.4]
86.7462463379
[ 86.5]
86.7869110107
[ 86.45]
87.079284668
[ 88.6]
88.0194854736
[ 88.]
88.6771469116
[ 86.45]
88.1258087158
[ 85.5]
87.1188964844
[ 85.15]
86.3897857666
[ 85.5]
86.2158203125
[ 86.]
86.4642028809
[ 86.35]
86.8203659058
[ 85.9]
86.8488006592
[ 84.35]
86.1193695068
[ 84.3]
85.4259185791
[ 88.35]
86.7681121826
[ 86.85]
87.764831543
[ 86.5]
87.5575027466
[ 82.55]
85.7867126465
[ 83.75]
84.5917358398
[ 81.8]
83.7763519287
[ 84.35]
84.0211639404
[ 84.3]
84.7654571533
[ 86.45]
85.8503646851
[ 84.65]
86.0653381348
[ 85.9]
86.1258773804
[ 86.15]
86.5632095337
[ 84.4]
86.1216812134
[ 85.45]
85.900100708
[ 84.75]
85.8294830322
[ 83.1]
84.9963302612
[ 82.9]
84.1769866943
[ 83.95]
84.2577362061
[ 84.7]
84.8888092041
[ 83.6]
84.8803863525
[ 82.75]
84.2239532471
[ 83.4]
84.0083007813
[ 83.25]
84.0717163086
[ 82.45]
83.758102417
[ 83.4]
83.7944412231
[ 82.1]
83.5538787842
[ 80.7]
82.5830078125
[ 78.8]
81.1195373535
[ 78.5]
80.0030670166
[ 80.15]
80.21144104
[ 80.]
80.6973114014
[ 79.15]
80.4874343872
[ 80.85]
80.8154602051
[ 81.85]
81.7753295898
[ 82.05]
82.4682693481
[ 82.]
82.7179718018
[ 80.8]
82.3010482788
[ 80.8]
81.8339233398
[ 80.3]
81.4881591797
[ 79.55]
80.9619140625
[ 79.45]
80.5422821045
[ 79.8]
80.5115814209
[ 79.15]
80.3513946533
[ 79.7]
80.3347167969
[ 81.15]
81.0368652344
[ 80.45]
81.4027862549
[ 82.15]
81.9938201904
[ 82.55]
82.7908935547
[ 82.6]
83.1978149414
[ 81.75]
83.0013122559
[ 81.65]
82.6564788818
[ 83.2]
83.1228179932
[ 80.]
82.4694213867
[ 78.55]
80.8135986328
[ 76.7]
79.1114654541
[ 75.7]
77.6809539795
[ 76.]
77.0717773438
[ 76.25]
77.0625
[ 77.45]
77.6008605957
[ 77.35]
78.056678772
[ 77.15]
78.0939178467
[ 77.1]
78.0147781372
[ 77.8]
78.2492218018
[ 78.1]
78.6450424194
[ 78.65]
79.0782470703
[ 76.85]
78.6614303589
[ 76.9]
78.0433807373
[ 76.6]
77.7257156372
[ 72.4]
75.9228744507
[ 72.8]
74.3290481567
[ 73.45]
74.1510314941
[ 73.45]
74.3230438232
[ 75.53]
75.1915817261
[ 77.18]
76.6698303223
[ 75.9]
77.112449646
[ 74.29]
76.2578430176
[ 73.14]
75.0574798584
[ 71.46]
73.7063446045
[ 70.06]
72.2790374756
[ 69.84]
71.3451843262
[ 71.72]
71.7194900513
[ 71.89]
72.43800354
[ 73.27]
73.2615509033
[ 72.74]
73.6748046875
[ 71.08]
73.0131530762
[ 70.58]
72.1295318604
[ 71.99]
72.2550964355
[ 71.34]
72.4582672119
[ 65.98]
70.2655792236
[ 72.77]
70.6857299805
[ 83.3]
76.387802124
[ 80.71]
80.4039382935
[ 82.39]
error mean simple: 29.5545040523
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                 0.263415              0.440217              0.531429   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01    2.333997   0.001066             0.007793                 0.245509   0  
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 08:27:55
INFO - ******************MLNX*******************
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current loss is: 
INFO - 1525.369140625
INFO - current loss is: 
INFO - 1299.36962890625
INFO - current loss is: 
INFO - 1297.758056640625
INFO - current loss is: 
INFO - 1634.9915771484375
INFO - current loss is: 
INFO - 1684.0296630859375
INFO - current loss is: 
INFO - 1808.4481201171875
INFO - current loss is: 
INFO - 1594.9705810546875
INFO - current loss is: 
INFO - 1357.4510498046875
INFO - current loss is: 
INFO - 1241.1075439453125
INFO - current loss is: 
INFO - 1327.1466064453125
INFO - current loss is: 
INFO - 982.2813720703125
INFO - current loss is: 
INFO - 629.7041015625
INFO - current loss is: 
INFO - 467.0248718261719
INFO - current loss is: 
INFO - 241.00140380859375
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 9.95251750946045
INFO - current loss is: 
INFO - 114.08488464355469
INFO - current loss is: 
INFO - 445.13848876953125
INFO - current loss is: 
INFO - 1178.9683837890625
INFO - current loss is: 
INFO - 1481.1968994140625
INFO - current loss is: 
INFO - 1532.299560546875
INFO - current loss is: 
INFO - 1152.35107421875
INFO - current loss is: 
INFO - 546.5888671875
INFO - current loss is: 
INFO - 247.79550170898438
INFO - current loss is: 
INFO - 91.94004821777344
INFO - current loss is: 
INFO - 3.769763708114624
INFO - current loss is: 
INFO - 29.07732582092285
INFO - current loss is: 
INFO - 98.32257080078125
INFO - current loss is: 
INFO - 157.50828552246094
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 105.2248764038086
INFO - current loss is: 
INFO - 88.27484130859375
INFO - current loss is: 
INFO - 79.96092987060547
INFO - current loss is: 
INFO - 64.43582916259766
INFO - current loss is: 
INFO - 32.576446533203125
INFO - current loss is: 
INFO - 5.1465325355529785
INFO - current loss is: 
INFO - 15.57779598236084
INFO - current loss is: 
INFO - 38.89032745361328
INFO - current loss is: 
INFO - 64.68531036376953
INFO - current loss is: 
INFO - 89.81976318359375
INFO - current loss is: 
INFO - 59.29844665527344
INFO - current loss is: 
INFO - 21.955583572387695
INFO - current loss is: 
INFO - 1.8297995328903198
INFO - current loss is: 
INFO - 9.509061813354492
INFO - SimpleRnn: epoch num: 
INFO - 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 18.822681427001953
INFO - current loss is: 
INFO - 22.501083374023438
INFO - current loss is: 
INFO - 25.359012603759766
INFO - current loss is: 
INFO - 18.156686782836914
INFO - current loss is: 
INFO - 6.138021469116211
INFO - current loss is: 
INFO - 1.8731799125671387
INFO - current loss is: 
INFO - 12.8851957321167
INFO - current loss is: 
INFO - 10.26138973236084
INFO - current loss is: 
INFO - 8.672463417053223
INFO - current loss is: 
INFO - 4.893024444580078
INFO - current loss is: 
INFO - 3.662860155105591
INFO - current loss is: 
INFO - 7.057671070098877
INFO - current loss is: 
INFO - 11.669127464294434
INFO - current loss is: 
INFO - 5.339780807495117
INFO - SimpleRnn: epoch num: 
INFO - 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.8059651851654053
INFO - current loss is: 
INFO - 2.9395546913146973
INFO - current loss is: 
INFO - 2.278799295425415
INFO - current loss is: 
INFO - 1.8370769023895264
INFO - current loss is: 
INFO - 0.8307165503501892
INFO - current loss is: 
INFO - 1.4638001918792725
INFO - current loss is: 
INFO - 2.29205322265625
INFO - current loss is: 
INFO - 3.2268385887145996
INFO - current loss is: 
INFO - 6.198283672332764
INFO - current loss is: 
INFO - 4.250312805175781
INFO - current loss is: 
INFO - 2.8555660247802734
INFO - current loss is: 
INFO - 2.8759419918060303
INFO - current loss is: 
INFO - 1.2748829126358032
INFO - current loss is: 
INFO - 1.8398600816726685
INFO - SimpleRnn: epoch num: 
INFO - 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 3.3691556453704834
INFO - current loss is: 
INFO - 2.5439085960388184
INFO - current loss is: 
INFO - 2.3344671726226807
INFO - current loss is: 
INFO - 2.917041540145874
INFO - current loss is: 
INFO - 1.0063350200653076
INFO - current loss is: 
INFO - 2.205068349838257
INFO - current loss is: 
INFO - 2.6474926471710205
INFO - current loss is: 
INFO - 2.099207878112793
INFO - current loss is: 
INFO - 3.575057029724121
INFO - current loss is: 
INFO - 5.196619510650635
INFO - current loss is: 
INFO - 2.3495068550109863
INFO - current loss is: 
INFO - 1.7626298666000366
INFO - current loss is: 
INFO - 4.5295090675354
INFO - current loss is: 
INFO - 1.9869548082351685
INFO - SimpleRnn: epoch num: 
INFO - 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 3.360353708267212
INFO - current loss is: 
INFO - 3.4873573780059814
INFO - current loss is: 
INFO - 2.1776480674743652
INFO - current loss is: 
INFO - 1.7588090896606445
INFO - current loss is: 
INFO - 0.8916640281677246
INFO - current loss is: 
INFO - 1.0974363088607788
INFO - current loss is: 
INFO - 1.8385732173919678
INFO - current loss is: 
INFO - 5.526753902435303
INFO - current loss is: 
INFO - 9.622380256652832
INFO - current loss is: 
INFO - 7.54591178894043
INFO - current loss is: 
INFO - 1.9864954948425293
INFO - current loss is: 
INFO - 3.6502108573913574
INFO - current loss is: 
INFO - 5.834314823150635
INFO - current loss is: 
INFO - 10.624019622802734
INFO - SimpleRnn: epoch num: 
INFO - 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 5.610835075378418
INFO - current loss is: 
INFO - 2.4311752319335938
INFO - current loss is: 
INFO - 8.70497989654541
INFO - current loss is: 
INFO - 16.8701229095459
INFO - current loss is: 
INFO - 21.69060707092285
INFO - current loss is: 
INFO - 19.148822784423828
INFO - current loss is: 
INFO - 7.253591537475586
INFO - current loss is: 
INFO - 1.7721718549728394
INFO - current loss is: 
INFO - 7.505040168762207
INFO - current loss is: 
INFO - 24.103857040405273
INFO - current loss is: 
INFO - 28.70105743408203
INFO - current loss is: 
INFO - 19.89651107788086
INFO - current loss is: 
INFO - 7.743105411529541
INFO - current loss is: 
INFO - 2.5324790477752686
INFO - SimpleRnn: epoch num: 
INFO - 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 4.391618728637695
INFO - current loss is: 
INFO - 8.535747528076172
INFO - current loss is: 
INFO - 13.58376407623291
INFO - current loss is: 
INFO - 12.375212669372559
INFO - current loss is: 
INFO - 6.816643238067627
INFO - current loss is: 
INFO - 1.2747761011123657
INFO - current loss is: 
INFO - 9.858750343322754
INFO - current loss is: 
INFO - 15.196009635925293
INFO - current loss is: 
INFO - 20.987411499023438
INFO - current loss is: 
INFO - 22.898792266845703
INFO - current loss is: 
INFO - 10.28704833984375
INFO - current loss is: 
INFO - 1.5554884672164917
INFO - current loss is: 
INFO - 7.598196029663086
INFO - current loss is: 
INFO - 16.351686477661133
INFO - SimpleRnn: epoch num: 
INFO - 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 13.487628936767578
INFO - current loss is: 
INFO - 10.719810485839844
INFO - current loss is: 
INFO - 8.312881469726562
INFO - current loss is: 
INFO - 2.2281758785247803
INFO - current loss is: 
INFO - 2.9387357234954834
INFO - current loss is: 
INFO - 11.709362983703613
INFO - current loss is: 
INFO - 19.57726287841797
INFO - current loss is: 
INFO - 9.881999015808105
INFO - current loss is: 
INFO - 5.561323165893555
INFO - current loss is: 
INFO - 3.2363665103912354
INFO - current loss is: 
INFO - 2.716442108154297
INFO - current loss is: 
INFO - 2.0868873596191406
INFO - current loss is: 
INFO - 1.8759396076202393
INFO - current loss is: 
INFO - 3.235245943069458
INFO - SimpleRnn: epoch num: 
INFO - 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 5.719634532928467
INFO - current loss is: 
INFO - 4.167999267578125
INFO - current loss is: 
INFO - 2.0684006214141846
INFO - current loss is: 
INFO - 1.3785210847854614
INFO - current loss is: 
INFO - 0.8872383236885071
INFO - current loss is: 
INFO - 1.0527598857879639
INFO - current loss is: 
INFO - 1.9658710956573486
INFO - current loss is: 
INFO - 2.479903221130371
INFO - current loss is: 
INFO - 3.171571969985962
INFO - current loss is: 
INFO - 4.724691867828369
INFO - current loss is: 
INFO - 5.452524662017822
INFO - current loss is: 
INFO - 2.8729770183563232
INFO - current loss is: 
INFO - 1.2389509677886963
INFO - current loss is: 
INFO - 1.223429560661316
INFO - SimpleRnn: epoch num: 
INFO - 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.7817766666412354
INFO - current loss is: 
INFO - 2.3668088912963867
INFO - current loss is: 
INFO - 3.2907874584198
INFO - current loss is: 
INFO - 2.8882501125335693
INFO - current loss is: 
INFO - 1.4189584255218506
INFO - current loss is: 
INFO - 1.4852794408798218
INFO - current loss is: 
INFO - 4.5187578201293945
INFO - current loss is: 
INFO - 1.6220066547393799
INFO - current loss is: 
INFO - 3.2512218952178955
INFO - current loss is: 
INFO - 3.4847378730773926
INFO - current loss is: 
INFO - 1.6339025497436523
INFO - current loss is: 
INFO - 1.0533497333526611
INFO - current loss is: 
INFO - 1.1815255880355835
INFO - current loss is: 
INFO - 4.118914604187012
INFO - SimpleRnn: epoch num: 
INFO - 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 5.737415790557861
INFO - current loss is: 
INFO - 3.7870256900787354
INFO - current loss is: 
INFO - 2.129318952560425
INFO - current loss is: 
INFO - 1.6030852794647217
INFO - current loss is: 
INFO - 0.7966723442077637
INFO - current loss is: 
INFO - 2.383533239364624
INFO - current loss is: 
INFO - 5.667694568634033
INFO - current loss is: 
INFO - 1.8848665952682495
INFO - current loss is: 
INFO - 3.1050660610198975
INFO - current loss is: 
INFO - 3.4364752769470215
INFO - current loss is: 
INFO - 1.6043939590454102
INFO - current loss is: 
INFO - 2.1311068534851074
INFO - current loss is: 
INFO - 1.8333255052566528
INFO - current loss is: 
INFO - 1.9553427696228027
INFO - SimpleRnn: epoch num: 
INFO - 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.5183515548706055
INFO - current loss is: 
INFO - 2.6914126873016357
INFO - current loss is: 
INFO - 2.561518669128418
INFO - current loss is: 
INFO - 1.5798479318618774
INFO - current loss is: 
INFO - 1.109314203262329
INFO - current loss is: 
INFO - 1.009671926498413
INFO - current loss is: 
INFO - 2.3435733318328857
INFO - current loss is: 
INFO - 3.844557762145996
INFO - current loss is: 
INFO - 4.279501438140869
INFO - current loss is: 
INFO - 2.805579900741577
INFO - current loss is: 
INFO - 1.9304571151733398
INFO - current loss is: 
INFO - 0.9524633288383484
INFO - current loss is: 
INFO - 1.1669646501541138
INFO - current loss is: 
INFO - 2.8113250732421875
INFO - SimpleRnn: epoch num: 
INFO - 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 3.900700569152832
INFO - current loss is: 
INFO - 2.5096614360809326
INFO - current loss is: 
INFO - 3.2303390502929688
INFO - current loss is: 
INFO - 3.8603463172912598
INFO - current loss is: 
INFO - 2.9972479343414307
INFO - current loss is: 
INFO - 1.1219481229782104
INFO - current loss is: 
INFO - 5.440733909606934
INFO - current loss is: 
INFO - 5.917088985443115
INFO - current loss is: 
INFO - 7.088927268981934
INFO - current loss is: 
INFO - 5.011039733886719
INFO - current loss is: 
INFO - 1.5005483627319336
INFO - current loss is: 
INFO - 1.4525995254516602
INFO - current loss is: 
INFO - 2.200643301010132
INFO - current loss is: 
INFO - 1.3280870914459229
INFO - SimpleRnn: epoch num: 
INFO - 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.579021692276001
INFO - current loss is: 
INFO - 2.1968870162963867
INFO - current loss is: 
INFO - 2.097344398498535
INFO - current loss is: 
INFO - 1.6166669130325317
INFO - current loss is: 
INFO - 0.8978515267372131
INFO - current loss is: 
INFO - 1.0941036939620972
INFO - current loss is: 
INFO - 1.686129093170166
INFO - current loss is: 
INFO - 2.1425466537475586
INFO - current loss is: 
INFO - 3.4427261352539062
INFO - current loss is: 
INFO - 2.647301197052002
INFO - current loss is: 
INFO - 1.631498098373413
INFO - current loss is: 
INFO - 1.1090909242630005
INFO - current loss is: 
INFO - 1.8405194282531738
INFO - current loss is: 
INFO - 1.336196780204773
INFO - SimpleRnn: epoch num: 
INFO - 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.4630210399627686
INFO - current loss is: 
INFO - 2.206770420074463
INFO - current loss is: 
INFO - 2.257021903991699
INFO - current loss is: 
INFO - 1.3607838153839111
INFO - current loss is: 
INFO - 0.7161156535148621
INFO - current loss is: 
INFO - 1.551186203956604
INFO - current loss is: 
INFO - 1.7252895832061768
INFO - current loss is: 
INFO - 1.1112782955169678
INFO - current loss is: 
INFO - 2.604581117630005
INFO - current loss is: 
INFO - 3.0255885124206543
INFO - current loss is: 
INFO - 1.6903247833251953
INFO - current loss is: 
INFO - 1.2954555749893188
INFO - current loss is: 
INFO - 2.8769922256469727
INFO - current loss is: 
INFO - 1.274774193763733
INFO - SimpleRnn: epoch num: 
INFO - 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 3.166316270828247
INFO - current loss is: 
INFO - 3.8371148109436035
INFO - current loss is: 
INFO - 2.147474527359009
INFO - current loss is: 
INFO - 1.372318148612976
INFO - current loss is: 
INFO - 1.3626755475997925
INFO - current loss is: 
INFO - 0.9891936779022217
INFO - current loss is: 
INFO - 3.7860445976257324
INFO - current loss is: 
INFO - 2.812833547592163
INFO - current loss is: 
INFO - 3.1975486278533936
INFO - current loss is: 
INFO - 2.551623821258545
INFO - current loss is: 
INFO - 1.6114391088485718
INFO - current loss is: 
INFO - 0.8235031962394714
INFO - current loss is: 
INFO - 1.5547717809677124
INFO - current loss is: 
INFO - 1.0959458351135254
INFO - SimpleRnn: epoch num: 
INFO - 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.080125570297241
INFO - current loss is: 
INFO - 2.1387743949890137
INFO - current loss is: 
INFO - 2.0876007080078125
INFO - current loss is: 
INFO - 1.240891456604004
INFO - current loss is: 
INFO - 1.485784888267517
INFO - current loss is: 
INFO - 2.4294960498809814
INFO - current loss is: 
INFO - 2.563241958618164
INFO - current loss is: 
INFO - 1.6689890623092651
INFO - current loss is: 
INFO - 3.7720799446105957
INFO - current loss is: 
INFO - 2.872864246368408
INFO - current loss is: 
INFO - 1.9193543195724487
INFO - current loss is: 
INFO - 1.5597918033599854
INFO - current loss is: 
INFO - 0.9252172708511353
INFO - current loss is: 
INFO - 1.448492407798767
INFO - SimpleRnn: epoch num: 
INFO - 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.0644843578338623
INFO - current loss is: 
INFO - 2.793370246887207
INFO - current loss is: 
INFO - 4.221817493438721
INFO - current loss is: 
INFO - 2.8010096549987793
INFO - current loss is: 
INFO - 0.7701038122177124
INFO - current loss is: 
INFO - 3.678286552429199
INFO - current loss is: 
INFO - 10.494935989379883
INFO - current loss is: 
INFO - 6.849950790405273
INFO - current loss is: 
INFO - 4.970774173736572
INFO - current loss is: 
INFO - 2.4173367023468018
INFO - current loss is: 
INFO - 4.331460475921631
INFO - current loss is: 
INFO - 7.729282379150391
INFO - current loss is: 
INFO - 12.902928352355957
INFO - current loss is: 
INFO - 9.393619537353516
INFO - SimpleRnn: epoch num: 
INFO - 20
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.9941930770874023
INFO - current loss is: 
INFO - 2.367236614227295
INFO - current loss is: 
INFO - 2.7731411457061768
INFO - current loss is: 
INFO - 3.044163703918457
INFO - current loss is: 
INFO - 0.9533208608627319
INFO - current loss is: 
INFO - 1.7005740404129028
INFO - current loss is: 
INFO - 2.5241892337799072
INFO - current loss is: 
INFO - 2.121218681335449
INFO - current loss is: 
INFO - 2.3460166454315186
INFO - current loss is: 
INFO - 5.696755886077881
INFO - current loss is: 
INFO - 8.701874732971191
INFO - current loss is: 
INFO - 6.2350664138793945
INFO - current loss is: 
INFO - 1.9656109809875488
INFO - current loss is: 
INFO - 1.0638631582260132
INFO - SimpleRnn: epoch num: 
INFO - 21
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.2204201221466064
INFO - current loss is: 
INFO - 2.126005172729492
INFO - current loss is: 
INFO - 1.755654215812683
INFO - current loss is: 
INFO - 1.2114280462265015
INFO - current loss is: 
INFO - 1.1301732063293457
INFO - current loss is: 
INFO - 1.2153770923614502
INFO - current loss is: 
INFO - 2.0111136436462402
INFO - current loss is: 
INFO - 1.0898842811584473
INFO - current loss is: 
INFO - 2.4671237468719482
INFO - current loss is: 
INFO - 2.291818857192993
INFO - current loss is: 
INFO - 1.7417031526565552
INFO - current loss is: 
INFO - 0.8700929880142212
INFO - current loss is: 
INFO - 2.41509747505188
INFO - current loss is: 
INFO - 1.9484730958938599
INFO - SimpleRnn: epoch num: 
INFO - 22
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.941007375717163
INFO - current loss is: 
INFO - 2.00504207611084
INFO - current loss is: 
INFO - 2.6129751205444336
INFO - current loss is: 
INFO - 2.0611953735351562
INFO - current loss is: 
INFO - 0.6978982090950012
INFO - current loss is: 
INFO - 3.326598882675171
INFO - current loss is: 
INFO - 8.27403736114502
INFO - current loss is: 
INFO - 4.188079357147217
INFO - current loss is: 
INFO - 2.6892893314361572
INFO - current loss is: 
INFO - 3.383148670196533
INFO - current loss is: 
INFO - 4.110387325286865
INFO - current loss is: 
INFO - 2.851030111312866
INFO - current loss is: 
INFO - 2.018254280090332
INFO - current loss is: 
INFO - 2.001652479171753
INFO - SimpleRnn: epoch num: 
INFO - 23
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 4.2201995849609375
INFO - current loss is: 
INFO - 3.558638334274292
INFO - current loss is: 
INFO - 1.716936469078064
INFO - current loss is: 
INFO - 3.0879805088043213
INFO - current loss is: 
INFO - 5.8782453536987305
INFO - current loss is: 
INFO - 6.186146259307861
INFO - current loss is: 
INFO - 2.5694966316223145
INFO - current loss is: 
INFO - 0.9816771149635315
INFO - current loss is: 
INFO - 2.6824393272399902
INFO - current loss is: 
INFO - 2.527768850326538
INFO - current loss is: 
INFO - 1.3371946811676025
INFO - current loss is: 
INFO - 0.8240308165550232
INFO - current loss is: 
INFO - 0.8259682059288025
INFO - current loss is: 
INFO - 4.165245056152344
INFO - SimpleRnn: epoch num: 
INFO - 24
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 5.724249362945557
INFO - current loss is: 
INFO - 4.365588665008545
INFO - current loss is: 
INFO - 1.7905945777893066
INFO - current loss is: 
INFO - 2.4799370765686035
INFO - current loss is: 
INFO - 5.060133934020996
INFO - current loss is: 
INFO - 5.704469203948975
INFO - current loss is: 
INFO - 2.58701753616333
INFO - current loss is: 
INFO - 0.9609858989715576
INFO - current loss is: 
INFO - 5.8882551193237305
INFO - current loss is: 
INFO - 13.80226993560791
INFO - current loss is: 
INFO - 14.62922477722168
INFO - current loss is: 
INFO - 8.415103912353516
INFO - current loss is: 
INFO - 2.1217987537384033
INFO - current loss is: 
INFO - 1.1567479372024536
INFO - SimpleRnn: epoch num: 
INFO - 25
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 2.682676315307617
INFO - current loss is: 
INFO - 2.8175580501556396
INFO - current loss is: 
INFO - 2.36226487159729
INFO - current loss is: 
INFO - 1.264376163482666
INFO - current loss is: 
INFO - 1.0382171869277954
INFO - current loss is: 
INFO - 0.9454909563064575
INFO - current loss is: 
INFO - 1.5960434675216675
INFO - current loss is: 
INFO - 1.8271883726119995
INFO - current loss is: 
INFO - 2.2952187061309814
INFO - current loss is: 
INFO - 2.833106279373169
INFO - current loss is: 
INFO - 3.2495391368865967
INFO - current loss is: 
INFO - 1.510604977607727
INFO - current loss is: 
INFO - 1.1595922708511353
INFO - current loss is: 
INFO - 1.3181414604187012
INFO - SimpleRnn: epoch num: 
INFO - 26
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.7931135892868042
INFO - current loss is: 
INFO - 1.881879210472107
INFO - current loss is: 
INFO - 1.6673871278762817
INFO - current loss is: 
INFO - 2.2024965286254883
INFO - current loss is: 
INFO - 2.0383026599884033
INFO - current loss is: 
INFO - 1.4042139053344727
INFO - current loss is: 
INFO - 1.4482908248901367
INFO - current loss is: 
INFO - 1.5453286170959473
INFO - current loss is: 
INFO - 2.223217010498047
INFO - current loss is: 
INFO - 2.676246404647827
INFO - current loss is: 
INFO - 2.7798516750335693
INFO - current loss is: 
INFO - 1.1500755548477173
INFO - current loss is: 
INFO - 1.5790461301803589
INFO - current loss is: 
INFO - 1.9275538921356201
INFO - SimpleRnn: epoch num: 
INFO - 27
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.7837051153182983
INFO - current loss is: 
INFO - 2.32442307472229
INFO - current loss is: 
INFO - 1.961262822151184
INFO - current loss is: 
INFO - 1.180101990699768
INFO - current loss is: 
INFO - 1.5681651830673218
INFO - current loss is: 
INFO - 2.6739494800567627
INFO - current loss is: 
INFO - 1.7494276762008667
INFO - current loss is: 
INFO - 0.9486134052276611
INFO - current loss is: 
INFO - 2.2039778232574463
INFO - current loss is: 
INFO - 2.0016016960144043
INFO - current loss is: 
INFO - 1.2625327110290527
INFO - current loss is: 
INFO - 1.0759458541870117
INFO - current loss is: 
INFO - 1.8420865535736084
INFO - current loss is: 
INFO - 0.9628340005874634
INFO - SimpleRnn: epoch num: 
INFO - 28
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 1.705851674079895
INFO - current loss is: 
INFO - 2.0763838291168213
INFO - current loss is: 
INFO - 2.3710477352142334
INFO - current loss is: 
INFO - 1.1669801473617554
INFO - current loss is: 
INFO - 1.4036263227462769
INFO - current loss is: 
INFO - 2.9728000164031982
INFO - current loss is: 
INFO - 3.577338695526123
INFO - current loss is: 
INFO - 0.9171051979064941
INFO - current loss is: 
INFO - 3.8852508068084717
INFO - current loss is: 
INFO - 6.986715316772461
INFO - current loss is: 
INFO - 4.876641273498535
INFO - current loss is: 
INFO - 1.807600975036621
INFO - current loss is: 
INFO - 0.7357512712478638
INFO - current loss is: 
INFO - 6.7104411125183105
INFO - SimpleRnn: epoch num: 
INFO - 29
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current loss is: 
INFO - 10.878646850585938
INFO - current loss is: 
INFO - 11.401140213012695
INFO - current loss is: 
INFO - 7.168765068054199
INFO - current loss is: 
INFO - 3.6656150817871094
INFO - current loss is: 
INFO - 0.6427507400512695
INFO - current loss is: 
INFO - 2.088500499725342
INFO - current loss is: 
INFO - 2.1393251419067383
INFO - current loss is: 
INFO - 1.2046828269958496
INFO - current loss is: 
INFO - 2.3126959800720215
INFO - current loss is: 
INFO - 3.3087451457977295
INFO - current loss is: 
INFO - 2.2045018672943115
INFO - current loss is: 
INFO - 0.6682940721511841
INFO - current loss is: 
INFO - 1.1607120037078857
INFO - current loss is: 
INFO - 0.947805643081665
INFO - all epochs loss are: 
INFO - [1220.760979788644, 506.35672656127383, 48.37039602654321, 11.235155599457878, 2.640689436878477, 2.7516395705086842, 4.242994896003178, 12.426025467259544, 10.836510385785784, 6.969147290502276, 2.7431767199720656, 2.5068769795554027, 2.718237348965236, 2.2574671123709, 3.3971365434782848, 1.8798488250800542, 1.868513094527381, 2.16491453562464, 2.013764577252524, 5.387097179889679, 3.177658941064562, 1.7495975068637304, 3.011471139533179, 2.8972833710057393, 5.3347411922046115, 1.9214298725128174, 1.8797859379223414, 1.6813305275780814, 2.9423953124455045, 3.556584366730281]
INFO - train_total_time: 
INFO - -1562.7621655464172
INFO - optimizer_step_time: 
INFO - [0.001001119613647461, 0.0, 0.0, 0.0, 0.0, 0.0009980201721191406, 0.000997781753540039, 0.0, 0.0, 0.0009844303131103516, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0009965896606445312, 0.0, 0.0009996891021728516, 0.0009999275207519531, 0.0010001659393310547, 0.0, 0.0, 0.0009992122650146484, 0.0010004043579101562, 0.0, 0.0010018348693847656, 0.00099945068359375, 0.0, 0.0, 0.0009853839874267578, 0.0010132789611816406, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009846687316894531, 0.0010001659393310547, 0.0009987354278564453, 0.0, 0.0, 0.0, 0.0009984970092773438, 0.0009984970092773438, 0.0009837150573730469, 0.0009984970092773438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009877681732177734, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0009996891021728516, 0.0010008811950683594, 0.0, 0.0010004043579101562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0010008811950683594, 0.0, 0.0010001659393310547, 0.0010013580322265625, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0009849071502685547, 0.00098419189453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0010001659393310547, 0.0, 0.0, 0.0010008811950683594, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009968280792236328, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0, 0.0010035037994384766, 0.0, 0.0, 0.0009989738464355469, 0.0010013580322265625, 0.0, 0.0, 0.0, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010008811950683594, 0.0009996891021728516, 0.0, 0.0009989738464355469, 0.0010001659393310547, 0.0009975433349609375, 0.0010020732879638672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0010004043579101562, 0.0010073184967041016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0009996891021728516, 0.0009999275207519531, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0010006427764892578, 0.0010030269622802734, 0.0, 0.0, 0.0010008811950683594, 0.0, 0.0, 0.0, 0.0, 0.0010035037994384766, 0.0009996891021728516, 0.0, 0.0010006427764892578, 0.0, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.00099945068359375, 0.0009992122650146484, 0.0, 0.0009870529174804688, 0.0010080337524414062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000997781753540039, 0.0, 0.0, 0.0010006427764892578, 0.0, 0.0, 0.00099945068359375, 0.0, 0.0010001659393310547, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.00099945068359375, 0.0009846687316894531, 0.0, 0.0009984970092773438, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0010001659393310547, 0.0009922981262207031, 0.0009870529174804688, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0009961128234863281, 0.0, 0.0, 0.0010006427764892578, 0.0, 0.0009989738464355469, 0.0, 0.0019974708557128906, 0.0009996891021728516, 0.0, 0.0009992122650146484, 0.003493070602416992, 0.0, 0.0009989738464355469, 0.0009996891021728516, 0.000997781753540039, 0.0009980201721191406, 0.0, 0.0, 0.0009999275207519531, 0.0009987354278564453, 0.0, 0.0, 0.0, 0.0, 0.0009920597076416016, 0.0, 0.0009999275207519531, 0.0009984970092773438, 0.0, 0.0009996891021728516, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0010001659393310547, 0.0010004043579101562, 0.0009837150573730469, 0.0, 0.0010030269622802734, 0.0009903907775878906, 0.0, 0.0, 0.0009980201721191406, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0010008811950683594, 0.0, 0.0010030269622802734, 0.0, 0.0009989738464355469, 0.0010006427764892578, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.00099945068359375, 0.001001119613647461, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0009984970092773438, 0.0, 0.0, 0.0, 0.0010025501251220703, 0.0009996891021728516, 0.0, 0.0009996891021728516, 0.0010006427764892578, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0010001659393310547, 0.0009992122650146484, 0.0, 0.0009877681732177734, 0.0009996891021728516, 0.0009989738464355469, 0.0010018348693847656, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.0010004043579101562, 0.00099945068359375, 0.0, 0.0010001659393310547, 0.0, 0.0010001659393310547, 0.0009996891021728516, 0.0, 0.0010001659393310547, 0.0009899139404296875, 0.0, 0.0, 0.0, 0.0, 0.0010035037994384766, 0.0, 0.0010001659393310547, 0.0009996891021728516, 0.001001119613647461, 0.0, 0.0009996891021728516, 0.0, 0.0010001659393310547, 0.0, 0.0009980201721191406, 0.001013040542602539, 0.0009996891021728516, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0010111331939697266, 0.0, 0.0010001659393310547, 0.0009975433349609375, 0.0, 0.0010008811950683594, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0009980201721191406, 0.0009984970092773438, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0010004043579101562, 0.0, 0.000997781753540039, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0010004043579101562, 0.0010001659393310547, 0.001001596450805664, 0.0, 0.0009963512420654297, 0.0, 0.0009992122650146484, 0.0010001659393310547, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0010042190551757812, 0.0009987354278564453]
INFO - backward_step_time: 
INFO - [0.014703035354614258, 0.028982162475585938, 0.048985958099365234, 0.053969383239746094, 0.0679619312286377, 0.0879511833190918, 0.09036040306091309, 0.10893583297729492, 0.12292861938476562, 0.13893556594848633, 0.14891409873962402, 0.17589712142944336, 0.17789626121520996, 0.18789243698120117, 0.21297168731689453, 0.21077513694763184, 0.28983545303344727, 0.3098289966583252, 0.2598536014556885, 0.2528538703918457, 0.2888340950012207, 0.29583024978637695, 0.30380845069885254, 0.30083584785461426, 0.3151376247406006, 0.3458559513092041, 0.3447999954223633, 0.3670334815979004, 0.36576342582702637, 0.38876795768737793, 0.396892786026001, 0.42277097702026367, 0.43323445320129395, 0.44008755683898926, 0.5339877605438232, 0.6616184711456299, 0.6236417293548584, 0.5117313861846924, 0.5137069225311279, 0.6176433563232422, 0.5406725406646729, 0.5766677856445312, 0.596656084060669, 0.5996546745300293, 0.5776689052581787, 0.6016533374786377, 0.6166460514068604, 0.616645336151123, 0.6486241817474365, 0.6746149063110352, 0.6806089878082275, 0.6656737327575684, 0.6966021060943604, 0.7445847988128662, 0.7369117736816406, 0.7545807361602783, 0.745943546295166, 0.7766873836517334, 0.7530405521392822, 0.8354697227478027, 0.7639951705932617, 0.9031217098236084, 0.8067095279693604, 0.8500406742095947, 0.9036712646484375, 0.8450267314910889, 0.8831474781036377, 0.871906042098999, 0.8644285202026367, 0.9665517807006836, 0.8974878787994385, 0.9652812480926514, 1.108555793762207, 0.9860410690307617, 1.1926515102386475, 1.167330265045166, 1.0406501293182373, 1.0140204429626465, 1.0120766162872314, 1.0832946300506592, 1.0353448390960693, 1.2426471710205078, 1.1143755912780762, 1.048396110534668, 1.1740539073944092, 1.1508209705352783, 1.1578385829925537, 1.15248703956604, 1.1669070720672607, 1.179013967514038, 1.240971565246582, 1.187258243560791, 1.2063214778900146, 1.2932541370391846, 1.2123041152954102, 1.2253215312957764, 1.2592968940734863, 1.2395000457763672, 1.360227108001709, 1.2752940654754639, 1.2932579517364502, 1.3652114868164062, 1.300250768661499, 1.3382303714752197, 1.36521315574646, 1.3931841850280762, 1.4211838245391846, 1.415283203125, 1.458165168762207, 1.4224085807800293, 1.4059555530548096, 1.4731574058532715, 1.4573869705200195, 1.477165699005127, 1.6301023960113525, 1.6580445766448975, 1.9408836364746094, 1.8169476985931396, 1.5371158123016357, 1.7999637126922607, 2.3486504554748535, 1.57509446144104, 1.8929364681243896, 1.6780335903167725, 1.6600453853607178, 1.63905668258667, 1.6777722835540771, 1.679121494293213, 1.6977794170379639, 1.7348856925964355, 1.801262617111206, 1.8525035381317139, 1.7719752788543701, 1.748039722442627, 1.8370089530944824, 1.8553762435913086, 1.8229329586029053, 1.8025269508361816, 1.8356666564941406, 1.837615966796875, 1.8674061298370361, 1.8754260540008545, 1.9136676788330078, 1.9465663433074951, 1.9926469326019287, 1.9320037364959717, 1.9804542064666748, 1.971461296081543, 1.9700639247894287, 1.9619078636169434, 2.0388236045837402, 2.0055947303771973, 2.052520513534546, 2.016357421875, 1.988574743270874, 2.1160998344421387, 2.1703341007232666, 2.1165263652801514, 2.067171096801758, 2.2380478382110596, 2.3326737880706787, 2.1114048957824707, 2.1320831775665283, 2.1372358798980713, 2.303901433944702, 2.1858294010162354, 2.180907964706421, 2.157775640487671, 2.273987054824829, 2.239142656326294, 2.2443861961364746, 2.315699338912964, 2.2537519931793213, 2.3027255535125732, 2.301758289337158, 2.2812769412994385, 2.502706527709961, 2.5704758167266846, 2.3291192054748535, 2.518040418624878, 2.4866716861724854, 2.371636390686035, 2.4625589847564697, 2.3917012214660645, 2.57464599609375, 2.6691832542419434, 2.4459927082061768, 2.550476312637329, 2.9346225261688232, 2.724452018737793, 2.6884517669677734, 2.517564535140991, 2.721432685852051, 2.6594691276550293, 2.6574721336364746, 2.8133952617645264, 2.7793846130371094, 2.817593574523926, 2.7843949794769287, 2.6978654861450195, 2.851719379425049, 2.678481340408325, 3.330141067504883, 3.4994139671325684, 3.5528411865234375, 3.254110813140869, 3.0686564445495605, 2.9513320922851562, 2.881340742111206, 2.8883442878723145, 2.799562454223633, 2.822648286819458, 2.860013723373413, 2.9822301864624023, 3.509047746658325, 3.309100389480591, 3.1831796169281006, 3.042266607284546, 3.6877315044403076, 3.437303304672241, 3.0741288661956787, 3.4630022048950195, 3.2021634578704834, 2.8991262912750244, 3.135188102722168, 3.1781811714172363, 3.1305360794067383, 3.0688886642456055, 3.148928165435791, 3.080566644668579, 3.0968611240386963, 3.0691885948181152, 3.1322274208068848, 3.0552549362182617, 3.092111349105835, 3.450648546218872, 4.002697706222534, 4.569322109222412, 12.350858449935913, 5.691029071807861, 5.896615266799927, 10.607239484786987, 7.163830995559692, 3.9523913860321045, 3.6421029567718506, 4.4936888217926025, 4.046013593673706, 3.72025465965271, 6.487749338150024, 5.446903467178345, 4.522398948669434, 4.208578824996948, 8.430078029632568, 9.69116997718811, 4.6718690395355225, 4.3763909339904785, 4.603352069854736, 4.229568958282471, 4.0016961097717285, 4.603861331939697, 4.10563850402832, 3.9147465229034424, 4.578366994857788, 4.256555795669556, 5.475751876831055, 4.107999563217163, 6.450804710388184, 6.520472764968872, 4.773669481277466, 4.50951361656189, 5.779838562011719, 5.263906002044678, 5.388901472091675, 4.010692834854126, 4.320516347885132, 4.13861083984375, 4.466414928436279, 4.649311065673828, 4.6863181591033936, 4.992128133773804, 4.361853122711182, 4.58044171333313, 4.485423803329468, 4.168600082397461, 4.126626253128052, 4.3315064907073975, 4.768256425857544, 4.262549161911011, 4.570354700088501, 4.601349115371704, 4.537391662597656, 4.846216201782227, 4.533390760421753, 4.777258396148682, 4.744288444519043, 4.778249502182007, 4.976136207580566, 5.030104637145996, 4.489418029785156, 4.860705852508545, 4.706292390823364, 4.8802289962768555, 5.224995374679565, 5.61478066444397, 5.772970676422119, 4.460434913635254, 5.103065013885498, 5.347066640853882, 4.532440662384033, 4.514547109603882, 4.930162668228149, 4.5553789138793945, 4.84821081161499, 5.5877978801727295, 6.125861644744873, 4.946921348571777, 4.697296619415283, 4.808236598968506, 4.728277683258057, 5.0391013622283936, 4.702295303344727, 4.696298837661743, 4.860202312469482, 5.059090614318848, 5.005119562149048, 4.837216377258301, 4.818227529525757, 4.750266075134277, 4.801239252090454, 4.908177614212036, 5.0131165981292725, 4.945155143737793, 4.827233552932739, 5.075079917907715, 4.82023024559021, 5.151036977767944, 4.969125509262085, 4.930163145065308, 4.956149578094482, 4.907177209854126, 4.979136228561401, 5.096068859100342, 4.982133626937866, 4.9551496505737305, 5.323944807052612, 5.077078580856323, 5.183018207550049, 4.993111848831177, 5.130048990249634, 5.14403772354126, 5.054110765457153, 5.053093910217285, 5.080079555511475, 5.20600700378418, 5.19201397895813, 5.252990245819092, 5.064090013504028, 5.624766826629639, 5.741696357727051, 6.718764781951904, 5.468855142593384, 5.296950101852417, 5.664726257324219, 5.391895532608032, 5.388899087905884, 5.652747631072998, 5.945580720901489, 6.144469261169434, 5.495837211608887, 5.600778818130493, 5.54679536819458, 5.504834890365601, 5.511848211288452, 5.530819416046143, 5.749692678451538, 5.862624883651733, 5.6257641315460205, 6.000651597976685, 5.536795616149902, 5.534818172454834, 5.579789161682129, 5.565799951553345, 5.578787565231323, 5.564798355102539, 5.681731939315796, 5.876620054244995, 5.753691673278809, 5.645751953125, 5.7456793785095215, 5.72370982170105, 5.845088481903076, 5.791667938232422, 5.86662220954895, 5.802661418914795, 5.981557607650757, 5.800663471221924, 6.213417053222656, 5.840636968612671, 6.33235764503479, 7.394353628158569, 6.134223699569702, 5.83564019203186, 5.875617265701294, 5.916595220565796, 6.297415018081665, 5.775675535202026, 6.101490497589111, 5.807631492614746, 6.195436000823975, 5.985617637634277, 5.925593137741089, 6.0104899406433105, 5.96556830406189, 6.246781587600708, 6.2489752769470215, 5.967582941055298, 6.056516885757446, 6.00354266166687, 6.131473064422607, 6.143468379974365]
INFO - optimizer_zero_grad_time: 
INFO - [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000997781753540039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009984970092773438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009984970092773438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009970664978027344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009837150573730469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009853839874267578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010161399841308594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
INFO - epoch_step_time: 
INFO - [7.894028902053833, 10.084741115570068, 13.115198135375977, 15.70119833946228, 17.799395084381104, 21.078266143798828, 23.040990829467773, 25.373817682266235, 30.184202194213867, 30.968343257904053, 33.72839665412903, 36.18337440490723, 39.14944767951965, 43.07167148590088, 48.835936069488525, 50.41688847541809, 52.81413435935974, 89.54321503639221, 78.7394711971283, 78.84073901176453, 71.82740616798401, 76.73890495300293, 76.32146406173706, 75.60552501678467, 77.1486349105835, 82.23074793815613, 86.20196557044983, 86.03697538375854, 92.9049289226532, 91.18116497993469]
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:212: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(0.3868), tensor(0.2767), tensor(0.9729), tensor(1.5152), tensor(1.1182), tensor(1.0718)]
32.9286346436
[ 47.7]
47.4663734436
[ 47.15]
48.2094497681
[ 47.4]
47.1121025085
[ 47.15]
46.7834396362
[ 47.8]
47.0650749207
[ 48.]
47.5338401794
[ 45.8]
46.7066230774
[ 46.15]
45.9032440186
[ 47.1]
46.2268333435
[ 47.5]
46.8966217041
[ 48.15]
47.5141563416
[ 48.15]
47.8251991272
[ 47.9]
47.7519226074
[ 48.05]
47.6909675598
[ 47.95]
47.6731910706
[ 47.5]
47.4465751648
[ 47.95]
47.45054245
[ 47.7]
47.480884552
[ 47.7]
47.4300956726
[ 46.9]
47.0360908508
[ 46.4]
46.4544143677
[ 46.65]
46.2644004822
[ 45.1]
45.6198196411
[ 45.15]
45.017742157
[ 46.]
45.262462616
[ 44.8]
45.1055870056
[ 43.9]
44.2895317078
[ 43.5]
43.5674781799
[ 43.8]
43.4294013977
[ 43.6]
43.4571151733
[ 44.25]
43.7288551331
[ 44.05]
43.8994941711
[ 44.1]
43.9020423889
[ 43.8]
43.7537345886
[ 43.25]
43.3687019348
[ 43.85]
43.3730583191
[ 42.5]
42.9594078064
[ 43.3]
42.8196601868
[ 42.5]
42.6508560181
[ 43.05]
42.6543922424
[ 42.9]
42.7446556091
[ 43.6]
43.0634117126
[ 43.85]
43.4675865173
[ 44.35]
43.8790168762
[ 45.]
44.41198349
[ 44.8]
44.6442222595
[ 45.]
44.7078742981
[ 45.05]
44.7781867981
[ 44.7]
44.6536369324
[ 44.95]
44.6231193542
[ 44.6]
44.5304031372
[ 44.5]
44.3644599915
[ 44.95]
44.4908561707
[ 45.3]
44.8379478455
[ 44.75]
44.7951011658
[ 45.75]
45.0490989685
[ 47.05]
46.0053062439
[ 46.95]
46.6388702393
[ 47.2]
46.8503913879
[ 46.75]
46.7051734924
[ 46.]
46.1680641174
[ 45.2]
45.4221839905
[ 45.1]
44.951675415
[ 44.7]
44.6563911438
[ 44.75]
44.5212745667
[ 43.75]
44.0547828674
[ 43.6]
43.5624198914
[ 44.05]
43.5948829651
[ 44.15]
43.8344688416
[ 44.4]
44.0625953674
[ 43.6]
43.8125152588
[ 43.25]
43.318561554
[ 43.]
42.9538345337
[ 43.95]
43.2509346008
[ 44.2]
43.7726554871
[ 44.3]
44.0463066101
[ 44.4]
44.1511878967
[ 45.2]
44.549987793
[ 46.75]
45.6122131348
[ 46.75]
46.3763694763
[ 46.95]
46.6290473938
[ 46.35]
46.4021339417
[ 46.15]
46.0418052673
[ 46.45]
46.022480011
[ 46.9]
46.3491249084
[ 46.35]
46.3420982361
[ 46.7]
46.3136405945
[ 47.1]
46.5733680725
[ 47.15]
46.8085594177
[ 46.95]
46.7878684998
[ 46.85]
46.6505699158
[ 46.6]
46.4601745605
[ 46.4]
46.2503242493
[ 45.75]
45.8434486389
[ 45.6]
45.4760780334
[ 46.15]
45.5976219177
[ 46.55]
46.0176124573
[ 46.3]
46.1523170471
[ 47.7]
46.724811554
[ 47.7]
47.2882080078
[ 47.15]
47.1872062683
[ 48.5]
47.5505714417
[ 48.6]
48.1000175476
[ 48.15]
48.0946884155
[ 47.55]
47.6269264221
[ 47.5]
47.2681465149
[ 45.7]
46.3548927307
[ 46.15]
45.7924842834
[ 45.15]
45.3423843384
[ 44.7]
44.7800827026
[ 44.65]
44.4719200134
[ 44.2]
44.2024345398
[ 43.5]
43.6987228394
[ 43.1]
43.1698913574
[ 43.3]
43.0149269104
[ 43.65]
43.2356147766
[ 44.05]
43.6127891541
[ 45.3]
44.4039039612
[ 45.8]
45.2113876343
[ 45.75]
45.530708313
[ 46.45]
45.8590126038
[ 47.7]
46.7023735046
[ 46.75]
46.881023407
[ 45.8]
46.1644935608
[ 44.05]
44.8114051819
[ 44.4]
44.0950317383
[ 46.]
44.8183441162
[ 46.2]
45.6975135803
[ 50.1]
47.7800559998
[ 49.15]
49.0316505432
[ 49.1]
48.9910774231
[ 49.1]
48.7944602966
[ 48.8]
48.6006507874
[ 48.65]
48.4172363281
[ 49.55]
48.746219635
[ 50.05]
49.3565330505
[ 51.05]
50.1408500671
[ 56.5]
53.0952262878
[ 57.1]
55.8040428162
[ 58.3]
57.2337608337
[ 57.85]
57.5358123779
[ 58.6]
57.7148284912
[ 57.5]
57.4484596252
[ 59.1]
57.7869644165
[ 60.9]
59.1902351379
[ 59.25]
59.4053344727
[ 58.25]
58.4202957153
[ 59.35]
58.2502632141
[ 61.4]
59.5469093323
[ 61.65]
60.7256088257
[ 62.05]
61.2577667236
[ 60.3]
60.6029624939
[ 61.7]
60.4792366028
[ 62.05]
61.0414009094
[ 62.75]
61.7006645203
[ 64.05]
62.6546096802
[ 63.6]
63.0651473999
[ 64.4]
63.3561058044
[ 64.4]
63.6226577759
[ 64.15]
63.5824928284
[ 64.4]
63.5857124329
[ 64.4]
63.6569824219
[ 64.5]
63.7336730957
[ 64.7]
63.8713912964
[ 65.]
64.1035690308
[ 65.]
64.2542495728
[ 65.]
64.2830123901
[ 64.7]
64.1370773315
[ 64.75]
64.0240097046
[ 65.85]
64.5173339844
[ 63.15]
63.7688407898
[ 64.95]
63.5469551086
[ 66.3]
64.6304702759
[ 64.75]
64.7303237915
[ 64.3]
64.0186691284
[ 65.1]
63.9692192078
[ 65.75]
64.5563430786
[ 66.9]
65.4928207397
[ 66.35]
65.8173141479
[ 65.9]
65.4737625122
[ 66.7]
65.5481567383
[ 65.85]
65.4441299438
[ 66.]
65.2514877319
[ 65.85]
65.1539840698
[ 64.95]
64.69115448
[ 64.95]
64.2919082642
[ 63.65]
63.5967483521
[ 61.95]
62.2655143738
[ 61.9]
61.37292099
[ 61.35]
60.9296913147
[ 59.3]
59.7917747498
[ 60.9]
59.6041030884
[ 61.7]
60.4466972351
[ 61.45]
60.8956489563
[ 62.3]
61.2768592834
[ 63.5]
62.1294975281
[ 61.85]
61.9707450867
[ 62.95]
61.883605957
[ 62.95]
62.1482925415
[ 70.05]
65.5243835449
[ 68.7]
67.992805481
[ 69.1]
68.3867263794
[ 68.8]
68.1654434204
[ 68.8]
67.975189209
[ 66.95]
67.0995254517
[ 68.45]
67.0032119751
[ 68.65]
67.5501785278
[ 69.25]
68.1107559204
[ 72.]
69.6626815796
[ 72.45]
71.1068725586
[ 72.6]
71.6585083008
[ 72.95]
71.8828659058
[ 71.95]
71.5370407104
[ 73.3]
71.7488861084
[ 73.05]
72.1101379395
[ 74.4]
72.7858428955
[ 72.85]
72.6318206787
[ 75.8]
73.4424819946
[ 75.6]
74.4405899048
[ 74.1]
73.9987258911
[ 72.6]
72.614692688
[ 75.45]
73.0576095581
[ 72.15]
72.6359863281
[ 70.2]
70.6816558838
[ 72.85]
70.6653823853
[ 71.2]
70.8427810669
[ 73.2]
71.4179534912
[ 73.7]
72.339225769
[ 74.1]
72.9387359619
[ 73.5]
72.8790054321
[ 73.55]
72.6475830078
[ 75.7]
73.569984436
[ 77.15]
75.1847915649
[ 77.55]
76.2529525757
[ 77.9]
76.7185134888
[ 78.35]
77.0574874878
[ 77.8]
77.0037689209
[ 77.95]
76.8751068115
[ 77.25]
76.5510177612
[ 75.75]
75.5776290894
[ 74.8]
74.4175949097
[ 74.25]
73.5937652588
[ 75.8]
73.9944000244
[ 79.4]
76.3038024902
[ 78.65]
77.7272949219
[ 78.6]
77.7618637085
[ 78.8]
77.6581039429
[ 79.9]
78.1857833862
[ 81.]
79.2066726685
[ 81.45]
80.0280227661
[ 82.65]
80.8790130615
[ 82.1]
81.1602020264
[ 83.45]
81.6445541382
[ 83.85]
82.3300857544
[ 83.65]
82.5549316406
[ 84.35]
82.8236312866
[ 83.55]
82.7125778198
[ 84.9]
83.0552062988
[ 87.05]
84.5365219116
[ 85.1]
84.7316665649
[ 85.55]
84.321723938
[ 85.3]
84.1284332275
[ 85.7]
84.2413253784
[ 85.35]
84.2560043335
[ 86.3]
84.590965271
[ 86.1]
84.8681564331
[ 86.5]
85.0682754517
[ 85.4]
84.7045440674
[ 86.5]
84.7633743286
[ 86.45]
85.0917510986
[ 88.6]
86.1930465698
[ 88.]
86.8508987427
[ 86.45]
86.1022949219
[ 85.5]
84.8855056763
[ 85.15]
84.1102676392
[ 85.5]
84.0456771851
[ 86.]
84.4401931763
[ 86.35]
84.8829727173
[ 85.9]
84.8815078735
[ 84.35]
83.993347168
[ 84.3]
83.2275619507
[ 88.35]
84.8929901123
[ 86.85]
85.9980010986
[ 86.5]
85.6686325073
[ 82.55]
83.4726715088
[ 83.75]
82.2077484131
[ 81.8]
81.4185638428
[ 84.35]
81.9581375122
[ 84.3]
82.8821716309
[ 86.45]
84.14453125
[ 84.65]
84.2275924683
[ 85.9]
84.2146682739
[ 86.15]
84.6308059692
[ 84.4]
84.0738296509
[ 85.45]
83.8332748413
[ 84.75]
83.7569580078
[ 83.1]
82.8291015625
[ 82.9]
81.9572677612
[ 83.95]
82.1629867554
[ 84.7]
82.9755630493
[ 83.6]
82.9502944946
[ 82.75]
82.1526794434
[ 83.4]
81.9215316772
[ 83.25]
82.0399398804
[ 82.45]
81.7160491943
[ 83.4]
81.7978363037
[ 82.1]
81.5147018433
[ 80.7]
80.4221496582
[ 78.8]
78.7949829102
[ 78.5]
77.6781845093
[ 80.15]
78.1346282959
[ 80.]
78.8102493286
[ 79.15]
78.5778198242
[ 80.85]
78.9473648071
[ 81.85]
80.0161514282
[ 82.05]
80.7450637817
[ 82.]
80.9152755737
[ 80.8]
80.3215713501
[ 80.8]
79.7666702271
[ 80.3]
79.4021224976
[ 79.55]
78.8649139404
[ 79.45]
78.4609451294
[ 79.8]
78.5030212402
[ 79.15]
78.3626327515
[ 79.7]
78.3857040405
[ 81.15]
79.2137527466
[ 80.45]
79.5873947144
[ 82.15]
80.2263870239
[ 82.55]
81.0411453247
[ 82.6]
81.4110565186
[ 81.75]
81.0722732544
[ 81.65]
80.6302108765
[ 83.2]
81.1922531128
[ 80.]
80.4040603638
[ 78.55]
78.5376052856
[ 76.7]
76.6782226563
[ 75.7]
75.2703475952
[ 76.]
74.8432998657
[ 76.25]
75.0419540405
[ 77.45]
75.779045105
[ 77.35]
76.2917480469
[ 77.15]
76.2739334106
[ 77.1]
76.1245269775
[ 77.8]
76.3813705444
[ 78.1]
76.8235549927
[ 78.65]
77.2940368652
[ 76.85]
76.7318954468
[ 76.9]
76.0138549805
[ 76.6]
75.6802978516
[ 72.4]
73.6480178833
[ 72.8]
72.0016479492
[ 73.45]
72.0127182007
[ 73.45]
72.4074783325
[ 75.53]
73.504234314
[ 77.18]
75.1541442871
[ 75.9]
75.492767334
[ 74.29]
74.343132019
[ 73.14]
72.9086914063
[ 71.46]
71.4464874268
[ 70.06]
69.9966964722
[ 69.84]
69.1542892456
[ 71.72]
69.8126449585
[ 71.89]
70.7320632935
[ 73.27]
71.6749649048
[ 72.74]
72.0218734741
[ 71.08]
71.1410980225
[ 70.58]
70.0995254517
[ 71.99]
70.3322906494
[ 71.34]
70.6294937134
[ 65.98]
68.0878753662
[ 72.77]
68.7852630615
[ 83.3]
75.7434463501
[ 80.71]
80.028175354
[ 82.39]
error mean simple: 28.8851113871
INFO - *****************finished executing******************
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                 0.212121              0.461957                  0.52   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01    1.953792   0.001076             0.007793                 0.206897   0  
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 65, in <module>
    from FeatureBuilder import FeatureBuilderMain
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114
    FullFeaturesDF[column] = preprocessing.normalize(FullFeaturesDF[column], norm='l2')
                 ^
IndentationError: expected an indented block
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 150
    df[column] = preprocessing.normalize(df[column], norm='l2')
     ^
IndentationError: expected an indented block
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 66, in <module>
    from FeatureBuilder import MyFeatureList
ImportError: cannot import name 'MyFeatureList'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:40:56
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
TypeError: 'module' object is not callable
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:41:43
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\lib\shape_base.py", line 535, in split
    len(indices_or_sections)
TypeError: object of type 'numpy.float64' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 106, in FeatureBuilderMain
    stock_list_splitted = np.split(stocks_list, number_of_splits)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\lib\shape_base.py", line 538, in split
    N = ary.shape[axis]
AttributeError: 'list' object has no attribute 'shape'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:41:44
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\lib\shape_base.py", line 535, in split
    len(indices_or_sections)
TypeError: object of type 'numpy.float64' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 106, in FeatureBuilderMain
    stock_list_splitted = np.split(stocks_list, number_of_splits)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\numpy\lib\shape_base.py", line 538, in split
    N = ary.shape[axis]
AttributeError: 'list' object has no attribute 'shape'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:42:49
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 110, in FeatureBuilderMain
    logging.info("****************** FeatureBuilderMain : " + stock_list + "*******************")
TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('<U40') dtype('<U40') dtype('<U40')
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:43:38
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
1.0
[array(['MLNX'],
      dtype='<U4')]
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 112, in FeatureBuilderMain
    logging.info("****************** FeatureBuilderMain : " + stock_list + "*******************")
TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('<U40') dtype('<U40') dtype('<U40')
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:45:13
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 108, in FeatureBuilderMain
1.0
    stock_list_splitted.tolist()
AttributeError: 'list' object has no attribute 'tolist'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:45:48
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
1.0
[array(['MLNX'],
      dtype='<U4')]
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 113, in FeatureBuilderMain
    logging.info("****************** FeatureBuilderMain : " + stock_list + "*******************")
TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('<U40') dtype('<U40') dtype('<U40')
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:50:14
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
[['MLNX']]
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 115, in FeatureBuilderMain
    stock_list.tolist()
AttributeError: 'list' object has no attribute 'tolist'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:50:40
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
[['MLNX', 'NVDA', 'FB'], ['INTC']]
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 115, in FeatureBuilderMain
    logging.info("****************** FeatureBuilderMain : " + stock_list + "*******************")
TypeError: must be str, not list
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/16/18 19:51:08
INFO - ****************** FeatureBuilderMain : ['MLNX']*******************
[['MLNX']]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 117, in FeatureBuilderMain
    CurrStockDataFrame = ImportStockFromWeb(stock_list,dates_range,time_granularity)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 59, in ImportStockFromWeb
    df_temp = web.DataReader(stocks_list, 'iex', start_date, end_date)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\data.py", line 322, in DataReader
    session=session).read()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 91, in read
    self._get_params(self.symbols))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\base.py", line 89, in _read_one_data
    return self._read_lines(out)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 117, in _read_lines
    return result[self.symbols]
TypeError: unhashable type: 'list'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:50:51
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\indexes\base.py", line 3078, in get_loc
    return self._engine.get_loc(key)
  File "pandas\_libs\index.pyx", line 140, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 162, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'M'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 116, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame[stock],feature_list)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\frame.py", line 2688, in __getitem__
    return self._getitem_column(key)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\frame.py", line 2695, in _getitem_column
    return self._get_item_cache(key)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\generic.py", line 2489, in _get_item_cache
    values = self._data.get(item)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\internals.py", line 4115, in get
    loc = self.items.get_loc(item)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\indexes\base.py", line 3080, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas\_libs\index.pyx", line 140, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 162, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'M'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:51:35
INFO - ****************** FeatureBuilderMain : ['MLNX']*******************
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connection.py", line 171, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\util\connection.py", line 56, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connectionpool.py", line 849, in _validate_conn
    conn.connect()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connection.py", line 314, in connect
    conn = self._new_conn()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connection.py", line 180, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x000002308B1D1278>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\requests\adapters.py", line 449, in send
    timeout=timeout
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\urllib3\util\retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.iextrading.com', port=443): Max retries exceeded with url: /1.0/stock/market/batch?symbols=SPY&types=chart&range=5y (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000002308B1D1278>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 113, in FeatureBuilderMain
    CurrStockDataFrame = ImportStockFromWeb(stock_list,dates_range,time_granularity)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 54, in ImportStockFromWeb
    ReferenceStock = web.DataReader('SPY', 'iex', start_date, end_date)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\data.py", line 322, in DataReader
    session=session).read()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 91, in read
    self._get_params(self.symbols))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\base.py", line 84, in _read_one_data
    out = self._read_url_as_StringIO(url, params=params)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\base.py", line 95, in _read_url_as_StringIO
    response = self._get_response(url, params=params)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\base.py", line 132, in _get_response
    headers=headers)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\requests\sessions.py", line 537, in get
    return self.request('GET', url, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\requests\sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\requests\sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\requests\adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.iextrading.com', port=443): Max retries exceeded with url: /1.0/stock/market/batch?symbols=SPY&types=chart&range=5y (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000002308B1D1278>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:52:05
INFO - ****************** FeatureBuilderMain : ['MLNX']*******************
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 113, in FeatureBuilderMain
    CurrStockDataFrame = ImportStockFromWeb(stock_list,dates_range,time_granularity)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 59, in ImportStockFromWeb
    df_temp = web.DataReader(stocks_list, 'iex', start_date, end_date)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\data.py", line 322, in DataReader
    session=session).read()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 91, in read
    self._get_params(self.symbols))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\base.py", line 89, in _read_one_data
    return self._read_lines(out)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 117, in _read_lines
    return result[self.symbols]
TypeError: unhashable type: 'list'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:52:44
INFO - ****************** FeatureBuilderMain : ['MLNX']*******************
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 113, in FeatureBuilderMain
    CurrStockDataFrame = ImportStockFromWeb(stock_list,dates_range,time_granularity)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 59, in ImportStockFromWeb
    df_temp = web.DataReader(stocks_list, 'iex', start_date, end_date)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\data.py", line 322, in DataReader
    session=session).read()
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 91, in read
    self._get_params(self.symbols))
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\base.py", line 89, in _read_one_data
    return self._read_lines(out)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas_datareader\iex\daily.py", line 117, in _read_lines
    return result[self.symbols]
TypeError: unhashable type: 'numpy.ndarray'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:53:22
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\indexes\base.py", line 3078, in get_loc
    return self._engine.get_loc(key)
  File "pandas\_libs\index.pyx", line 140, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 162, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'M'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 115, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame[stock],feature_list)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\frame.py", line 2688, in __getitem__
    return self._getitem_column(key)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\frame.py", line 2695, in _getitem_column
    return self._get_item_cache(key)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\generic.py", line 2489, in _get_item_cache
    values = self._data.get(item)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\internals.py", line 4115, in get
    loc = self.items.get_loc(item)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\indexes\base.py", line 3080, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas\_libs\index.pyx", line 140, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 162, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'M'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:55:25
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame[stock],feature_list)
NameError: name 'stock' is not defined
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:55:53
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\indexes\base.py", line 3078, in get_loc
    return self._engine.get_loc(key)
  File "pandas\_libs\index.pyx", line 140, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 162, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'MLNX'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame[stock],feature_list)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\frame.py", line 2688, in __getitem__
    return self._getitem_column(key)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\frame.py", line 2695, in _getitem_column
    return self._get_item_cache(key)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\generic.py", line 2489, in _get_item_cache
    values = self._data.get(item)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\internals.py", line 4115, in get
    loc = self.items.get_loc(item)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\indexes\base.py", line 3080, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas\_libs\index.pyx", line 140, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\index.pyx", line 162, in pandas._libs.index.IndexEngine.get_loc
  File "pandas\_libs\hashtable_class_helper.pxi", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas\_libs\hashtable_class_helper.pxi", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'MLNX'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:57:15
INFO - ****************** FeatureBuilderMain : MLNX*******************
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
TypeError: CalcFeatures() missing 1 required positional argument: 'feature_list'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 06:58:03
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 70, in CalcFeatures
    if feature_list.close == True:
AttributeError: type object 'MyFeatureList' has no attribute 'close'
pydev debugger: starting (pid: 17984)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
hello all, todays date & time is: 11/17/18 07:02:26
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1951, in <module>
    main()
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1945, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1305, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1312, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\_pydev_imps\_pydev_execfile.py", line 25, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 70, in CalcFeatures
    if feature_list.close == True:
AttributeError: type object 'MyFeatureList' has no attribute 'close'
pydev debugger: starting (pid: 16844)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
hello all, todays date & time is: 11/17/18 07:04:52
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1951, in <module>
    main()
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1945, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1305, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\pydevd.py", line 1312, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "C:\Users\mofir\.p2\pool\plugins\org.python.pydev.core_7.0.3.201811082356\pysrc\_pydev_imps\_pydev_execfile.py", line 25, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 70, in CalcFeatures
    if feature_list.close == True:
AttributeError: type object 'MyFeatureList' has no attribute 'close'
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 65, in <module>
    from FeatureBuilder.FeatureBuilderMain import FeatureBuilderMain
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 102
    def FeatureBuilderMain(stocks_list, (MyFeatureList) feature_list, dates_range,time_granularity):
                                        ^
SyntaxError: invalid syntax
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:07:46
INFO - ****************** FeatureBuilderMain : MLNX*******************
<class 'FeatureBuilder.FeatureBuilderMain.MyFeatureList'>
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 114, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 70, in CalcFeatures
    if feature_list.close == True:
AttributeError: type object 'MyFeatureList' has no attribute 'close'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:08:51
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 105, in FeatureBuilderMain
    print(feature_list.close)
AttributeError: type object 'MyFeatureList' has no attribute 'close'
hello all, todays date & time is: 11/17/18 07:08:51
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 594, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 105, in FeatureBuilderMain
    print(feature_list.close)
AttributeError: type object 'MyFeatureList' has no attribute 'close'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:11:26
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 589, in <module>
    print(my_feature_list.close)
AttributeError: type object 'MyFeatureList' has no attribute 'close'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
hello all, todays date & time is: 11/17/18 07:12:01
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 590, in <module>
close
open
high
low
    print(my_feature_list.close)
AttributeError: type object 'MyFeatureList' has no attribute 'close'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:12:03
close
open
high
low
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 590, in <module>
    print(my_feature_list.close)
AttributeError: type object 'MyFeatureList' has no attribute 'close'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:12:37
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 590, in <module>
    print(my_feature_list.close())
AttributeError: type object 'MyFeatureList' has no attribute 'close'
close
open
high
low
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 585, in <module>
    my_feature_list = MyFeatureList()
TypeError: __init__() missing 8 required positional arguments: 'close', 'open', 'high', 'low', 'volume', 'rolling_mean', 'daily_returns', and 'market_direction'
hello all, todays date & time is: 11/17/18 07:13:27
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:20:13
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 592, in <module>
    print(Full_feature_list.close)
AttributeError: type object 'MyFeatureList' has no attribute 'close'
close
open
high
low
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:23:20
INFO - ****************** FeatureBuilderMain : MLNX*******************
False
close
open
high
low
False
False
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 598, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 110, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 85, in CalcFeatures
    if feature_list.rolling_bands == True:
AttributeError: 'MyFeatureList' object has no attribute 'rolling_bands'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:27:19
INFO - ****************** FeatureBuilderMain : MLNX*******************
False
False
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 595, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 112, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 88, in CalcFeatures
    if feature_list.rolling_bands == True:
AttributeError: 'MyFeatureList' object has no attribute 'rolling_bands'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:28:17
INFO - ****************** FeatureBuilderMain : MLNX*******************
False
False
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 595, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 113, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 89, in CalcFeatures
    if feature_list.rolling_bands == True:
AttributeError: 'MyFeatureList' object has no attribute 'rolling_bands'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:36:12
True
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 593, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 109, in FeatureBuilderMain
    CurrFeaturesDF     = CalcFeatures(CurrStockDataFrame,feature_list)
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 85, in CalcFeatures
    if feature_list.rolling_bands == True:
AttributeError: 'MyFeatureList' object has no attribute 'rolling_bands'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:37:21
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 591, in <module>
    time_granularity=time_granularity
  File "C:\Users\mofir\egit-master\ws\MLproject\FeatureBuilder\FeatureBuilderMain.py", line 112, in FeatureBuilderMain
    print(AllStocksDfList.head(5))
AttributeError: 'list' object has no attribute 'head'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:37:41
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
[             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 619, in <module>
    for i in len(stock_list):
TypeError: 'int' object is not iterable
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:38:46
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
<bound method NDFrame.head of              close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]>
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - 1786.0401000976562
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 10266900.376810892
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 2040.2564447947912
INFO - SimpleRnn: epoch num: 
INFO - 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 616.6702023914883
INFO - SimpleRnn: epoch num: 
INFO - 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 12542.898861748832
INFO - SimpleRnn: epoch num: 
INFO - 5
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 279.198518208095
INFO - SimpleRnn: epoch num: 
INFO - 6
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 210.0362021582467
INFO - SimpleRnn: epoch num: 
INFO - 7
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 313.03868835312977
INFO - SimpleRnn: epoch num: 
INFO - 8
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 92.39824591364179
INFO - SimpleRnn: epoch num: 
INFO - 9
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 60.27529123851231
INFO - SimpleRnn: epoch num: 
INFO - 10
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 23.20534578391484
INFO - SimpleRnn: epoch num: 
INFO - 11
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 82.39384416171482
INFO - SimpleRnn: epoch num: 
INFO - 12
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 21.401617697307042
INFO - SimpleRnn: epoch num: 
INFO - 13
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 88.34499795096261
INFO - SimpleRnn: epoch num: 
INFO - 14
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 50.81461897918156
INFO - SimpleRnn: epoch num: 
INFO - 15
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 108.06161917958941
INFO - SimpleRnn: epoch num: 
INFO - 16
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 61.75735180718558
INFO - SimpleRnn: epoch num: 
INFO - 17
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 45.66605966431754
INFO - SimpleRnn: epoch num: 
INFO - 18
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 36.01828186852591
INFO - SimpleRnn: epoch num: 
INFO - 19
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:49:35
INFO - ****************** FeatureBuilderMain : MLNX*******************
             open   high     low  close   volume
2013-11-18  40.60  41.32  39.770  40.09  1412106
2013-11-19  40.00  40.90  39.864  40.40   763335
2013-11-20  40.46  40.91  39.670  40.10   650005
2013-11-21  40.57  41.90  40.110  41.82   789361
2013-11-22  41.79  42.39  41.400  41.91   574924
<bound method NDFrame.head of              close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]>
INFO - ****************** FeatureBuilderMain : NVDA*******************
               open     high      low    close   volume
2013-11-18  14.9766  15.1092  14.9009  14.9387  7924864
2013-11-19  14.8674  15.0006  14.6770  14.6960  5167934
2013-11-20  14.7056  14.7436  14.4486  14.4771  5119353
2013-11-21  14.4962  14.6199  14.4295  14.5913  5339156
2013-11-22  14.6104  14.6104  14.4200  14.4486  3966684
<bound method NDFrame.head of              close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]>
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - 1907.8538469587054
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - 3.5532491783099386e+17
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - all epochs loss are: 
INFO - [1907.8538469587054, 3.5532491783099386e+17, nan, nan, nan]
INFO - train_total_time: 
INFO - -60.15477728843689
INFO - optimizer_step_time: 
INFO - [0.0009989738464355469, 0.0, 0.0010042190551757812, 0.0010030269622802734, 0.0, 0.0010025501251220703, 0.0010001659393310547, 0.0010001659393310547, 0.001001596450805664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010025501251220703, 0.0010001659393310547, 0.0, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009989738464355469, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0009989738464355469, 0.0010001659393310547, 0.0, 0.0, 0.00099945068359375, 0.0009996891021728516, 0.00099945068359375, 0.00099945068359375, 0.00099945068359375, 0.0, 0.0, 0.0010023117065429688, 0.0010018348693847656, 0.0, 0.0, 0.0009987354278564453, 0.0010020732879638672, 0.0, 0.0009980201721191406, 0.0, 0.0, 0.0, 0.0010035037994384766, 0.0009996891021728516, 0.0010035037994384766, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0009984970092773438]
INFO - backward_step_time: 
INFO - [0.01301264762878418, 0.028981685638427734, 0.03797411918640137, 0.04597306251525879, 0.06294417381286621, 0.07095670700073242, 0.08494973182678223, 0.09894227981567383, 0.10593914985656738, 0.11993098258972168, 0.12492609024047852, 0.14991426467895508, 0.16090774536132812, 0.15890812873840332, 0.18987131118774414, 0.1968843936920166, 0.22786784172058105, 0.23188400268554688, 0.24186110496520996, 0.24287676811218262, 0.2948267459869385, 0.3647899627685547, 0.34779906272888184, 0.3188157081604004, 0.2988283634185791, 0.3058204650878906, 0.30983829498291016, 0.31781697273254395, 0.3507962226867676, 0.42175912857055664, 0.3767819404602051, 0.6262836456298828, 0.3780515193939209, 0.4279026985168457, 0.3929450511932373, 0.4128835201263428, 0.43795156478881836, 0.47670531272888184, 0.6706130504608154, 0.4957144260406494, 0.49669408798217773, 0.4997110366821289, 0.48270106315612793, 0.5506806373596191, 0.5846636295318604, 0.5256965160369873, 0.5706706047058105, 0.5556774139404297, 0.5546772480010986, 0.7035939693450928, 0.5956580638885498, 0.5926587581634521, 0.6196420192718506, 0.5916578769683838, 0.6776101589202881, 0.6216404438018799, 0.7625625133514404, 0.7305803298950195, 0.7995374202728271, 0.6936001777648926, 0.7435696125030518, 0.861504316329956, 0.7855467796325684, 0.8015382289886475, 0.7805473804473877, 0.8085348606109619, 0.8165299892425537, 0.8255255222320557, 0.7925434112548828, 0.7995419502258301]
INFO - optimizer_zero_grad_time: 
INFO - [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001001596450805664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
INFO - epoch_step_time: 
INFO - [7.065576076507568, 9.699137687683105, 13.02818512916565, 13.697462797164917, 16.66441559791565]
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:214: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan)]
nan
[ 47.7]
nan
[ 47.15]
nan
[ 47.4]
nan
[ 47.15]
nan
[ 47.8]
nan
[ 48.]
nan
[ 45.8]
nan
[ 46.15]
nan
[ 47.1]
nan
[ 47.5]
nan
[ 48.15]
nan
[ 48.15]
nan
[ 47.9]
nan
[ 48.05]
nan
[ 47.95]
nan
[ 47.5]
nan
[ 47.95]
nan
[ 47.7]
nan
[ 47.7]
nan
[ 46.9]
nan
[ 46.4]
nan
[ 46.65]
nan
[ 45.1]
nan
[ 45.15]
nan
[ 46.]
nan
[ 44.8]
nan
[ 43.9]
nan
[ 43.5]
nan
[ 43.8]
nan
[ 43.6]
nan
[ 44.25]
nan
[ 44.05]
nan
[ 44.1]
nan
[ 43.8]
nan
[ 43.25]
nan
[ 43.85]
nan
[ 42.5]
nan
[ 43.3]
nan
[ 42.5]
nan
[ 43.05]
nan
[ 42.9]
nan
[ 43.6]
nan
[ 43.85]
nan
[ 44.35]
nan
[ 45.]
nan
[ 44.8]
nan
[ 45.]
nan
[ 45.05]
nan
[ 44.7]
nan
[ 44.95]
nan
[ 44.6]
nan
[ 44.5]
nan
[ 44.95]
nan
[ 45.3]
nan
[ 44.75]
nan
[ 45.75]
nan
[ 47.05]
nan
[ 46.95]
nan
[ 47.2]
nan
[ 46.75]
nan
[ 46.]
nan
[ 45.2]
nan
[ 45.1]
nan
[ 44.7]
nan
[ 44.75]
nan
[ 43.75]
nan
[ 43.6]
nan
[ 44.05]
nan
[ 44.15]
nan
[ 44.4]
nan
[ 43.6]
nan
[ 43.25]
nan
[ 43.]
nan
[ 43.95]
nan
[ 44.2]
nan
[ 44.3]
nan
[ 44.4]
nan
[ 45.2]
nan
[ 46.75]
nan
[ 46.75]
nan
[ 46.95]
nan
[ 46.35]
nan
[ 46.15]
nan
[ 46.45]
nan
[ 46.9]
nan
[ 46.35]
nan
[ 46.7]
nan
[ 47.1]
nan
[ 47.15]
nan
[ 46.95]
nan
[ 46.85]
nan
[ 46.6]
nan
[ 46.4]
nan
[ 45.75]
nan
[ 45.6]
nan
[ 46.15]
nan
[ 46.55]
nan
[ 46.3]
nan
[ 47.7]
nan
[ 47.7]
nan
[ 47.15]
nan
[ 48.5]
nan
[ 48.6]
nan
[ 48.15]
nan
[ 47.55]
nan
[ 47.5]
nan
[ 45.7]
nan
[ 46.15]
nan
[ 45.15]
nan
[ 44.7]
nan
[ 44.65]
nan
[ 44.2]
nan
[ 43.5]
nan
[ 43.1]
nan
[ 43.3]
nan
[ 43.65]
nan
[ 44.05]
nan
[ 45.3]
nan
[ 45.8]
nan
[ 45.75]
nan
[ 46.45]
nan
[ 47.7]
nan
[ 46.75]
nan
[ 45.8]
nan
[ 44.05]
nan
[ 44.4]
nan
[ 46.]
nan
[ 46.2]
nan
[ 50.1]
nan
[ 49.15]
nan
[ 49.1]
nan
[ 49.1]
nan
[ 48.8]
nan
[ 48.65]
nan
[ 49.55]
nan
[ 50.05]
nan
[ 51.05]
nan
[ 56.5]
nan
[ 57.1]
nan
[ 58.3]
nan
[ 57.85]
nan
[ 58.6]
nan
[ 57.5]
nan
[ 59.1]
nan
[ 60.9]
nan
[ 59.25]
nan
[ 58.25]
nan
[ 59.35]
nan
[ 61.4]
nan
[ 61.65]
nan
[ 62.05]
nan
[ 60.3]
nan
[ 61.7]
nan
[ 62.05]
nan
[ 62.75]
nan
[ 64.05]
nan
[ 63.6]
nan
[ 64.4]
nan
[ 64.4]
nan
[ 64.15]
nan
[ 64.4]
nan
[ 64.4]
nan
[ 64.5]
nan
[ 64.7]
nan
[ 65.]
nan
[ 65.]
nan
[ 65.]
nan
[ 64.7]
nan
[ 64.75]
nan
[ 65.85]
nan
[ 63.15]
nan
[ 64.95]
nan
[ 66.3]
nan
[ 64.75]
nan
[ 64.3]
nan
[ 65.1]
nan
[ 65.75]
nan
[ 66.9]
nan
[ 66.35]
nan
[ 65.9]
nan
[ 66.7]
nan
[ 65.85]
nan
[ 66.]
nan
[ 65.85]
nan
[ 64.95]
nan
[ 64.95]
nan
[ 63.65]
nan
[ 61.95]
nan
[ 61.9]
nan
[ 61.35]
nan
[ 59.3]
nan
[ 60.9]
nan
[ 61.7]
nan
[ 61.45]
nan
[ 62.3]
nan
[ 63.5]
nan
[ 61.85]
nan
[ 62.95]
nan
[ 62.95]
nan
[ 70.05]
nan
[ 68.7]
nan
[ 69.1]
nan
[ 68.8]
nan
[ 68.8]
nan
[ 66.95]
nan
[ 68.45]
nan
[ 68.65]
nan
[ 69.25]
nan
[ 72.]
nan
[ 72.45]
nan
[ 72.6]
nan
[ 72.95]
nan
[ 71.95]
nan
[ 73.3]
nan
[ 73.05]
nan
[ 74.4]
nan
[ 72.85]
nan
[ 75.8]
nan
[ 75.6]
nan
[ 74.1]
nan
[ 72.6]
nan
[ 75.45]
nan
[ 72.15]
nan
[ 70.2]
nan
[ 72.85]
nan
[ 71.2]
nan
[ 73.2]
nan
[ 73.7]
nan
[ 74.1]
nan
[ 73.5]
nan
[ 73.55]
nan
[ 75.7]
nan
[ 77.15]
nan
[ 77.55]
nan
[ 77.9]
nan
[ 78.35]
nan
[ 77.8]
nan
[ 77.95]
nan
[ 77.25]
nan
[ 75.75]
nan
[ 74.8]
nan
[ 74.25]
nan
[ 75.8]
nan
[ 79.4]
nan
[ 78.65]
nan
[ 78.6]
nan
[ 78.8]
nan
[ 79.9]
nan
[ 81.]
nan
[ 81.45]
nan
[ 82.65]
nan
[ 82.1]
nan
[ 83.45]
nan
[ 83.85]
nan
[ 83.65]
nan
[ 84.35]
nan
[ 83.55]
nan
[ 84.9]
nan
[ 87.05]
nan
[ 85.1]
nan
[ 85.55]
nan
[ 85.3]
nan
[ 85.7]
nan
[ 85.35]
nan
[ 86.3]
nan
[ 86.1]
nan
[ 86.5]
nan
[ 85.4]
nan
[ 86.5]
nan
[ 86.45]
nan
[ 88.6]
nan
[ 88.]
nan
[ 86.45]
nan
[ 85.5]
nan
[ 85.15]
nan
[ 85.5]
nan
[ 86.]
nan
[ 86.35]
nan
[ 85.9]
nan
[ 84.35]
nan
[ 84.3]
nan
[ 88.35]
nan
[ 86.85]
nan
[ 86.5]
nan
[ 82.55]
nan
[ 83.75]
nan
[ 81.8]
nan
[ 84.35]
nan
[ 84.3]
nan
[ 86.45]
nan
[ 84.65]
nan
[ 85.9]
nan
[ 86.15]
nan
[ 84.4]
nan
[ 85.45]
nan
[ 84.75]
nan
[ 83.1]
nan
[ 82.9]
nan
[ 83.95]
nan
[ 84.7]
nan
[ 83.6]
nan
[ 82.75]
nan
[ 83.4]
nan
[ 83.25]
nan
[ 82.45]
nan
[ 83.4]
nan
[ 82.1]
nan
[ 80.7]
nan
[ 78.8]
nan
[ 78.5]
nan
[ 80.15]
nan
[ 80.]
nan
[ 79.15]
nan
[ 80.85]
nan
[ 81.85]
nan
[ 82.05]
nan
[ 82.]
nan
[ 80.8]
nan
[ 80.8]
nan
[ 80.3]
nan
[ 79.55]
nan
[ 79.45]
nan
[ 79.8]
nan
[ 79.15]
nan
[ 79.7]
nan
[ 81.15]
nan
[ 80.45]
nan
[ 82.15]
nan
[ 82.55]
nan
[ 82.6]
nan
[ 81.75]
nan
[ 81.65]
nan
[ 83.2]
nan
[ 80.]
nan
[ 78.55]
nan
[ 76.7]
nan
[ 75.7]
nan
[ 76.]
nan
[ 76.25]
nan
[ 77.45]
nan
[ 77.35]
nan
[ 77.15]
nan
[ 77.1]
nan
[ 77.8]
nan
[ 78.1]
nan
[ 78.65]
nan
[ 76.85]
nan
[ 76.9]
nan
[ 76.6]
nan
[ 72.4]
nan
[ 72.8]
nan
[ 73.45]
nan
[ 73.45]
nan
[ 75.53]
nan
[ 77.18]
nan
[ 75.9]
nan
[ 74.29]
nan
[ 73.14]
nan
[ 71.46]
nan
[ 70.06]
nan
[ 69.84]
nan
[ 71.72]
nan
[ 71.89]
nan
[ 73.27]
nan
[ 72.74]
nan
[ 71.08]
nan
[ 70.58]
nan
[ 71.99]
nan
[ 71.34]
nan
[ 65.98]
nan
[ 72.77]
nan
[ 83.3]
nan
[ 80.71]
nan
[ 82.39]
error mean simple: nan
C:\Users\mofir\egit-master\ws\MLproject\main\main.py:514: RuntimeWarning: invalid value encountered in greater
  buy_vector = next_predicted_value > curr_predicted_value
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:93: RuntimeWarning: invalid value encountered in long_scalars
  false_buy_ratio   = false_buy_vector.sum()/buy_vector.sum()
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:140: RuntimeWarning: invalid value encountered in greater
  buy_vector = predictad_value_shifted > last_value
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                      NaN                   1.0                   0.0   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01         NaN        0.0             0.007793                 0.491935   0  
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 3
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 4
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:52:13
INFO - ****************** FeatureBuilderMain : MLNX*******************
INFO - ****************** FeatureBuilderMain : NVDA*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - all epochs loss are: 
INFO - [nan, nan, nan]
INFO - train_total_time: 
INFO - -28.505033493041992
INFO - optimizer_step_time: 
INFO - [0.001997709274291992, 0.001001119613647461, 0.0, 0.0009970664978027344, 0.0, 0.0009996891021728516, 0.001001596450805664, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0009801387786865234, 0.0, 0.0010008811950683594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0009999275207519531, 0.0009992122650146484, 0.0009999275207519531, 0.0010001659393310547, 0.0, 0.0, 0.0010013580322265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010013580322265625, 0.0]
INFO - backward_step_time: 
INFO - [0.013991594314575195, 0.02998638153076172, 0.04797220230102539, 0.0699605941772461, 0.10694217681884766, 0.13692402839660645, 0.15591073036193848, 0.12892508506774902, 0.12992525100708008, 0.14591574668884277, 0.15391111373901367, 0.18189525604248047, 0.19289064407348633, 0.19888043403625488, 0.17189884185791016, 0.18291354179382324, 0.19186973571777344, 0.19788408279418945, 0.20588040351867676, 0.24186015129089355, 0.2528343200683594, 0.29782938957214355, 0.26784515380859375, 0.2768402099609375, 0.2948310375213623, 0.320814847946167, 0.3208134174346924, 0.3078196048736572, 0.3277912139892578, 0.3268101215362549, 0.3487987518310547, 0.3727836608886719, 0.3777801990509033, 0.36978650093078613, 0.3867771625518799, 0.42575693130493164, 0.5077075958251953, 0.4377470016479492, 0.46073365211486816, 0.45873260498046875, 0.5526797771453857, 0.4757246971130371]
INFO - optimizer_zero_grad_time: 
INFO - [0.0, 0.0, 0.0009984970092773438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
INFO - epoch_step_time: 
INFO - [7.930527687072754, 9.025158643722534, 11.548351764678955]
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:214: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan)]
nan
[ 47.7]
nan
[ 47.15]
nan
[ 47.4]
nan
[ 47.15]
nan
[ 47.8]
nan
[ 48.]
nan
[ 45.8]
nan
[ 46.15]
nan
[ 47.1]
nan
[ 47.5]
nan
[ 48.15]
nan
[ 48.15]
nan
[ 47.9]
nan
[ 48.05]
nan
[ 47.95]
nan
[ 47.5]
nan
[ 47.95]
nan
[ 47.7]
nan
[ 47.7]
nan
[ 46.9]
nan
[ 46.4]
nan
[ 46.65]
nan
[ 45.1]
nan
[ 45.15]
nan
[ 46.]
nan
[ 44.8]
nan
[ 43.9]
nan
[ 43.5]
nan
[ 43.8]
nan
[ 43.6]
nan
[ 44.25]
nan
[ 44.05]
nan
[ 44.1]
nan
[ 43.8]
nan
[ 43.25]
nan
[ 43.85]
nan
[ 42.5]
nan
[ 43.3]
nan
[ 42.5]
nan
[ 43.05]
nan
[ 42.9]
nan
[ 43.6]
nan
[ 43.85]
nan
[ 44.35]
nan
[ 45.]
nan
[ 44.8]
nan
[ 45.]
nan
[ 45.05]
nan
[ 44.7]
nan
[ 44.95]
nan
[ 44.6]
nan
[ 44.5]
nan
[ 44.95]
nan
[ 45.3]
nan
[ 44.75]
nan
[ 45.75]
nan
[ 47.05]
nan
[ 46.95]
nan
[ 47.2]
nan
[ 46.75]
nan
[ 46.]
nan
[ 45.2]
nan
[ 45.1]
nan
[ 44.7]
nan
[ 44.75]
nan
[ 43.75]
nan
[ 43.6]
nan
[ 44.05]
nan
[ 44.15]
nan
[ 44.4]
nan
[ 43.6]
nan
[ 43.25]
nan
[ 43.]
nan
[ 43.95]
nan
[ 44.2]
nan
[ 44.3]
nan
[ 44.4]
nan
[ 45.2]
nan
[ 46.75]
nan
[ 46.75]
nan
[ 46.95]
nan
[ 46.35]
nan
[ 46.15]
nan
[ 46.45]
nan
[ 46.9]
nan
[ 46.35]
nan
[ 46.7]
nan
[ 47.1]
nan
[ 47.15]
nan
[ 46.95]
nan
[ 46.85]
nan
[ 46.6]
nan
[ 46.4]
nan
[ 45.75]
nan
[ 45.6]
nan
[ 46.15]
nan
[ 46.55]
nan
[ 46.3]
nan
[ 47.7]
nan
[ 47.7]
nan
[ 47.15]
nan
[ 48.5]
nan
[ 48.6]
nan
[ 48.15]
nan
[ 47.55]
nan
[ 47.5]
nan
[ 45.7]
nan
[ 46.15]
nan
[ 45.15]
nan
[ 44.7]
nan
[ 44.65]
nan
[ 44.2]
nan
[ 43.5]
nan
[ 43.1]
nan
[ 43.3]
nan
[ 43.65]
nan
[ 44.05]
nan
[ 45.3]
nan
[ 45.8]
nan
[ 45.75]
nan
[ 46.45]
nan
[ 47.7]
nan
[ 46.75]
nan
[ 45.8]
nan
[ 44.05]
nan
[ 44.4]
nan
[ 46.]
nan
[ 46.2]
nan
[ 50.1]
nan
[ 49.15]
nan
[ 49.1]
nan
[ 49.1]
nan
[ 48.8]
nan
[ 48.65]
nan
[ 49.55]
nan
[ 50.05]
nan
[ 51.05]
nan
[ 56.5]
nan
[ 57.1]
nan
[ 58.3]
nan
[ 57.85]
nan
[ 58.6]
nan
[ 57.5]
nan
[ 59.1]
nan
[ 60.9]
nan
[ 59.25]
nan
[ 58.25]
nan
[ 59.35]
nan
[ 61.4]
nan
[ 61.65]
nan
[ 62.05]
nan
[ 60.3]
nan
[ 61.7]
nan
[ 62.05]
nan
[ 62.75]
nan
[ 64.05]
nan
[ 63.6]
nan
[ 64.4]
nan
[ 64.4]
nan
[ 64.15]
nan
[ 64.4]
nan
[ 64.4]
nan
[ 64.5]
nan
[ 64.7]
nan
[ 65.]
nan
[ 65.]
nan
[ 65.]
nan
[ 64.7]
nan
[ 64.75]
nan
[ 65.85]
nan
[ 63.15]
nan
[ 64.95]
nan
[ 66.3]
nan
[ 64.75]
nan
[ 64.3]
nan
[ 65.1]
nan
[ 65.75]
nan
[ 66.9]
nan
[ 66.35]
nan
[ 65.9]
nan
[ 66.7]
nan
[ 65.85]
nan
[ 66.]
nan
[ 65.85]
nan
[ 64.95]
nan
[ 64.95]
nan
[ 63.65]
nan
[ 61.95]
nan
[ 61.9]
nan
[ 61.35]
nan
[ 59.3]
nan
[ 60.9]
nan
[ 61.7]
nan
[ 61.45]
nan
[ 62.3]
nan
[ 63.5]
nan
[ 61.85]
nan
[ 62.95]
nan
[ 62.95]
nan
[ 70.05]
nan
[ 68.7]
nan
[ 69.1]
nan
[ 68.8]
nan
[ 68.8]
nan
[ 66.95]
nan
[ 68.45]
nan
[ 68.65]
nan
[ 69.25]
nan
[ 72.]
nan
[ 72.45]
nan
[ 72.6]
nan
[ 72.95]
nan
[ 71.95]
nan
[ 73.3]
nan
[ 73.05]
nan
[ 74.4]
nan
[ 72.85]
nan
[ 75.8]
nan
[ 75.6]
nan
[ 74.1]
nan
[ 72.6]
nan
[ 75.45]
nan
[ 72.15]
nan
[ 70.2]
nan
[ 72.85]
nan
[ 71.2]
nan
[ 73.2]
nan
[ 73.7]
nan
[ 74.1]
nan
[ 73.5]
nan
[ 73.55]
nan
[ 75.7]
nan
[ 77.15]
nan
[ 77.55]
nan
[ 77.9]
nan
[ 78.35]
nan
[ 77.8]
nan
[ 77.95]
nan
[ 77.25]
nan
[ 75.75]
nan
[ 74.8]
nan
[ 74.25]
nan
[ 75.8]
nan
[ 79.4]
nan
[ 78.65]
nan
[ 78.6]
nan
[ 78.8]
nan
[ 79.9]
nan
[ 81.]
nan
[ 81.45]
nan
[ 82.65]
nan
[ 82.1]
nan
[ 83.45]
nan
[ 83.85]
nan
[ 83.65]
nan
[ 84.35]
nan
[ 83.55]
nan
[ 84.9]
nan
[ 87.05]
nan
[ 85.1]
nan
[ 85.55]
nan
[ 85.3]
nan
[ 85.7]
nan
[ 85.35]
nan
[ 86.3]
nan
[ 86.1]
nan
[ 86.5]
nan
[ 85.4]
nan
[ 86.5]
nan
[ 86.45]
nan
[ 88.6]
nan
[ 88.]
nan
[ 86.45]
nan
[ 85.5]
nan
[ 85.15]
nan
[ 85.5]
nan
[ 86.]
nan
[ 86.35]
nan
[ 85.9]
nan
[ 84.35]
nan
[ 84.3]
nan
[ 88.35]
nan
[ 86.85]
nan
[ 86.5]
nan
[ 82.55]
nan
[ 83.75]
nan
[ 81.8]
nan
[ 84.35]
nan
[ 84.3]
nan
[ 86.45]
nan
[ 84.65]
nan
[ 85.9]
nan
[ 86.15]
nan
[ 84.4]
nan
[ 85.45]
nan
[ 84.75]
nan
[ 83.1]
nan
[ 82.9]
nan
[ 83.95]
nan
[ 84.7]
nan
[ 83.6]
nan
[ 82.75]
nan
[ 83.4]
nan
[ 83.25]
nan
[ 82.45]
nan
[ 83.4]
nan
[ 82.1]
nan
[ 80.7]
nan
[ 78.8]
nan
[ 78.5]
nan
[ 80.15]
nan
[ 80.]
nan
[ 79.15]
nan
[ 80.85]
nan
[ 81.85]
nan
[ 82.05]
nan
[ 82.]
nan
[ 80.8]
nan
[ 80.8]
nan
[ 80.3]
nan
[ 79.55]
nan
[ 79.45]
nan
[ 79.8]
nan
[ 79.15]
nan
[ 79.7]
nan
[ 81.15]
nan
[ 80.45]
nan
[ 82.15]
nan
[ 82.55]
nan
[ 82.6]
nan
[ 81.75]
nan
[ 81.65]
nan
[ 83.2]
nan
[ 80.]
nan
[ 78.55]
nan
[ 76.7]
nan
[ 75.7]
nan
[ 76.]
nan
[ 76.25]
nan
[ 77.45]
nan
[ 77.35]
nan
[ 77.15]
nan
[ 77.1]
nan
[ 77.8]
nan
[ 78.1]
nan
[ 78.65]
nan
[ 76.85]
nan
[ 76.9]
nan
[ 76.6]
nan
[ 72.4]
nan
[ 72.8]
nan
[ 73.45]
nan
[ 73.45]
nan
[ 75.53]
nan
[ 77.18]
nan
[ 75.9]
nan
[ 74.29]
nan
[ 73.14]
nan
[ 71.46]
nan
[ 70.06]
nan
[ 69.84]
nan
[ 71.72]
nan
[ 71.89]
nan
[ 73.27]
nan
[ 72.74]
nan
[ 71.08]
nan
[ 70.58]
nan
[ 71.99]
nan
[ 71.34]
nan
[ 65.98]
nan
[ 72.77]
nan
[ 83.3]
nan
[ 80.71]
nan
[ 82.39]
error mean simple: nan
C:\Users\mofir\egit-master\ws\MLproject\main\main.py:514: RuntimeWarning: invalid value encountered in greater
  buy_vector = next_predicted_value > curr_predicted_value
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:93: RuntimeWarning: invalid value encountered in long_scalars
  false_buy_ratio   = false_buy_vector.sum()/buy_vector.sum()
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:140: RuntimeWarning: invalid value encountered in greater
  buy_vector = predictad_value_shifted > last_value
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                      NaN                   1.0                   0.0   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01         NaN        0.0             0.007793                 0.491935   0  
               close      open      high       low
2013-11-18   14.9387   14.9766   15.1092   14.9009
2013-11-19   14.6960   14.8674   15.0006   14.6770
2013-11-20   14.4771   14.7056   14.7436   14.4486
2013-11-21   14.5913   14.4962   14.6199   14.4295
2013-11-22   14.4486   14.6104   14.6104   14.4200
2013-11-25   14.7627   14.4486   14.7817   14.4486
2013-11-26   14.8864   14.9245   15.0435   14.8245
2013-11-27   14.9435   14.8769   14.9863   14.7817
2013-11-29   14.8483   14.9435   14.9816   14.7531
2013-12-02   14.9911   14.7912   15.0958   14.7627
2013-12-03   14.9816   15.0387   15.1862   14.8864
2013-12-04   15.1910   14.9435   15.2195   14.9007
2013-12-05   14.9435   15.2195   15.2195   14.9245
2013-12-06   14.7246   15.1244   15.1244   14.6865
2013-12-09   14.4724   14.7246   14.7722   14.4486
2013-12-10   14.8103   14.4771   14.8769   14.4581
2013-12-11   14.6675   14.8388   14.8578   14.6389
2013-12-12   14.3819   14.6199   14.6960   14.3534
2013-12-13   14.2963   14.4105   14.5152   14.2772
2013-12-16   14.3153   14.3058   14.4390   14.2963
2013-12-17   14.3819   14.3058   14.4200   14.1821
2013-12-18   14.5818   14.4105   14.6104   14.2297
2013-12-19   14.6389   14.5628   14.6580   14.4676
2013-12-20   14.9292   14.6675   14.9435   14.6294
2013-12-23   15.0197   15.0197   15.1434   14.8959
2013-12-24   15.0577   15.1053   15.1053   14.9054
2013-12-26   14.9150   15.1148   15.1577   14.8769
2013-12-27   15.0006   14.9911   15.0292   14.9054
2013-12-30   15.2005   15.0101   15.2100   14.9911
2013-12-31   15.2481   15.2291   15.3242   15.1339
...              ...       ...       ...       ...
2018-09-19  271.9800  270.2725  272.6990  268.2500
2018-09-20  266.2800  267.0500  268.7600  264.1000
2018-09-21  263.4500  266.7600  268.6000  262.1100
2018-09-24  265.7000  262.2000  265.8400  258.6800
2018-09-25  268.4100  268.3700  269.4200  264.9100
2018-09-26  266.9200  268.6100  270.2400  266.2800
2018-09-27  267.4000  268.3400  269.2000  266.1295
2018-09-28  281.0200  272.7300  281.9152  271.6000
2018-10-01  289.3600  284.1600  292.0600  282.6000
2018-10-02  286.4800  288.2500  292.7600  285.5800
2018-10-03  286.7300  289.3200  289.6200  282.5300
2018-10-04  279.2900  285.2700  286.2500  276.1800
2018-10-05  269.8600  278.2900  280.8000  267.5400
2018-10-08  265.7700  266.5000  271.1600  260.0800
2018-10-09  265.5400  264.9400  268.7600  262.8000
2018-10-10  245.6900  261.2600  263.1100  245.6000
2018-10-11  235.1300  242.1700  247.5600  234.2610
2018-10-12  246.5400  245.5066  249.5412  239.6500
2018-10-15  235.3800  246.0000  246.0000  235.3400
2018-10-16  245.8300  239.9300  246.2800  237.9400
2018-10-17  243.0600  248.3400  249.8800  241.0800
2018-10-18  239.5300  245.8600  247.4100  237.0900
2018-10-19  229.1700  241.7600  242.5500  227.7000
2018-10-22  231.2200  231.2800  235.3200  227.0700
2018-10-23  221.0600  220.4300  224.1900  216.7100
2018-10-24  199.4100  219.5100  221.3900  198.8500
2018-10-25  207.8400  195.4700  209.7500  193.6815
2018-10-26  198.2900  198.3100  204.8420  193.1200
2018-10-29  185.6200  203.9900  204.1300  176.0100
2018-10-30  203.0000  186.5500  203.4000  185.6200

[1247 rows x 4 columns]
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - all epochs loss are: 
INFO - [nan, nan, nan]
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - train_total_time: 
INFO - -30.12145757675171
INFO - optimizer_step_time: 
INFO - [0.0010025501251220703, 0.0, 0.0009992122650146484, 0.0, 0.0, 0.0010013580322265625, 0.0, 0.0, 0.0, 0.00099945068359375, 0.0010008811950683594, 0.0, 0.0, 0.0010001659393310547, 0.0010001659393310547, 0.0, 0.0009996891021728516, 0.00099945068359375, 0.0, 0.0, 0.0009815692901611328, 0.0009982585906982422, 0.0009999275207519531, 0.0, 0.0009980201721191406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009992122650146484, 0.0009989738464355469, 0.0, 0.0009996891021728516, 0.0, 0.0009970664978027344, 0.0, 0.0, 0.0, 0.0, 0.0]
INFO - backward_step_time: 
INFO - [0.010973930358886719, 0.03198099136352539, 0.0419769287109375, 0.0519709587097168, 0.0679619312286377, 0.07595562934875488, 0.08694911003112793, 0.09594345092773438, 0.09794259071350098, 0.12490987777709961, 0.12191033363342285, 0.13492250442504883, 0.15091252326965332, 0.16590571403503418, 0.16590595245361328, 0.17789626121520996, 0.18988776206970215, 0.19988584518432617, 0.22388982772827148, 0.23786306381225586, 0.23288273811340332, 0.23186850547790527, 0.2608470916748047, 0.27683591842651367, 0.282839298248291, 0.304826021194458, 0.33180880546569824, 0.3388051986694336, 0.3268108367919922, 0.35179948806762695, 0.3737828731536865, 0.3797793388366699, 0.41274285316467285, 0.45673656463623047, 0.5606777667999268, 0.5476830005645752, 0.7175867557525635, 0.7085902690887451, 0.6536233425140381, 0.8894872665405273, 0.7275815010070801, 1.0508008003234863]
INFO - optimizer_zero_grad_time: 
INFO - [0.0, 0.0, 0.0, 0.0009989738464355469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009915828704833984, 0.0, 0.0, 0.0, 0.0]
INFO - epoch_step_time: 
INFO - [6.878415822982788, 8.96784257888794, 14.27419924736023]
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:214: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan)]
nan
[ 102.4867]
nan
[ 120.7558]
nan
[ 125.9429]
nan
[ 127.3268]
nan
[ 133.7185]
nan
[ 136.2075]
nan
[ 127.1575]
nan
[ 132.484]
nan
[ 135.5436]
nan
[ 138.4339]
nan
[ 136.5702]
nan
[ 138.105]
nan
[ 137.7961]
nan
[ 141.3641]
nan
[ 144.3839]
nan
[ 143.8656]
nan
[ 143.8756]
nan
[ 143.158]
nan
[ 147.5133]
nan
[ 146.8456]
nan
[ 148.6196]
nan
[ 159.4033]
nan
[ 149.098]
nan
[ 149.4668]
nan
[ 150.892]
nan
[ 151.2109]
nan
[ 151.8587]
nan
[ 151.1112]
nan
[ 156.7921]
nan
[ 156.5629]
nan
[ 158.9349]
nan
[ 157.8386]
nan
[ 153.3138]
nan
[ 151.6395]
nan
[ 146.0881]
nan
[ 151.2408]
nan
[ 146.1878]
nan
[ 144.0749]
nan
[ 138.8625]
nan
[ 142.57]
nan
[ 142.9986]
nan
[ 146.2675]
nan
[ 153.1843]
nan
[ 155.3569]
nan
[ 161.9647]
nan
[ 160.091]
nan
[ 164.3965]
nan
[ 163.6989]
nan
[ 165.4031]
nan
[ 164.546]
nan
[ 166.938]
nan
[ 167.5359]
nan
[ 165.5925]
nan
[ 164.7952]
nan
[ 166.6988]
nan
[ 161.1973]
nan
[ 163.8384]
nan
hello all, todays date & time is: 11/17/18 07:53:26
[ 161.9647]
nan
[ 163.9381]
nan
INFO - ****************** FeatureBuilderMain : MLNX*******************
[ 163.8384]
nan
[ 165.9214]
nan
[ 166.6489]
nan
[ 171.7717]
nan
[ 169.7286]
nan
[ 171.5325]
nan
[ 164.1872]
nan
[ 155.4367]
nan
[ 167.8349]
nan
[ 166.4197]
nan
[ 164.5958]
nan
[ 160.9282]
nan
[ 160.9581]
nan
[ 158.616]
nan
[ 162.1473]
nan
[ 165.3892]
nan
[ 164.7807]
nan
[ 163.4041]
nan
[ 164.5613]
nan
[ 164.2919]
nan
[ 165.2695]
nan
[ 169.0202]
nan
[ 170.0377]
nan
[ 165.4989]
nan
[ 165.3992]
nan
[ 166.1673]
nan
[ 163.2844]
nan
[ 168.5813]
nan
[ 169.1898]
nan
[ 169.9479]
nan
[ 168.9803]
nan
[ 179.6637]
nan
[ 187.0853]
nan
[ 186.8858]
nan
[ 185.3795]
nan
[ 180.3121]
nan
[ 178.5565]
nan
[ 170.5763]
nan
[ 171.5339]
nan
[ 175.2946]
nan
[ 175.2447]
nan
[ 178.3271]
nan
[ 178.5565]
nan
[ 178.9256]
nan
[ 180.4219]
nan
[ 180.3221]
nan
[ 180.8508]
nan
[ 184.9307]
nan
[ 188.4619]
nan
[ 190.4669]
nan
[ 190.5567]
nan
[ 194.1079]
nan
[ 197.4396]
nan
[ 197.26]
nan
[ 197.0905]
nan
[ 197.3099]
nan
[ 196.4121]
nan
[ 196.1328]
nan
[ 198.1877]
nan
[ 193.1802]
nan
[ 195.2051]
nan
[ 201.3599]
nan
[ 203.3349]
nan
[ 206.2976]
nan
[ 206.6866]
nan
[ 205.4297]
nan
[ 208.1729]
nan
[ 209.1106]
nan
[ 211.5047]
nan
[ 208.6418]
nan
[ 204.8113]
nan
[ 215.6045]
nan
[ 212.1032]
nan
[ 213.6493]
nan
[ 209.4597]
nan
[ 211.0857]
nan
[ 210.8363]
nan
[ 213.5496]
nan
[ 215.5147]
nan
[ 214.5464]
nan
[ 216.5727]
nan
[ 213.7578]
nan
[ 210.3339]
nan
[ 196.0694]
nan
[ 200.3517]
nan
[ 197.3272]
nan
[ 186.3268]
nan
[ 187.4049]
nan
[ 188.9222]
nan
[ 191.6473]
nan
[ 191.1482]
nan
[ 194.3125]
nan
[ 190.4994]
nan
[ 185.8477]
nan
[ 186.1372]
nan
[ 191.2181]
nan
[ 197.5468]
nan
[ 195.76]
nan
[ 196.4487]
nan
[ 195.5403]
nan
[ 194.9215]
nan
[ 197.0876]
nan
[ 196.8181]
nan
[ 197.0477]
nan
[ 193.1546]
nan
[ 198.9942]
nan
[ 212.0908]
nan
[ 213.2088]
nan
[ 215.0155]
nan
[ 221.6037]
nan
[ 221.5438]
nan
[ 223.2807]
nan
[ 223.68]
nan
[ 222.582]
nan
[ 219.7171]
nan
[ 224.3189]
nan
[ 224.0394]
nan
[ 229.6993]
nan
[ 233.2729]
nan
[ 238.4836]
nan
[ 235.3791]
nan
[ 235.9281]
nan
[ 242.8957]
nan
[ 246.4094]
nan
[ 242.2868]
nan
[ 245.3613]
nan
[ 240.0707]
nan
[ 233.1032]
nan
[ 213.3186]
nan
[ 225.1774]
nan
[ 228.3916]
nan
[ 217.1317]
nan
[ 231.6658]
nan
[ 227.623]
nan
[ 232.2148]
nan
[ 240.9891]
nan
[ 246.06]
nan
[ 243.4048]
nan
[ 248.6354]
nan
[ 241.0789]
nan
[ 241.868]
nan
[ 245.6436]
nan
[ 246.2928]
nan
[ 245.7734]
nan
[ 241.7182]
nan
[ 231.9396]
nan
[ 236.2645]
nan
[ 235.3755]
nan
[ 241.878]
nan
[ 241.5583]
nan
[ 240.8991]
nan
[ 245.0443]
nan
[ 249.4691]
nan
[ 247.4215]
nan
[ 248.4503]
nan
[ 249.0496]
nan
[ 250.1883]
nan
[ 240.7193]
nan
[ 249.2893]
nan
[ 248.2705]
nan
[ 241.5683]
nan
[ 232.6987]
nan
[ 244.1953]
nan
[ 225.2573]
nan
[ 221.0922]
nan
[ 231.3203]
nan
[ 220.7926]
nan
[ 225.0875]
nan
[ 225.9765]
nan
[ 221.1222]
nan
[ 214.0005]
nan
[ 215.1591]
nan
[ 227.6446]
nan
[ 225.9765]
nan
[ 234.3268]
nan
[ 231.2304]
nan
[ 231.2204]
nan
[ 237.2633]
nan
[ 236.0947]
nan
[ 228.7732]
nan
[ 228.4436]
nan
[ 223.6193]
nan
[ 220.9424]
nan
[ 216.4077]
nan
[ 224.9577]
nan
[ 226.0664]
nan
[ 224.6381]
nan
[ 226.8755]
nan
[ 226.0464]
nan
[ 232.7186]
nan
[ 238.7816]
nan
[ 248.3904]
nan
[ 250.1084]
nan
[ 255.4821]
nan
[ 259.827]
nan
[ 254.2336]
nan
[ 255.0626]
nan
[ 245.274]
nan
[ 245.7235]
nan
[ 247.4215]
nan
[ 245.6536]
nan
[ 243.9555]
nan
[ 242.4173]
nan
[ 247.4046]
nan
[ 247.5545]
nan
[ 249.1436]
nan
[ 248.454]
nan
[ 252.8516]
nan
[ 252.0521]
nan
[ 257.4791]
nan
[ 264.7051]
nan
[ 264.925]
nan
[ 265.005]
nan
[ 262.7562]
nan
[ 262.1365]
nan
[ 260.4774]
nan
[ 262.4364]
nan
[ 262.2565]
nan
[ 266.764]
nan
[ 265.1149]
nan
[ 264.945]
nan
[ 260.0277]
nan
[ 262.1665]
nan
[ 256.9694]
nan
[ 250.8127]
nan
[ 238.9892]
nan
[ 241.8576]
nan
[ 235.5911]
nan
[ 240.7283]
nan
[ 236.7704]
nan
[ 242.1075]
nan
[ 236.7104]
nan
[ 242.5972]
nan
[ 247.1947]
nan
[ 249.1137]
nan
[ 253.1115]
nan
[ 247.3946]
nan
[ 251.0926]
nan
[ 249.1836]
nan
[ 248.0642]
nan
[ 253.5512]
nan
[ 251.5623]
nan
[ 251.8921]
nan
[ 250.7528]
nan
[ 249.2736]
nan
[ 248.574]
nan
[ 251.7322]
nan
[ 254.7006]
nan
[ 251.8821]
nan
[ 243.9965]
nan
[ 244.7261]
nan
[ 246.3352]
nan
[ 250.4829]
nan
[ 251.9621]
nan
[ 253.891]
nan
[ 256.8094]
nan
[ 258.2786]
nan
[ 256.3197]
nan
[ 254.6506]
nan
[ 255.9799]
nan
[ 261.287]
nan
[ 258.9383]
nan
[ 257.2992]
nan
[ 244.6861]
nan
[ 247.7044]
nan
[ 253.1814]
nan
[ 262.6762]
nan
[ 266.694]
nan
[ 272.0711]
nan
[ 275.7491]
nan
[ 274.2299]
nan
[ 278.49]
nan
[ 277.81]
nan
[ 280.68]
nan
[ 283.7]
nan
[ 278.42]
nan
[ 272.72]
nan
[ 271.86]
nan
[ 274.73]
nan
[ 272.8]
nan
[ 268.2]
nan
[ 271.34]
nan
[ 276.43]
nan
[ 273.93]
nan
[ 271.02]
nan
[ 271.98]
nan
[ 266.28]
nan
[ 263.45]
nan
[ 265.7]
nan
[ 268.41]
nan
[ 266.92]
nan
[ 267.4]
nan
[ 281.02]
nan
[ 289.36]
nan
[ 286.48]
nan
[ 286.73]
nan
[ 279.29]
nan
[ 269.86]
nan
[ 265.77]
nan
[ 265.54]
nan
[ 245.69]
nan
[ 235.13]
nan
[ 246.54]
nan
[ 235.38]
nan
[ 245.83]
nan
[ 243.06]
nan
[ 239.53]
nan
[ 229.17]
nan
[ 231.22]
nan
[ 221.06]
nan
[ 199.41]
nan
[ 207.84]
nan
[ 198.29]
nan
[ 185.62]
nan
[ 203.]
error mean simple: nan
C:\Users\mofir\egit-master\ws\MLproject\main\main.py:514: RuntimeWarning: invalid value encountered in greater
  buy_vector = next_predicted_value > curr_predicted_value
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:93: RuntimeWarning: invalid value encountered in long_scalars
  false_buy_ratio   = false_buy_vector.sum()/buy_vector.sum()
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:140: RuntimeWarning: invalid value encountered in greater
  buy_vector = predictad_value_shifted > last_value
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
INFO - SimpleRnn: epoch num: 
INFO - 0
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                      NaN                   1.0                   0.0   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01         NaN        0.0             0.010368                 0.540323   0  
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - all epochs loss are: 
INFO - [nan, nan, nan]
INFO - train_total_time: 
INFO - -27.499166011810303
INFO - optimizer_step_time: 
INFO - [0.001995086669921875, 0.000997781753540039, 0.0, 0.0009982585906982422, 0.00099945068359375, 0.0, 0.0, 0.0, 0.0010001659393310547, 0.0010001659393310547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009996891021728516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009999275207519531, 0.0010001659393310547, 0.000982046127319336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009989738464355469, 0.0, 0.0009982585906982422, 0.0010013580322265625, 0.0009999275207519531, 0.0, 0.0, 0.0, 0.0]
INFO - backward_step_time: 
INFO - [0.011972188949584961, 0.02098679542541504, 0.03897690773010254, 0.049970388412475586, 0.06596136093139648, 0.06496024131774902, 0.07495784759521484, 0.10294318199157715, 0.10394144058227539, 0.10993719100952148, 0.12392640113830566, 0.13392281532287598, 0.14191675186157227, 0.1429142951965332, 0.16890287399291992, 0.18089532852172852, 0.19188904762268066, 0.20689964294433594, 0.203902006149292, 0.21287822723388672, 0.23386335372924805, 0.23586082458496094, 0.2718234062194824, 0.254871129989624, 0.2868342399597168, 0.3108224868774414, 0.292832612991333, 0.3028252124786377, 0.3138000965118408, 0.32181429862976074, 0.3288097381591797, 0.35677409172058105, 0.4247550964355469, 0.4667317867279053, 0.4377455711364746, 0.42975282669067383, 0.55867600440979, 0.5796668529510498, 0.5816669464111328, 0.44574451446533203, 0.46173524856567383, 0.4617147445678711]
INFO - optimizer_zero_grad_time: 
INFO - [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
INFO - epoch_step_time: 
INFO - [7.322769641876221, 8.687004566192627, 11.488396406173706]
hello from PredictSimpleRnn
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:214: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  evaluation_summary.append(loss.data[0])
INFO - evaluation_summary: 
INFO - [tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(nan)]
nan
[ 47.7]
nan
[ 47.15]
nan
[ 47.4]
nan
[ 47.15]
nan
[ 47.8]
nan
[ 48.]
nan
[ 45.8]
nan
[ 46.15]
nan
[ 47.1]
nan
[ 47.5]
nan
[ 48.15]
nan
[ 48.15]
nan
[ 47.9]
nan
[ 48.05]
nan
[ 47.95]
nan
[ 47.5]
nan
[ 47.95]
nan
[ 47.7]
nan
[ 47.7]
nan
[ 46.9]
nan
[ 46.4]
nan
[ 46.65]
nan
[ 45.1]
nan
[ 45.15]
nan
[ 46.]
nan
[ 44.8]
nan
[ 43.9]
nan
[ 43.5]
nan
[ 43.8]
nan
[ 43.6]
nan
[ 44.25]
nan
[ 44.05]
nan
[ 44.1]
nan
[ 43.8]
nan
[ 43.25]
nan
[ 43.85]
nan
[ 42.5]
nan
[ 43.3]
nan
[ 42.5]
nan
[ 43.05]
nan
[ 42.9]
nan
[ 43.6]
nan
[ 43.85]
nan
[ 44.35]
nan
[ 45.]
nan
[ 44.8]
nan
[ 45.]
nan
[ 45.05]
nan
[ 44.7]
nan
[ 44.95]
nan
[ 44.6]
nan
[ 44.5]
nan
[ 44.95]
nan
[ 45.3]
nan
[ 44.75]
nan
[ 45.75]
nan
[ 47.05]
nan
[ 46.95]
nan
[ 47.2]
nan
[ 46.75]
nan
[ 46.]
nan
[ 45.2]
nan
[ 45.1]
nan
[ 44.7]
nan
[ 44.75]
nan
[ 43.75]
nan
[ 43.6]
nan
[ 44.05]
nan
[ 44.15]
nan
[ 44.4]
nan
[ 43.6]
nan
[ 43.25]
nan
[ 43.]
nan
[ 43.95]
nan
[ 44.2]
nan
[ 44.3]
nan
[ 44.4]
nan
[ 45.2]
nan
[ 46.75]
nan
[ 46.75]
nan
[ 46.95]
nan
[ 46.35]
nan
[ 46.15]
nan
[ 46.45]
nan
[ 46.9]
nan
[ 46.35]
nan
[ 46.7]
nan
[ 47.1]
nan
[ 47.15]
nan
[ 46.95]
nan
[ 46.85]
nan
[ 46.6]
nan
[ 46.4]
nan
[ 45.75]
nan
[ 45.6]
nan
[ 46.15]
nan
[ 46.55]
nan
[ 46.3]
nan
[ 47.7]
nan
[ 47.7]
nan
[ 47.15]
nan
[ 48.5]
nan
[ 48.6]
nan
[ 48.15]
nan
[ 47.55]
nan
[ 47.5]
nan
[ 45.7]
nan
[ 46.15]
nan
[ 45.15]
nan
[ 44.7]
nan
[ 44.65]
nan
[ 44.2]
nan
[ 43.5]
nan
[ 43.1]
nan
[ 43.3]
nan
[ 43.65]
nan
[ 44.05]
nan
[ 45.3]
nan
[ 45.8]
nan
[ 45.75]
nan
[ 46.45]
nan
[ 47.7]
nan
[ 46.75]
nan
[ 45.8]
nan
[ 44.05]
nan
[ 44.4]
nan
[ 46.]
nan
[ 46.2]
nan
[ 50.1]
nan
[ 49.15]
nan
[ 49.1]
nan
[ 49.1]
nan
[ 48.8]
nan
[ 48.65]
nan
[ 49.55]
nan
[ 50.05]
nan
[ 51.05]
nan
[ 56.5]
nan
[ 57.1]
nan
[ 58.3]
nan
[ 57.85]
nan
[ 58.6]
nan
[ 57.5]
nan
[ 59.1]
nan
[ 60.9]
nan
[ 59.25]
nan
[ 58.25]
nan
[ 59.35]
nan
[ 61.4]
nan
[ 61.65]
nan
[ 62.05]
nan
[ 60.3]
nan
[ 61.7]
nan
[ 62.05]
nan
[ 62.75]
nan
[ 64.05]
nan
[ 63.6]
nan
[ 64.4]
nan
[ 64.4]
nan
[ 64.15]
nan
[ 64.4]
nan
[ 64.4]
nan
[ 64.5]
nan
[ 64.7]
nan
[ 65.]
nan
[ 65.]
nan
[ 65.]
nan
[ 64.7]
nan
[ 64.75]
nan
[ 65.85]
nan
[ 63.15]
nan
[ 64.95]
nan
[ 66.3]
nan
[ 64.75]
nan
[ 64.3]
nan
[ 65.1]
nan
[ 65.75]
nan
[ 66.9]
nan
[ 66.35]
nan
[ 65.9]
nan
[ 66.7]
nan
[ 65.85]
nan
[ 66.]
nan
[ 65.85]
nan
[ 64.95]
nan
[ 64.95]
nan
[ 63.65]
nan
[ 61.95]
nan
[ 61.9]
nan
[ 61.35]
nan
[ 59.3]
nan
[ 60.9]
nan
[ 61.7]
nan
[ 61.45]
nan
[ 62.3]
nan
[ 63.5]
nan
[ 61.85]
nan
[ 62.95]
nan
[ 62.95]
nan
[ 70.05]
nan
[ 68.7]
nan
[ 69.1]
nan
[ 68.8]
nan
[ 68.8]
nan
[ 66.95]
nan
[ 68.45]
nan
[ 68.65]
nan
[ 69.25]
nan
[ 72.]
nan
[ 72.45]
nan
[ 72.6]
nan
[ 72.95]
nan
[ 71.95]
nan
[ 73.3]
nan
[ 73.05]
nan
[ 74.4]
nan
[ 72.85]
nan
[ 75.8]
nan
[ 75.6]
nan
[ 74.1]
nan
[ 72.6]
nan
[ 75.45]
nan
[ 72.15]
nan
[ 70.2]
nan
[ 72.85]
nan
[ 71.2]
nan
[ 73.2]
nan
[ 73.7]
nan
[ 74.1]
nan
[ 73.5]
nan
[ 73.55]
nan
[ 75.7]
nan
[ 77.15]
nan
[ 77.55]
nan
[ 77.9]
nan
[ 78.35]
nan
[ 77.8]
nan
[ 77.95]
nan
[ 77.25]
nan
[ 75.75]
nan
[ 74.8]
nan
[ 74.25]
nan
[ 75.8]
nan
[ 79.4]
nan
[ 78.65]
nan
[ 78.6]
nan
[ 78.8]
nan
[ 79.9]
nan
[ 81.]
nan
[ 81.45]
nan
[ 82.65]
nan
[ 82.1]
nan
[ 83.45]
nan
[ 83.85]
nan
[ 83.65]
nan
[ 84.35]
nan
[ 83.55]
nan
[ 84.9]
nan
[ 87.05]
nan
[ 85.1]
nan
[ 85.55]
nan
[ 85.3]
nan
[ 85.7]
nan
[ 85.35]
nan
[ 86.3]
nan
[ 86.1]
nan
[ 86.5]
nan
[ 85.4]
nan
[ 86.5]
nan
[ 86.45]
nan
[ 88.6]
nan
[ 88.]
nan
[ 86.45]
nan
[ 85.5]
nan
[ 85.15]
nan
[ 85.5]
nan
[ 86.]
nan
[ 86.35]
nan
[ 85.9]
nan
[ 84.35]
nan
[ 84.3]
nan
[ 88.35]
nan
[ 86.85]
nan
[ 86.5]
nan
[ 82.55]
nan
[ 83.75]
nan
[ 81.8]
nan
[ 84.35]
nan
[ 84.3]
nan
[ 86.45]
nan
[ 84.65]
nan
[ 85.9]
nan
[ 86.15]
nan
[ 84.4]
nan
[ 85.45]
nan
[ 84.75]
nan
[ 83.1]
nan
[ 82.9]
nan
[ 83.95]
nan
[ 84.7]
nan
[ 83.6]
nan
[ 82.75]
nan
[ 83.4]
nan
[ 83.25]
nan
[ 82.45]
nan
[ 83.4]
nan
[ 82.1]
nan
[ 80.7]
nan
[ 78.8]
nan
[ 78.5]
nan
[ 80.15]
nan
[ 80.]
nan
[ 79.15]
nan
[ 80.85]
nan
[ 81.85]
nan
[ 82.05]
nan
[ 82.]
nan
[ 80.8]
nan
[ 80.8]
nan
[ 80.3]
nan
[ 79.55]
nan
[ 79.45]
nan
[ 79.8]
nan
[ 79.15]
nan
[ 79.7]
nan
[ 81.15]
nan
[ 80.45]
nan
[ 82.15]
nan
[ 82.55]
nan
[ 82.6]
nan
[ 81.75]
nan
[ 81.65]
nan
[ 83.2]
nan
[ 80.]
nan
[ 78.55]
nan
[ 76.7]
nan
[ 75.7]
nan
[ 76.]
nan
[ 76.25]
nan
[ 77.45]
nan
[ 77.35]
nan
[ 77.15]
nan
[ 77.1]
nan
[ 77.8]
nan
[ 78.1]
nan
[ 78.65]
nan
[ 76.85]
nan
[ 76.9]
nan
[ 76.6]
nan
[ 72.4]
nan
[ 72.8]
nan
[ 73.45]
nan
[ 73.45]
nan
[ 75.53]
nan
[ 77.18]
nan
[ 75.9]
nan
[ 74.29]
nan
[ 73.14]
nan
[ 71.46]
nan
[ 70.06]
nan
[ 69.84]
nan
[ 71.72]
nan
[ 71.89]
nan
[ 73.27]
nan
[ 72.74]
nan
[ 71.08]
nan
[ 70.58]
nan
[ 71.99]
nan
[ 71.34]
nan
[ 65.98]
nan
[ 72.77]
nan
[ 83.3]
nan
[ 80.71]
nan
[ 82.39]
error mean simple: nan
C:\Users\mofir\egit-master\ws\MLproject\main\main.py:514: RuntimeWarning: invalid value encountered in greater
  buy_vector = next_predicted_value > curr_predicted_value
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:93: RuntimeWarning: invalid value encountered in long_scalars
  false_buy_ratio   = false_buy_vector.sum()/buy_vector.sum()
C:\Users\mofir\egit-master\ws\MLproject\Statistics\CalculateStats.py:140: RuntimeWarning: invalid value encountered in greater
  buy_vector = predictad_value_shifted > last_value
      false_buy_ratio_in_down  false_negative_ratio  false_positive_ratio  \
0.01                      NaN                   1.0                   0.0   

      mean_error  mean_gain  mean_potential_gain  missing_buy_in_up_ratio std  
0.01         NaN        0.0             0.007793                 0.491935   0  
INFO - *****************finished executing******************
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 07:59:15
INFO - ****************** FeatureBuilderMain : MLNX*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 622, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 504, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 437, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 150, in ConstructTestData
    df[column] = preprocessing.normalize(df[column], norm='l2')
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\preprocessing\data.py", line 1546, in normalize
    estimator='the normalize function', dtype=FLOAT_DTYPES)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\utils\validation.py", line 547, in check_array
    "if it contains a single sample.".format(array))
ValueError: Expected 2D array, got 1D array instead:
array=[ 40.09  40.4   40.1  ...,  83.3   80.71  82.39].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 08:01:16
INFO - ****************** FeatureBuilderMain : MLNX*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
INFO - SimpleRnn: epoch num: 
INFO - 0
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
C:\Users\mofir\egit-master\ws\MLproject\Models\SimpleRNN.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), 0.5)
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 1
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - SimpleRnn: epoch num: 
INFO - 2
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
INFO - current batch mean loss is: 
INFO - nan
INFO - all epochs loss are: 
INFO - [nan, nan, nan]
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 08:03:04
INFO - ****************** FeatureBuilderMain : MLNX*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 623, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 438, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 223, in ConstructTestData
    y_ho_data = std_scale.transform(y_ho_data)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\preprocessing\data.py", line 755, in transform
    X -= self.mean_
ValueError: non-broadcastable output operand with shape (374,1) doesn't match the broadcast shape (374,8)
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 08:06:47
INFO - ****************** FeatureBuilderMain : MLNX*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 621, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 503, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 436, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 154, in ConstructTestData
    std_scale = scaler.fit(df[column])#TODO - do i need to seperate here to train and test?
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\preprocessing\data.py", line 617, in fit
    return self.partial_fit(X, y)
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\preprocessing\data.py", line 641, in partial_fit
    force_all_finite='allow-nan')
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\utils\validation.py", line 547, in check_array
    "if it contains a single sample.".format(array))
ValueError: Expected 2D array, got 1D array instead:
array=[ 40.09  40.4   40.1  ...,  83.3   80.71  82.39].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 08:08:22
INFO - ****************** FeatureBuilderMain : MLNX*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 621, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 503, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 436, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 154, in ConstructTestData
    std_scale = scaler.fit(df[column].reshape(-1, 1))#TODO - do i need to seperate here to train and test?
  File "C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\pandas\core\generic.py", line 4376, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'reshape'
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\odo\backends\pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.
You can access NaTType as type(pandas.NaT)
  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))
C:\Users\mofir\AppData\Local\Continuum\anaconda3\envs\pytorch\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
hello all, todays date & time is: 11/17/18 08:10:09
INFO - ****************** FeatureBuilderMain : MLNX*******************
             close   open     high      low
2013-11-18  40.090  40.60  41.3200  39.7700
2013-11-19  40.400  40.00  40.9000  39.8640
2013-11-20  40.100  40.46  40.9100  39.6700
2013-11-21  41.820  40.57  41.9000  40.1100
2013-11-22  41.910  41.79  42.3900  41.4000
2013-11-25  39.250  39.92  41.6800  38.9400
2013-11-26  38.750  38.90  39.2700  38.3000
2013-11-27  39.150  38.60  39.2900  37.7900
2013-11-29  38.940  39.18  39.2200  38.5000
2013-12-02  38.565  38.64  39.1900  37.9000
2013-12-03  37.970  38.31  38.5000  37.7100
2013-12-04  37.630  37.58  37.9800  37.3000
2013-12-05  38.590  37.51  39.7700  37.5000
2013-12-06  39.160  39.00  40.1399  38.8400
2013-12-09  39.030  40.44  41.3800  38.9200
2013-12-10  39.080  39.00  39.4200  38.3300
2013-12-11  38.070  38.05  39.2100  37.5900
2013-12-12  37.010  37.96  38.1900  36.7500
2013-12-13  37.010  37.00  37.3400  36.6800
2013-12-16  37.160  37.53  38.2000  36.9700
2013-12-17  37.870  37.09  38.0880  36.5000
2013-12-18  37.290  37.60  38.1800  37.0700
2013-12-19  39.500  36.59  39.7300  36.4850
2013-12-20  40.000  39.47  40.5000  39.1900
2013-12-23  39.800  39.94  41.1900  39.6200
2013-12-24  39.350  39.67  39.8350  39.0400
2013-12-26  39.789  39.06  40.4399  39.0600
2013-12-27  39.840  40.00  40.5500  39.6900
2013-12-30  39.730  39.62  39.9699  39.3300
2013-12-31  39.970  39.24  40.9400  39.0500
...            ...    ...      ...      ...
2018-09-19  78.100  77.85  78.2000  76.9320
2018-09-20  78.650  78.45  79.2750  77.7250
2018-09-21  76.850  78.40  79.0500  76.7500
2018-09-24  76.900  76.45  77.7500  76.2500
2018-09-25  76.600  76.40  77.0500  75.6000
2018-09-26  72.400  76.55  76.6500  72.1500
2018-09-27  72.800  72.40  73.9250  72.2000
2018-09-28  73.450  72.65  75.0000  72.6500
2018-10-01  73.450  73.72  74.9200  73.0600
2018-10-02  75.530  79.79  79.7900  74.9400
2018-10-03  77.180  76.19  77.7200  74.8400
2018-10-04  75.900  77.19  77.3200  75.3001
2018-10-05  74.290  75.66  76.8000  73.6000
2018-10-08  73.140  73.93  74.7200  72.3077
2018-10-09  71.460  72.83  73.1500  71.3700
2018-10-10  70.060  73.22  73.4700  69.8600
2018-10-11  69.840  69.56  71.8500  68.9081
2018-10-12  71.720  71.15  72.2700  70.5000
2018-10-15  71.890  71.30  72.1200  70.4450
2018-10-16  73.270  72.27  73.6300  71.6500
2018-10-17  72.740  73.53  73.5300  72.0600
2018-10-18  71.080  72.34  72.8000  70.8150
2018-10-19  70.580  71.81  72.1600  69.9100
2018-10-22  71.990  70.99  72.3400  70.5100
2018-10-23  71.340  70.48  71.6976  68.6200
2018-10-24  65.980  70.76  70.7600  65.6800
2018-10-25  72.770  73.03  75.5000  70.7500
2018-10-26  83.300  82.00  86.0700  81.0200
2018-10-29  80.710  83.64  84.0000  78.7300
2018-10-30  82.390  80.15  82.5000  79.3600

[1247 rows x 4 columns]
Traceback (most recent call last):
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 623, in <module>
    HyperParameter_Optimizations(CurrStockDataFrame, config_model_default, stat_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 505, in HyperParameter_Optimizations
    real_value,predictad_value = RunNetworkArch(CurrStockDataFrame, curr_config.Network_Params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 438, in RunNetworkArch
    Data         = ConstructTestData(df, model_params)
  File "C:\Users\mofir\egit-master\ws\MLproject\main\main.py", line 175, in ConstructTestData
    y_data = (np.asarray(df['close'])[1:])
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
